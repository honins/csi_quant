#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
AIé¢„æµ‹æ¨¡å‹æ»šåŠ¨å›æµ‹è„šæœ¬

è¯¥è„šæœ¬å®ç°AIé¢„æµ‹æ¨¡å‹çš„æ»šåŠ¨å›æµ‹ï¼Œæ¨¡æ‹Ÿæ¨¡å‹åœ¨ä¸åŒæ—¶é—´ç‚¹è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹ï¼Œ
å¹¶ç»Ÿè®¡é¢„æµ‹çš„æˆåŠŸç‡ï¼Œä»¥æ›´çœŸå®åœ°è¯„ä¼°æ¨¡å‹çš„é•¿æœŸè¡¨ç°ã€‚
"""

import sys
import os
import logging
import pandas as pd
from datetime import datetime, timedelta
import numpy as np
from sklearn.metrics import brier_score_loss, log_loss
# æ–°å¢ï¼šç”¨äºç¦»çº¿æ¦‚ç‡æ ‡å®š
from sklearn.linear_model import LogisticRegression
from sklearn.isotonic import IsotonicRegression
import json

# å‡è®¾é¡¹ç›®æ ¹ç›®å½•åœ¨sys.pathä¸­ï¼Œæˆ–è€…æ‰‹åŠ¨æ·»åŠ 
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.data.data_module import DataModule
from src.strategy.strategy_module import StrategyModule

from src.utils.utils import resolve_confidence_param
from src.prediction.prediction_utils import setup_logging, predict_and_validate
from src.utils.trade_date import is_trading_day
from src.utils.reporting import format_backtest_summary

def run_rolling_backtest(start_date_str: str, end_date_str: str, training_window_days: int = 365, 
                         reuse_model: bool = True, retrain_interval_days: int = None,
                         generate_report: bool = True, report_dir: str = None):
    setup_logging()
    logger = logging.getLogger("RollingBacktest")

    try:
        # ä½¿ç”¨æ ‡å‡†é…ç½®åŠ è½½å™¨ï¼ˆè‡ªåŠ¨åˆå¹¶æ‰€æœ‰é…ç½®æ–‡ä»¶ï¼‰
        from src.utils.config_loader import load_config as load_config_improved
        config = load_config_improved()
        

        
        # åº”ç”¨è®­ç»ƒç­–ç•¥é…ç½®
        if retrain_interval_days is not None:
            config.setdefault('ai', {})['retrain_interval_days'] = retrain_interval_days
        config.setdefault('ai', {})['enable_model_reuse'] = reuse_model
        
        # åˆå§‹åŒ–æ¨¡å—
        data_module = DataModule(config)
        strategy_module = StrategyModule(config)
        # ä½¿ç”¨AIä¼˜åŒ–å™¨
        from src.ai.ai_optimizer_improved import AIOptimizerImproved
        ai_optimizer = AIOptimizerImproved(config)

        # ğŸ”’ ç¦æ­¢å›æµ‹è¿‡ç¨‹ä¸­è‡ªåŠ¨è®­ç»ƒæ¨¡å‹ï¼Œåªå…è®¸ä½¿ç”¨å·²è®­ç»ƒæ¨¡å‹
        if not ai_optimizer._load_model():
            logger.error("âŒ æœªæ‰¾åˆ°å·²è®­ç»ƒçš„æ¨¡å‹ï¼")
            logger.error("ğŸ’¡ è¯·å…ˆè¿è¡Œä»¥ä¸‹å‘½ä»¤è®­ç»ƒæ¨¡å‹ï¼š")
            logger.error("   python run.py ai -m optimize  # AIä¼˜åŒ–+è®­ç»ƒ")
            logger.error("   python run.py ai -m full      # å®Œæ•´é‡è®­ç»ƒ")
            return {
                'success': False,
                'error': 'æœªæ‰¾åˆ°å·²è®­ç»ƒæ¨¡å‹ï¼Œè¯·å…ˆè®­ç»ƒï¼'
            }

        start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
        end_date = datetime.strptime(end_date_str, "%Y-%m-%d")

        results = []
        current_date = start_date
        training_count = 0  # è®°å½•å®é™…è®­ç»ƒæ¬¡æ•°

        # é¢„å…ˆè·å–æ‰€æœ‰å¯ç”¨äº¤æ˜“æ—¥
        all_data = data_module.get_history_data(start_date=start_date, end_date=end_date)
        all_data = data_module.preprocess_data(all_data)
        available_dates = set(pd.to_datetime(all_data['date']).dt.date)
        
        # æ–°å¢ï¼šæ§åˆ¶å‰Nå¤©çš„è¯¦ç»†æ—¥å¿—è¾“å‡ºï¼ˆé»˜è®¤å‰5å¤©ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
        try:
            first_n_days = int(os.environ.get('RB_FIRST_N_DAYS', '5'))
        except Exception:
            first_n_days = 5
        detailed_days_counter = 0
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸš€ æ»šåŠ¨å›æµ‹é…ç½®")
        logger.info(f"{'='*60}")
        logger.info(f"ğŸ“… å›æµ‹æœŸé—´: {start_date_str} è‡³ {end_date_str}")
        logger.info(f"ğŸ¤– åªä½¿ç”¨å·²è®­ç»ƒæ¨¡å‹: å¯ç”¨")
        logger.info(f"ğŸ“Š å¯ç”¨äº¤æ˜“æ—¥: {len(available_dates)} å¤©")
        logger.info(f"ğŸ“ å°†è¾“å‡ºå‰ {first_n_days} å¤©çš„è¯¦ç»†é¢„æµ‹æ—¥å¿—ï¼ˆconfidence åˆ†å¸ƒï¼‰")
        logger.info(f"{'='*60}")

        while current_date <= end_date:
            # æ–°å¢ï¼šåˆ¤æ–­è¯¥æ—¥æœŸæ˜¯å¦åœ¨äº¤æ˜“æ•°æ®æºä¸­
            if current_date.date() not in available_dates:
                current_date += timedelta(days=1)
                continue
            if not is_trading_day(current_date.date()):
                current_date += timedelta(days=1)
                continue

            logger.info(f"\n--- æ»šåŠ¨å›æµ‹æ—¥æœŸ: {current_date.strftime('%Y-%m-%d')} ---")

            # åªç”¨å·²è®­ç»ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹å’ŒéªŒè¯
            result = predict_and_validate(
                predict_date=current_date,
                data_module=data_module,
                strategy_module=strategy_module,
                ai_optimizer=ai_optimizer,
                config=config,
                logger=logger,
                force_retrain=False,  # ç¦æ­¢è‡ªåŠ¨è®­ç»ƒ
                only_use_trained_model=True  # ç¦æ­¢ä»»ä½•è®­ç»ƒå’Œä¿å­˜
            )

            # å‰Nå¤©è¯¦ç»†æ—¥å¿—ï¼šè¾“å‡º confidence åŠé˜ˆå€¼
            if result is not None and getattr(result, 'date', None) is not None and detailed_days_counter < first_n_days:
                try:
                    # å›ºå®šé˜ˆå€¼ï¼šä»é…ç½®è¯»å–
                    final_threshold = resolve_confidence_param(config, 'final_threshold', 0.5)
                    logger.info("[è¯¦ç»†æ—¥å¿—-é˜ˆå€¼ä¸ç½®ä¿¡åº¦]")
                    logger.info(f"  é˜ˆå€¼(final_threshold): {final_threshold:.4f}")
                    logger.info(f"  confidence: {getattr(result, 'confidence', None):.4f}")
                    logger.info(f"  é¢„æµ‹ç»“æœ: {'æ˜¯ä½ç‚¹' if getattr(result, 'predicted_low_point', False) else 'éä½ç‚¹'}  | å®é™…: {('æ˜¯ä½ç‚¹' if getattr(result, 'actual_low_point', False) else 'é/æœªçŸ¥')}  | æ˜¯å¦æ­£ç¡®: {getattr(result, 'prediction_correct', None)}")
                except Exception as e:
                    logger.warning(f"è¾“å‡ºè¯¦ç»†æ—¥å¿—æ—¶å‡ºé”™: {e}")
                detailed_days_counter += 1

            # ç»Ÿè®¡è®­ç»ƒæ¬¡æ•°ï¼ˆç†è®ºä¸Šåº”ä¸º0ï¼‰
            if hasattr(ai_optimizer, '_last_training_date') and ai_optimizer._last_training_date == current_date:
                training_count += 1

            if result is not None and getattr(result, 'date', None) is not None:
                results.append(result)

            current_date += timedelta(days=1) # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæ—¥æœŸ

        # ç»Ÿè®¡å’Œå¯è§†åŒ–ç»“æœ
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ“Š å›æµ‹æ•ˆç‡ç»Ÿè®¡")
        logger.info(f"{'='*60}")
        logger.info(f"ğŸ¯ æ€»å›æµ‹å¤©æ•°: {len(results)}")
        logger.info(f"ğŸ”„ å®é™…è®­ç»ƒæ¬¡æ•°: {training_count}")
        logger.info(f"âš¡ æ•ˆç‡æå‡: {((len(results) - training_count) / len(results) * 100):.1f}%")
        logger.info(f"{'='*60}")
        
        results_df = pd.DataFrame([vars(r) for r in results])
        if 'date' not in results_df.columns:
            logger.error(f"ç»“æœDataFrameç¼ºå°‘dateåˆ—ï¼Œå®é™…åˆ—: {results_df.columns.tolist()}")
            raise ValueError("ç»“æœDataFrameç¼ºå°‘dateåˆ—")
        # ç¡®ä¿dateä¸ºdatetimeç±»å‹
        results_df['date'] = pd.to_datetime(results_df['date'])
        results_df.set_index('date', inplace=True)
        # ç¡®ä¿prediction_correctä¸ºboolç±»å‹ - ä¿®å¤FutureWarning
        results_df['prediction_correct'] = results_df['prediction_correct'].fillna(False).infer_objects(copy=False).astype(bool)

        # è¿‡æ»¤æ‰æ— æ³•éªŒè¯çš„è¡Œ
        results_df_validated = results_df.dropna(subset=['prediction_correct'])

        if not results_df_validated.empty:
            total_predictions = len(results_df_validated)
            correct_predictions = results_df_validated['prediction_correct'].sum()
            success_rate = (correct_predictions / total_predictions) if total_predictions > 0 else 0

            logger.info("\n--- æ»šåŠ¨å›æµ‹ç»Ÿè®¡ç»“æœ ---")
            logger.info(f"æ€»é¢„æµ‹æ—¥æœŸæ•° (å¯éªŒè¯): {total_predictions}")
            logger.info(f"æ­£ç¡®é¢„æµ‹æ•°: {correct_predictions}")
            logger.info(f"é¢„æµ‹æˆåŠŸç‡: {success_rate:.2%}")

            # ========== åˆ†ç±»æŒ‡æ ‡ï¼ˆPrecision / Recall / F1 / Specificity / Balanced Accuracyï¼‰==========
            # ä»…ä½¿ç”¨å¯éªŒè¯çš„æ•°æ®è¡Œè®¡ç®—æ··æ·†çŸ©é˜µ
            y_pred = results_df_validated['predicted_low_point'].astype(bool)
            y_true = results_df_validated['actual_low_point'].astype(bool)

            tp = int(((y_pred == True) & (y_true == True)).sum())
            fp = int(((y_pred == True) & (y_true == False)).sum())
            tn = int(((y_pred == False) & (y_true == False)).sum())
            fn = int(((y_pred == False) & (y_true == True)).sum())

            pred_pos = tp + fp
            pred_neg = tn + fn
            actual_pos = tp + fn
            actual_neg = tn + fp

            success_rate = (tp + tn) / max((tp + tn + fp + fn), 1)
            precision = tp / max((tp + fp), 1) if (tp + fp) > 0 else 0.0
            recall = tp / max((tp + fn), 1) if (tp + fn) > 0 else 0.0
            specificity = tn / max((tn + fp), 1) if (tn + fp) > 0 else 0.0
            balanced_acc = (recall + specificity) / 2.0
            f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0.0
            
            # ç»Ÿä¸€ä½¿ç”¨â€œå¯éªŒè¯æ ·æœ¬æ•°â€
            total_predictions_validated = int((tp + tn + fp + fn))
        
            # æ–°å¢ï¼šå…ˆåˆå§‹åŒ–æ¦‚ç‡æ ¡å‡†ç›¸å…³å˜é‡ï¼Œé¿å…ä¸Šæ–¹è¯Šæ–­å¼‚å¸¸æ—¶æœªå®šä¹‰
            brier_value = None
            logloss_value = None
            ece_value = None
            calib_compare = []
            calib_bins_map = {}
        
            # æ–°å¢ï¼šç½®ä¿¡åº¦åˆ†å¸ƒè¯Šæ–­ï¼ˆconfidenceï¼‰
            try:
                conf_series = results_df['confidence'].astype(float).dropna()

                def _safe_stat(s: pd.Series):
                    if s.empty:
                        return {
                            'mean': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0,
                            'q10': 0.0, 'q25': 0.0, 'q50': 0.0, 'q75': 0.0, 'q90': 0.0
                        }
                    qs = s.quantile([0.1, 0.25, 0.5, 0.75, 0.9])
                    return {
                        'mean': float(s.mean()),
                        'std': float(s.std(ddof=0)),
                        'min': float(s.min()),
                        'max': float(s.max()),
                        'q10': float(qs.loc[0.1]),
                        'q25': float(qs.loc[0.25]),
                        'q50': float(qs.loc[0.5]),
                        'q75': float(qs.loc[0.75]),
                        'q90': float(qs.loc[0.9]),
                    }

                conf_stat = _safe_stat(conf_series)

                # ç›´æ–¹åˆ†å¸ƒï¼ˆconfidenceï¼‰ï¼šå«é˜ˆå€¼é™„è¿‘çš„ç»†åˆ†
                final_threshold = resolve_confidence_param(config, 'final_threshold', 0.5)
                bins = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]
                bin_labels = ["[0.0,0.2)", "[0.2,0.4)", "[0.4,0.5)", "[0.5,0.6)", "[0.6,0.8)", "[0.8,1.0]"]
                bin_counts = {lbl: 0 for lbl in bin_labels}
                if len(conf_series) > 0:
                    counts, _ = np.histogram(conf_series, bins=bins)
                    for i, lbl in enumerate(bin_labels):
                        bin_counts[lbl] = int(counts[i])
                bin_perc = {k: (v / len(conf_series) * 100 if len(conf_series) > 0 else 0.0) for k, v in bin_counts.items()}

                # ç°åŒºï¼ˆå†³ç­–å¸¦ï¼‰
                band_top = (config.get('confidence_weights', {}) or {}).get('decision_band', {}) or {}
                band_stg = ((config.get('strategy', {}) or {}).get('confidence_weights', {}) or {}).get('decision_band', {}) or {}
                band_def = ((config.get('default_strategy', {}) or {}).get('confidence_weights', {}) or {}).get('decision_band', {}) or {}
                band_cfg = band_top if band_top else (band_stg if band_stg else band_def)
                abstain_thr = band_cfg.get('abstain_threshold')
                enter_thr = band_cfg.get('enter_threshold')
                if isinstance(abstain_thr, (int, float)) and isinstance(enter_thr, (int, float)):
                    gray_lower = float(abstain_thr)
                    gray_upper = float(enter_thr)
                else:
                    # å›é€€ï¼šè‹¥æœªé…ç½®å†³ç­–å¸¦ï¼Œä½¿ç”¨å›ºå®šÂ±0.05èŒƒå›´
                    gray_width = 0.05
                    gray_lower = max(0.0, final_threshold - gray_width)
                    gray_upper = min(1.0, final_threshold + gray_width)
                if gray_lower > gray_upper:
                    gray_lower, gray_upper = gray_upper, gray_lower
                gray_lower = max(0.0, min(gray_lower, 1.0))
                gray_upper = max(0.0, min(gray_upper, 1.0))
                gray_mask = (results_df['confidence'] >= gray_lower) & (results_df['confidence'] <= gray_upper)
                gray_df = results_df[gray_mask]
                gray_total = len(gray_df)
                gray_pos = int((gray_df['predicted_low_point'] == True).sum()) if gray_total > 0 else 0
                gray_correct = int((gray_df['prediction_correct'] == True).sum()) if gray_total > 0 else 0

                # ç›¸å…³æ€§ï¼šconfidence ä¸ future_max_riseã€prediction_correct
                corr_final_rise = 0.0
                corr_final_correct = 0.0
                try:
                    tmp = results_df[['confidence', 'future_max_rise']].dropna()
                    if not tmp.empty and tmp['confidence'].nunique() > 1 and tmp['future_max_rise'].nunique() > 1:
                        corr_final_rise = float(tmp['confidence'].corr(tmp['future_max_rise']))
                except Exception:
                    pass
                try:
                    tmp2 = results_df[['confidence', 'prediction_correct']].dropna()
                    if not tmp2.empty and tmp2['confidence'].nunique() > 1 and tmp2['prediction_correct'].nunique() > 1:
                        corr_final_correct = float(tmp2['confidence'].corr(tmp2['prediction_correct'].astype(float)))
                except Exception:
                    pass

                logger.info("\n--- ç½®ä¿¡åº¦åˆ†å¸ƒè¯Šæ–­ ---")
                logger.info(f"confidence: mean={conf_stat['mean']:.4f}, std={conf_stat['std']:.4f}, min={conf_stat['min']:.4f}, max={conf_stat['max']:.4f}")
                logger.info(f"quantiles(10/25/50/75/90): {conf_stat['q10']:.4f} / {conf_stat['q25']:.4f} / {conf_stat['q50']:.4f} / {conf_stat['q75']:.4f} / {conf_stat['q90']:.4f}")
                logger.info(f"åˆ†ç®±(confidence)ï¼š" + ", ".join([f"{lbl}: {bin_counts[lbl]} ({bin_perc[lbl]:.2f}%)" for lbl in bin_labels]))
                logger.info(f"é˜ˆå€¼(final_threshold)={final_threshold:.2f}ï¼Œç°åŒº[{gray_lower:.2f}, {gray_upper:.2f}] è¦†ç›–: {gray_total} æ¡ï¼Œå æ¯” {(gray_total/len(results_df)*100 if len(results_df)>0 else 0):.2f}%ï¼›ç°åŒºä¸­é¢„æµ‹æ­£ç±» {gray_pos} æ¡ï¼Œæ­£ç¡® {gray_correct} æ¡")
                logger.info(f"ç›¸å…³æ€§ï¼šconfidence vs future_max_rise = {corr_final_rise:.3f}ï¼Œconfidence vs prediction_correct = {corr_final_correct:.3f}")
            except Exception as e:
                logger.warning(f"ç½®ä¿¡åº¦åˆ†å¸ƒè¯Šæ–­å¤±è´¥: {e}")

            # æ–°å¢ï¼šæ¦‚ç‡æ ¡å‡†è¯„ä¼°ï¼ˆBrier / LogLoss / ECE & å¯é æ€§è¡¨ï¼‰
            try:
                if not results_df_validated.empty and 'confidence' in results_df_validated.columns and 'actual_low_point' in results_df_validated.columns:
                    y_true_arr = results_df_validated['actual_low_point'].astype(int).values
                    y_prob_arr = results_df_validated['confidence'].astype(float).values
                    mask = ~np.isnan(y_prob_arr)
                    y_true_f = y_true_arr[mask]
                    y_prob_f = np.clip(y_prob_arr[mask], 1e-6, 1 - 1e-6)

                    if y_true_f.size > 0 and np.unique(y_true_f).size > 1:
                        brier_value = float(brier_score_loss(y_true_f, y_prob_f))
                        logloss_value = float(log_loss(y_true_f, y_prob_f))

                        # è®¡ç®— ECE ä¸å¯é æ€§è¡¨ï¼ˆ10 ç­‰å®½åˆ†ç®±ï¼‰
                        def _ece_and_bins(probs: np.ndarray, truths: np.ndarray, n_bins: int = 10):
                            probs = np.clip(np.asarray(probs, dtype=float), 1e-6, 1 - 1e-6)
                            truths = np.asarray(truths, dtype=int)
                            bins = np.linspace(0.0, 1.0, n_bins + 1)
                            bin_ids = np.digitize(probs, bins, right=True) - 1
                            bin_ids = np.clip(bin_ids, 0, n_bins - 1)

                            total_cnt = probs.size
                            ece_sum = 0.0
                            bins_rows = []
                            for k in range(n_bins):
                                lo, hi = bins[k], bins[k+1]
                                idx = (bin_ids == k)
                                cnt = int(idx.sum())
                                if cnt > 0:
                                    avg_conf = float(probs[idx].mean())
                                    acc = float(truths[idx].mean())
                                    gap = abs(acc - avg_conf)
                                    weight = cnt / max(total_cnt, 1)
                                    ece_sum += weight * gap
                                    right_bracket = ')' if k < n_bins - 1 else ']'
                                    bins_rows.append({
                                        'range': f"[{lo:.1f},{hi:.1f}{right_bracket}",
                                        'count': cnt,
                                        'avg_conf': avg_conf,
                                        'acc': acc,
                                        'gap': gap,
                                    })
                                else:
                                    right_bracket = ')' if k < n_bins - 1 else ']'
                                    bins_rows.append({
                                        'range': f"[{lo:.1f},{hi:.1f}{right_bracket}",
                                        'count': 0,
                                        'avg_conf': 0.0,
                                        'acc': 0.0,
                                        'gap': 0.0,
                                    })
                            return float(ece_sum), bins_rows

                        # Original ç›´æ¥ä½œä¸ºåŸºçº¿
                        ece_orig2, bins_orig2 = _ece_and_bins(y_prob_f, y_true_f)
                        ece_value = ece_orig2
                        calib_compare.append({'method': 'åŸå§‹(Original)', 'brier': brier_value, 'logloss': logloss_value, 'ece': ece_orig2})
                        calib_bins_map['original'] = bins_orig2

                        # Platt scalingï¼ˆç”¨æ¦‚ç‡ä½œä¸ºå•ç‰¹å¾è¿›è¡Œé€»è¾‘å›å½’æ ¡å‡†ï¼‰
                        try:
                            X = y_prob_f.reshape(-1, 1)
                            lr = LogisticRegression(solver='lbfgs', max_iter=1000)
                            lr.fit(X, y_true_f)
                            p_platt = lr.predict_proba(X)[:, 1]
                            brier_platt = float(brier_score_loss(y_true_f, p_platt))
                            logloss_platt = float(log_loss(y_true_f, p_platt))
                            ece_platt, bins_platt = _ece_and_bins(p_platt, y_true_f)
                            calib_compare.append({'method': 'Platt(é€»è¾‘å›å½’)', 'brier': brier_platt, 'logloss': logloss_platt, 'ece': ece_platt})
                            calib_bins_map['platt'] = bins_platt
                            logger.info(f"Plattæ ¡å‡†ï¼šBrier={brier_platt:.4f}, LogLoss={logloss_platt:.4f}, ECE(10)={ece_platt:.4f}")
                        except Exception as ce:
                            logger.warning(f"Platt æ ‡å®šå¤±è´¥: {ce}")

                        # Isotonic Regression
                        try:
                            iso = IsotonicRegression(out_of_bounds='clip')
                            p_iso = iso.fit_transform(y_prob_f, y_true_f)
                            brier_iso = float(brier_score_loss(y_true_f, p_iso))
                            logloss_iso = float(log_loss(y_true_f, p_iso))
                            ece_iso, bins_iso = _ece_and_bins(p_iso, y_true_f)
                            calib_compare.append({'method': 'Isotonic(ä¿åºå›å½’)', 'brier': brier_iso, 'logloss': logloss_iso, 'ece': ece_iso})
                            calib_bins_map['isotonic'] = bins_iso
                            logger.info(f"Isotonicæ ¡å‡†ï¼šBrier={brier_iso:.4f}, LogLoss={logloss_iso:.4f}, ECE(10)={ece_iso:.4f}")
                        except Exception as ie:
                            logger.warning(f"Isotonic æ ‡å®šå¤±è´¥: {ie}")
                    else:
                        logger.info("æ¦‚ç‡æ ¡å‡†è¯„ä¼°ï¼šç±»åˆ«å•ä¸€æˆ–æ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡ Brier/LogLoss/ECE è®¡ç®—")
            except Exception as e:
                logger.warning(f"æ¦‚ç‡æ ¡å‡†è¯„ä¼°å¤±è´¥: {e}")

            # è·å–ç­–ç•¥å‚æ•°
            rise_threshold = config.get('strategy', {}).get('rise_threshold', 0.04)
            max_days = config.get('strategy', {}).get('max_days', 20)
            confidence_weights = config.get('strategy', {}).get('confidence_weights', {})
            rsi_oversold = confidence_weights.get('rsi_oversold_threshold', 30)
            rsi_low = confidence_weights.get('rsi_low_threshold', 40)
            # ä»å¤šä¸ªå€™é€‰ä½ç½®æ¨æ–­ final_thresholdï¼ˆç”¨äºæŠ¥å‘Šå±•ç¤ºï¼‰
            final_threshold = resolve_confidence_param(config, 'final_threshold', 0.5)

            # ç”ŸæˆæŠ¥å‘Š
            report_path = None
            if generate_report:
                # ç¡®ä¿æŠ¥å‘Šç›®å½•
                base_results_dir = os.path.join(os.path.dirname(__file__), '..', 'results')
                reports_dir = report_dir or os.path.join(base_results_dir, 'reports')
                if not os.path.exists(reports_dir):
                    os.makedirs(reports_dir)
                ts_str = datetime.now().strftime('%Y%m%d_%H%M%S')
                report_path = os.path.join(reports_dir, f"report_rolling_backtest_{ts_str}.md")

                # é€‰å–å…³é”®ä¿¡å·ï¼ˆé¢„æµ‹ä¸ºæ­£ç±»ï¼‰æŒ‰ç½®ä¿¡åº¦æ’åº
                pos_signals = []
                try:
                    pos_df = results_df_validated[results_df_validated['predicted_low_point'] == True].copy()
                    pos_df = pos_df.sort_values(by=['confidence'], ascending=False)
                    for idx, (dt, row) in enumerate(pos_df.head(15).iterrows(), start=1):
                        pos_signals.append({
                            'index': idx,
                            'date': dt.strftime('%Y-%m-%d'),
                            'predicted': 'æ˜¯' if row.get('predicted_low_point') else 'å¦',
                            'actual': 'æ˜¯' if row.get('actual_low_point') else 'å¦',
                            'confidence': row.get('confidence', 0),
                            'future_max_rise': row.get('future_max_rise', 0),
                            'days_to_rise': f"{int(row.get('days_to_target', 0))}" if not pd.isna(row.get('days_to_target')) else "N/A",
                            'predict_price': row.get('predict_price') if row.get('predict_price') is not None else 'N/A',
                            'correct': 'âœ…' if row.get('prediction_correct') else 'âŒ'
                        })
                except Exception as e:
                    pos_signals = [{'index': 1, 'error': f"ç”Ÿæˆæ ·ä¾‹è¡Œæ—¶å‡ºç°å¼‚å¸¸: {e}"}]

                # æ–°å¢ï¼šå…¨åŒºé—´ Top-Nï¼ˆæŒ‰ confidence é™åºï¼ŒåŒ…å«æœªè¾¾é˜ˆå€¼ï¼‰
                top_all_signals = []
                try:
                    all_df = results_df.copy()
                    all_df = all_df.sort_values(by=['confidence'], ascending=False)
                    for idx, (dt, row) in enumerate(all_df.head(15).iterrows(), start=1):
                        top_all_signals.append({
                            'index': idx,
                            'date': dt.strftime('%Y-%m-%d'),
                            'predicted': 'æ˜¯' if row.get('predicted_low_point') else 'å¦',
                            'actual': 'æ˜¯' if row.get('actual_low_point') else 'å¦',
                            'confidence': row.get('confidence', 0),
                            'future_max_rise': row.get('future_max_rise', 0),
                            'days_to_rise': int(row.get('days_to_target') or 0) if row.get('days_to_target') is not None else 0,
                            'predict_price': row.get('predict_price') if row.get('predict_price') is not None else 'N/A',
                            'correct': ('âœ…' if row.get('prediction_correct') else ('âŒ' if row.get('prediction_correct') is not None else 'N/A'))
                        })
                except Exception as e:
                    top_all_signals = [{'index': 1, 'error': f"ç”ŸæˆTop-Næ—¶å‡ºç°å¼‚å¸¸: {e}"}]

     
                # æŒ‰æœˆä»½ç»Ÿè®¡é¢„æµ‹åˆ†å¸ƒï¼ˆæ›¿ä»£åŸå›¾è¡¨çš„æ—¶é—´è½´ä¿¡æ¯ï¼‰
                monthly_stats = []
                try:
                    monthly_groups = results_df_validated.groupby(results_df_validated.index.to_period('M'))
                    for period, group in monthly_groups:
                        month_total = len(group)
                        month_correct = group['prediction_correct'].sum()
                        month_pos_pred = (group['predicted_low_point'] == True).sum()
                        month_pos_actual = (group['actual_low_point'] == True).sum()
                        monthly_stats.append({
                            'month': str(period),
                            'total': month_total,
                            'correct': month_correct,
                            'success_rate': month_correct / month_total if month_total > 0 else 0,
                            'pred_positive': month_pos_pred,
                            'actual_positive': month_pos_actual
                        })
                except Exception as e:
                    monthly_stats = [{'error': f"æœˆåº¦ç»Ÿè®¡å¼‚å¸¸: {e}"}]

                # æ„å»ºå®Œæ•´æŠ¥å‘Š
                report_lines = []
                report_lines.append(f"# AIæ»šåŠ¨å›æµ‹æŠ¥å‘Š")
                report_lines.append("")
                report_lines.append(f"## åŸºæœ¬ä¿¡æ¯")
                report_lines.append(f"- **å›æµ‹æœŸé—´**: {start_date_str} è‡³ {end_date_str}")
                report_lines.append(f"- **ä½¿ç”¨æ¨¡å‹**: å·²è®­ç»ƒæ¨¡å‹ï¼ˆç¦æ­¢å›æµ‹è®­ç»ƒï¼‰")
                report_lines.append(f"- **ç­–ç•¥å‚æ•°**: rise_threshold={resolve_confidence_param(config, 'rise_threshold', 0.04):.3%}, max_days={config.get('strategy', {}).get('max_days', 20)}")
                report_lines.append(f"- **ç½®ä¿¡åº¦é˜ˆå€¼**: {final_threshold:.3f}")
                report_lines.append(f"- **è®­ç»ƒæ•ˆç‡**: {training_count}/{len(results)} (èŠ‚çœ {((len(results) - training_count) / len(results) * 100):.1f}%)")
                report_lines.append("")
                
                report_lines.append("## æ€»ä½“æŒ‡æ ‡")
                report_lines.append(f"- **æ€»é¢„æµ‹æ—¥æœŸæ•°(å¯éªŒè¯)**: {total_predictions_validated}")
                report_lines.append(f"- **æ­£ç¡®é¢„æµ‹æ•°**: {correct_predictions}")
                report_lines.append(f"- **å‡†ç¡®ç‡(Accuracy)**: {success_rate:.3%}")
                report_lines.append(f"- **ç²¾ç¡®ç‡(Precision)**: {precision:.3%}")
                report_lines.append(f"- **å¬å›ç‡(Recall)**: {recall:.3%}")
                report_lines.append(f"- **F1åˆ†æ•°(F1 Score)**: {(2*precision*recall/max(precision+recall, 1e-12)):.3%}")
                report_lines.append(f"- **ç‰¹å¼‚æ€§(Specificity)**: {specificity:.3%}")
                report_lines.append(f"- **å¹³è¡¡å‡†ç¡®ç‡(Balanced Accuracy)**: {balanced_acc:.3%}")
                report_lines.append("")

                # æ–°å¢ï¼šç½®ä¿¡åº¦åˆ†å¸ƒè¯Šæ–­ï¼ˆå†™å…¥æŠ¥å‘Šï¼‰
                try:
                    report_lines.append("## ç½®ä¿¡åº¦åˆ†å¸ƒè¯Šæ–­")
                    report_lines.append(f"- confidence: å‡å€¼={conf_stat['mean']:.3f}, æ ‡å‡†å·®={conf_stat['std']:.3f}, æœ€å°={conf_stat['min']:.3f}, æœ€å¤§={conf_stat['max']:.3f}")
                    report_lines.append(f"- åˆ†ä½æ•°(10/25/50/75/90): {conf_stat['q10']:.3f} / {conf_stat['q25']:.3f} / {conf_stat['q50']:.3f} / {conf_stat['q75']:.3f} / {conf_stat['q90']:.3f}")
                    report_lines.append("")
                    report_lines.append("### confidence ç›´æ–¹åˆ†å¸ƒ")
                    report_lines.append("| åŒºé—´ | æ•°é‡ | å æ¯” |")
                    report_lines.append("|------|------|------|")
                    for lbl in bin_labels:
                        report_lines.append(f"| {lbl} | {int(bin_counts[lbl])} | {bin_perc[lbl]:.2f}% |")
                    report_lines.append("")
                    report_lines.append(f"- é˜ˆå€¼={final_threshold:.2f}ï¼Œç°åŒº[{gray_lower:.2f}, {gray_upper:.2f}] è¦†ç›–: {gray_total} æ¡ï¼Œå æ¯” {(gray_total/len(results_df)*100 if len(results_df)>0 else 0):.2f}%ï¼›ç°åŒºä¸­é¢„æµ‹æ­£ç±» {gray_pos} æ¡ï¼Œæ­£ç¡® {gray_correct} æ¡")
                    report_lines.append(f"- ç›¸å…³æ€§ï¼šconfidence vs future_max_rise = {corr_final_rise:.3f}ï¼Œconfidence vs prediction_correct = {corr_final_correct:.3f}")
                    report_lines.append("")
                except Exception as e:
                    report_lines.append(f"(ç½®ä¿¡åº¦åˆ†å¸ƒè¯Šæ–­ç”Ÿæˆå¤±è´¥: {e})")
                    report_lines.append("")

                report_lines.append("## æ¦‚ç‡æ ¡å‡†è¯„ä¼°")
                if brier_value is not None:
                    report_lines.append(f"- å¸ƒé‡Œå°”åˆ†æ•°(Brier Score): {brier_value:.4f}ï¼ˆè¶Šä½è¶Šå¥½ï¼‰")
                else:
                    report_lines.append(f"- å¸ƒé‡Œå°”åˆ†æ•°(Brier Score): N/A")
                if logloss_value is not None:
                    report_lines.append(f"- å¯¹æ•°æŸå¤±(Log Loss): {logloss_value:.4f}ï¼ˆè¶Šä½è¶Šå¥½ï¼‰")
                else:
                    report_lines.append(f"- å¯¹æ•°æŸå¤±(Log Loss): N/A")
                if ece_value is not None:
                    report_lines.append(f"- æœŸæœ›æ ¡å‡†è¯¯å·®(ECE,10 bins): {ece_value:.4f}ï¼ˆè¶Šä½è¶Šå¥½ï¼‰")
                else:
                    report_lines.append(f"- æœŸæœ›æ ¡å‡†è¯¯å·®(ECE,10 bins): N/A")
                report_lines.append("")

                # ç¦»çº¿æ¦‚ç‡æ ‡å®šå¯¹æ¯”å®éªŒï¼ˆä¸æ”¹ä¸»é€»è¾‘ï¼Œä»…è¾“å‡ºå¯¹æ¯”è¡¨æ ¼ï¼‰
                if calib_compare:
                    report_lines.append("### ç¦»çº¿æ¦‚ç‡æ ‡å®šå¯¹æ¯”å®éªŒï¼ˆOriginal vs Platt vs Isotonicï¼‰")
                    report_lines.append("| æ–¹æ³• | å¸ƒé‡Œå°”åˆ†æ•°(Brier) | å¯¹æ•°æŸå¤±(LogLoss) | æœŸæœ›æ ¡å‡†è¯¯å·®(ECE,10) |")
                    report_lines.append("|------|------:|--------:|--------:|")
                    for row in calib_compare:
                        report_lines.append(f"| {row['method']} | {row['brier']:.4f} | {row['logloss']:.4f} | {row['ece']:.4f} |")
                    report_lines.append("")
                    # å¯é æ€§ï¼ˆåˆ†ç®±ï¼‰è¡¨ï¼ˆä»…å±•ç¤ºæ¯ç§æ–¹æ³•çš„10ä¸ªåˆ†ç®±ï¼‰
                    for key, title in [( 'original','åŸå§‹(Original)' ), ( 'platt','Platt(é€»è¾‘å›å½’)' ), ( 'isotonic','Isotonic(ä¿åºå›å½’)' )]:
                        bins_rows = calib_bins_map.get(key)
                        if bins_rows:
                            report_lines.append(f"#### å¯é æ€§åˆ†ç®± - {title}")
                            report_lines.append("| ç½®ä¿¡åº¦åŒºé—´ | æ ·æœ¬æ•° | å¹³å‡ç½®ä¿¡åº¦ | å®é™…æ¯”ä¾‹ | ç»å¯¹å·® |")
                            report_lines.append("|-----------:|------:|-----------:|--------:|------:|")
                            for br in bins_rows:
                                report_lines.append(f"| {br['range']} | {br['count']} | {br['avg_conf']:.3f} | {br['acc']:.3f} | {br['gap']:.3f} |")
                            report_lines.append("")

                report_lines.append("## æ¯æ—¥é¢„æµ‹æ˜ç»†")
                report_lines.append("")
                report_lines.append("**å­—æ®µè¯´æ˜ï¼š**")
                report_lines.append("- **ç½®ä¿¡åº¦**: AIæ¨¡å‹è¾“å‡ºçš„é¢„æµ‹æ¦‚ç‡ (0-1)")
                report_lines.append("- **é˜ˆå€¼(used)**: å®é™…ä½¿ç”¨çš„å†³ç­–é˜ˆå€¼ï¼Œå½“AIç½®ä¿¡åº¦â‰¥æ­¤å€¼æ—¶åšå‡ºæ­£å‘é¢„æµ‹")
                report_lines.append("- **ç­–ç•¥è¯¦æƒ…**: æ–‡æœ¬åˆå¹¶ï¼ŒåŒ…å«ç­–ç•¥åŸå› ä¸ç­–ç•¥æŒ‡æ ‡")
                report_lines.append("")
                report_lines.append("| æ—¥æœŸ | é¢„æµ‹ä»·æ ¼ | é¢„æµ‹ç»“æœ | ç½®ä¿¡åº¦ | é˜ˆå€¼(used) | å®é™…ç»“æœ | è¶‹åŠ¿ | æœªæ¥æœ€å¤§æ¶¨å¹… | è¾¾æ ‡ç”¨æ—¶(å¤©) | é¢„æµ‹æ­£ç¡® | ç­–ç•¥è¯¦æƒ… |")
                report_lines.append("|------|----------|----------|--------|------------|------|-------------|-------------|----------|----------|------------|")
                for dt, row in results_df.iterrows():
                    date_str = pd.to_datetime(dt).strftime('%Y-%m-%d') if not pd.isna(dt) else ''
                    pp_val = row.get('predict_price')
                    predict_price = f"{float(pp_val):.3f}" if pp_val is not None and not pd.isna(pp_val) else ''
                    predicted = "æ˜¯" if row.get('predicted_low_point') else "å¦"
                    confidence = f"{row.get('confidence', 0):.3f}"
                    used_threshold = row.get('used_threshold')
                    used_threshold_str = f"{float(used_threshold):.3f}" if used_threshold is not None and not pd.isna(used_threshold) else "N/A"
                    actual = "æ˜¯" if row.get('actual_low_point') else "å¦"
                    # æ–°å¢ï¼šæå–è¶‹åŠ¿çŠ¶æ€
                    trend_str = ''
                    ind = row.get('strategy_indicators')
                    if isinstance(ind, dict):
                        trend_str = ind.get('trend_regime', '')
                    elif isinstance(ind, str):
                        s = ind.strip()
                        if s.startswith('{') and s.endswith('}'):
                            try:
                                d = json.loads(s)
                                if isinstance(d, dict):
                                    trend_str = d.get('trend_regime', '')
                            except Exception:
                                trend_str = ''
                    max_rise = f"{float(row.get('future_max_rise', 0)):.3%}" if not pd.isna(row.get('future_max_rise')) else "N/A"
                    days_to_target = f"{int(row.get('days_to_target', 0))}" if not pd.isna(row.get('days_to_target')) else "N/A"
                    prediction_correct = "æ˜¯" if row.get('prediction_correct') else "å¦"
                    if pd.isna(row.get('actual_low_point')):
                        actual = 'æ•°æ®ä¸è¶³'
                    if pd.isna(row.get('prediction_correct')):
                        prediction_correct = 'æ•°æ®ä¸è¶³'
                    # åˆå¹¶ç­–ç•¥åŸå› ä¸ç­–ç•¥æŒ‡æ ‡ä¸ºæ–‡æœ¬
                    reasons_val = row.get('strategy_reasons')
                    if isinstance(reasons_val, (list, tuple)):
                        reasons_str = ' | '.join(map(str, reasons_val))
                    elif isinstance(reasons_val, str):
                        reasons_str = reasons_val
                    else:
                        reasons_str = ''
                    indicators_val = row.get('strategy_indicators')
                    indicators_kv = ''
                    if isinstance(indicators_val, dict):
                        try:
                            indicators_kv = ', '.join([f"{k}={v}" for k, v in indicators_val.items()])
                        except Exception:
                            indicators_kv = json.dumps(indicators_val, ensure_ascii=False)
                    elif isinstance(indicators_val, str):
                        s = indicators_val.strip()
                        if s.startswith('{') and s.endswith('}'):
                            try:
                                d = json.loads(s)
                                if isinstance(d, dict):
                                    indicators_kv = ', '.join([f"{k}={v}" for k, v in d.items()])
                                else:
                                    indicators_kv = s
                            except Exception:
                                indicators_kv = s
                        else:
                            indicators_kv = s
                    details_parts = []
                    if reasons_str:
                        details_parts.append(f"åŸå› : {reasons_str}")
                    if indicators_kv:
                        details_parts.append(f"æŒ‡æ ‡: {indicators_kv}")
                    details_str = 'ï¼› '.join(details_parts)
                    report_lines.append(f"| {date_str} | {predict_price} | {predicted} | {confidence} | {used_threshold_str} | {actual} | {trend_str} | {max_rise} | {days_to_target} | {prediction_correct} | {details_str} |")
                report_lines.append("")

                # æ–°å¢ï¼šè¶‹åŠ¿åˆ†å¸ƒä¸å‘½ä¸­ç‡ï¼ˆå«éœ‡è¡åŒºé—´ï¼‰
                try:
                    if 'strategy_indicators' in results_df.columns and 'prediction_correct' in results_df.columns:
                        def _extract_trend_for_group(v):
                            if isinstance(v, dict):
                                return v.get('trend_regime', '')
                            if isinstance(v, str):
                                s = v.strip()
                                if s.startswith('{') and s.endswith('}'):
                                    try:
                                        d = json.loads(s)
                                        if isinstance(d, dict):
                                            return d.get('trend_regime', '')
                                    except Exception:
                                        return ''
                            return ''
                        trend_series = results_df['strategy_indicators'].apply(_extract_trend_for_group)
                        correct_series = results_df['prediction_correct'].fillna(False).astype(bool)
                        # ç»Ÿè®¡
                        stats = {}
                        for tr in ['bull', 'sideways', 'bear', '']:
                            mask = (trend_series == tr)
                            cnt = int(mask.sum())
                            if cnt > 0:
                                hit = int(correct_series[mask].sum())
                                rate = hit / cnt if cnt > 0 else 0.0
                                stats[tr if tr else 'unknown'] = (cnt, hit, rate)
                        if stats:
                            report_lines.append("## è¶‹åŠ¿åˆ†å¸ƒä¸å‘½ä¸­ç‡ï¼ˆå«éœ‡è¡åŒºé—´sidewaysï¼‰")
                            report_lines.append("| è¶‹åŠ¿ | æ ·æœ¬æ•° | å‘½ä¸­æ•° | å‘½ä¸­ç‡ |")
                            report_lines.append("|------|------:|------:|------:|")
                            for k in ['bull', 'sideways', 'bear', 'unknown']:
                                if k in stats:
                                    c, h, r = stats[k]
                                    report_lines.append(f"| {k} | {c} | {h} | {r:.3%} |")
                            report_lines.append("")

                            # æ–°å¢ï¼šåˆ†è¶‹åŠ¿çš„ç½®ä¿¡åº¦åˆ†å¸ƒç»Ÿè®¡
                            try:
                                if 'confidence' in results_df.columns and 'strategy_confidence' in results_df.columns:
                                    report_lines.append("### åˆ†è¶‹åŠ¿ç½®ä¿¡åº¦åˆ†å¸ƒç»Ÿè®¡")
                                    report_lines.append("| è¶‹åŠ¿ | æ ·æœ¬æ•° | AIç½®ä¿¡åº¦å‡å€¼ | AIç½®ä¿¡åº¦æ ‡å‡†å·® | ç­–ç•¥ç½®ä¿¡åº¦å‡å€¼ | ç­–ç•¥ç½®ä¿¡åº¦æ ‡å‡†å·® | å‘½ä¸­ç‡ |")
                                    report_lines.append("|------|------:|------------:|-------------:|---------------:|----------------:|------:|")
                                    
                                    ai_conf_series = pd.to_numeric(results_df['confidence'], errors='coerce')
                                    strategy_conf_series = pd.to_numeric(results_df['strategy_confidence'], errors='coerce')
                                    
                                    for k in ['bull', 'sideways', 'bear', 'unknown']:
                                        if k in stats:
                                            mask = (trend_series == ('' if k=='unknown' else k))
                                            cnt = int(mask.sum())
                                            if cnt > 0:
                                                ai_mean = float(ai_conf_series[mask].mean()) if not ai_conf_series[mask].isna().all() else 0.0
                                                ai_std = float(ai_conf_series[mask].std()) if not ai_conf_series[mask].isna().all() else 0.0
                                                strategy_mean = float(strategy_conf_series[mask].mean()) if not strategy_conf_series[mask].isna().all() else 0.0
                                                strategy_std = float(strategy_conf_series[mask].std()) if not strategy_conf_series[mask].isna().all() else 0.0
                                                _, _, hit_rate = stats[k]
                                                report_lines.append(f"| {k} | {cnt} | {ai_mean:.3f} | {ai_std:.3f} | {strategy_mean:.3f} | {strategy_std:.3f} | {hit_rate:.2%} |")
                                    report_lines.append("")
                            except Exception:
                                pass

                            # éœ‡è¡åŒºé—´(sideways)æœ‰æ•ˆæ€§éªŒè¯
                            try:
                                # æå–ä»·æ ¼å’ŒMA20
                                price_series = results_df.get('predict_price')
                                def _extract_ma20(v):
                                    if isinstance(v, dict):
                                        return v.get('ma20', float('nan'))
                                    if isinstance(v, str):
                                        s = v.strip()
                                        if s.startswith('{') and s.endswith('}'):
                                            try:
                                                d = json.loads(s)
                                                if isinstance(d, dict):
                                                    return d.get('ma20', float('nan'))
                                            except Exception:
                                                return float('nan')
                                    return float('nan')
                                ma20_series = results_df['strategy_indicators'].apply(_extract_ma20) if 'strategy_indicators' in results_df.columns else None

                                # è®¡ç®—æ—¥å†…æ³¢åŠ¨(|æ”¶ç›Š|)ä¸ç›¸å¯¹MA20åç¦»
                                vol_series = None
                                if isinstance(price_series, pd.Series):
                                    # æŒ‰ç´¢å¼•æ’åºï¼Œé¿å…é”™ä¹±
                                    price_series = price_series.sort_index()
                                    vol_series = price_series.pct_change().abs()

                                dev_series = None
                                if isinstance(price_series, pd.Series) and isinstance(ma20_series, pd.Series):
                                    with pd.option_context('mode.use_inf_as_na', True):
                                        dev_series = ((price_series - ma20_series).abs() / ma20_series.replace(0, pd.NA)).astype(float)

                                # åˆ†ç»„ç»Ÿè®¡
                                if isinstance(trend_series, pd.Series):
                                    report_lines.append("## éœ‡è¡åŒºé—´æœ‰æ•ˆæ€§éªŒè¯")
                                    report_lines.append("- æŒ‡æ ‡è§£é‡Šï¼š|æ—¥æ”¶ç›Š|ä¸­ä½æ•°ç”¨äºè¡¡é‡æ³¢åŠ¨å¼ºåº¦ï¼›MA20ç›¸å¯¹åç¦»ä¸­ä½æ•°ç”¨äºæµ‹é‡ä»·æ ¼æ˜¯å¦å›´ç»•å‡çº¿æ³¢åŠ¨ï¼›è¿‘å‡çº¿å æ¯”(|åç¦»|â‰¤1%)ç”¨äºåˆ¤æ–­è´´è¿‘å‡çº¿çš„å¤©æ•°å æ¯”ã€‚")
                                    report_lines.append("| è¶‹åŠ¿ | æ ·æœ¬æ•° | |æ—¥æ”¶ç›Š|ä¸­ä½æ•° | MA20ç›¸å¯¹åç¦»ä¸­ä½æ•° | è¿‘å‡çº¿å æ¯”(|åç¦»|â‰¤1%) |")
                                    report_lines.append("|------|------:|-------------:|--------------------:|-----------------------:|")
                                    for k in ['bull', 'sideways', 'bear', 'unknown']:
                                        mask = (trend_series == ('' if k=='unknown' else k))
                                        cnt = int(mask.sum())
                                        if cnt == 0:
                                            continue
                                        vol_med = float(vol_series[mask].median()) if isinstance(vol_series, pd.Series) else float('nan')
                                        dev_med = float(dev_series[mask].median()) if isinstance(dev_series, pd.Series) else float('nan')
                                        near_ma = None
                                        if isinstance(dev_series, pd.Series):
                                            near_ma = float((dev_series[mask] <= 0.01).mean())
                                        report_lines.append(f"| {k} | {cnt} | {vol_med:.3%} | {dev_med:.3%} | {near_ma:.3%} |")
                                    report_lines.append("")
                            except Exception:
                                pass
                except Exception:
                    pass
                report_lines.append("")
                report_lines.append(f"**ç­–ç•¥å‚æ•°**: æ¶¨å¹…é˜ˆå€¼={resolve_confidence_param(config, 'rise_threshold', 0.04):.3%}, æœ€å¤§è§‚å¯Ÿå¤©æ•°={config.get('strategy', {}).get('max_days', 20)}, RSIè¶…å–={config.get('strategy', {}).get('rsi_oversold', 30)}, RSIåä½={config.get('strategy', {}).get('rsi_low', 40)}, ç½®ä¿¡åº¦é˜ˆå€¼={final_threshold:.3f}")
                report_lines.append("")

                report_lines.append("## å…³é”®ä¿¡å·è¯¦æƒ…ï¼ˆæŒ‰ç½®ä¿¡åº¦é™åºï¼Œæœ€å¤š15æ¡ï¼‰")
                report_lines.append("| åºå· | æ—¥æœŸ | é¢„æµ‹ | å®é™… | ç½®ä¿¡åº¦ | æœªæ¥æœ€å¤§æ¶¨å¹… | ç”¨æ—¶å¤©æ•° | é¢„æµ‹ä»· | ç»“æœ |")
                report_lines.append("|------|------|------|------|--------|------------|----------|----------|---------|")
                if len(pos_signals) > 0:
                    for signal in pos_signals:
                        if 'error' in signal:
                            report_lines.append(f"| {signal.get('index', 1)} | - | - | - | - | - | - | - | {signal['error']} |")
                        else:
                            pp_str = (f"{float(signal['predict_price']):.3f}" if isinstance(signal.get('predict_price'), (int, float)) else str(signal.get('predict_price')))
                            report_lines.append(f"| {signal['index']} | {signal['date']} | {signal['predicted']} | {signal['actual']} | {signal['confidence']:.3f} | {signal['future_max_rise']:.3%} | {signal['days_to_rise']} | {pp_str} | {signal['correct']} |")
                else:
                    report_lines.append("- (æœ¬æ¬¡æ— æ­£ç±»ä¿¡å·æˆ–æ— æ³•ç”Ÿæˆæ ·ä¾‹)")
                report_lines.append("")

                # æ–°å¢ï¼šå…¨åŒºé—´ Top-N confidenceï¼ˆåŒ…å«æœªè¾¾é˜ˆå€¼ï¼‰")
                report_lines.append("## å…¨åŒºé—´ Top-N confidenceï¼ˆåŒ…å«æœªè¾¾é˜ˆå€¼ï¼‰")
                report_lines.append("| åºå· | æ—¥æœŸ | é¢„æµ‹ | å®é™… | ç½®ä¿¡åº¦ | æœªæ¥æœ€å¤§æ¶¨å¹… | ç”¨æ—¶å¤©æ•° | é¢„æµ‹ä»· | ç»“æœ |")
                report_lines.append("|------|------|------|------|--------|-------------|----------|---------|------|")
                if len(top_all_signals) > 0:
                    for signal in top_all_signals:
                        if 'error' in signal:
                            report_lines.append(f"| {signal.get('index', 1)} | - | - | - | - | - | - | - | {signal['error']} |")
                        else:
                            pp_str = (f"{float(signal['predict_price']):.3f}" if isinstance(signal.get('predict_price'), (int, float)) else str(signal.get('predict_price')))
                            report_lines.append(f"| {signal['index']} | {signal['date']} | {signal['predicted']} | {signal['actual']} | {signal['confidence']:.3f} | {signal['future_max_rise']:.3%} | {signal['days_to_rise']} | {pp_str} | {signal['correct']} |")
                else:
                    report_lines.append("- (æ— æ³•ç”ŸæˆTop-Nåˆ—è¡¨)")
                report_lines.append("")

                report_lines.append("## ç­–ç•¥å‚æ•°è¯¦æƒ…")
                report_lines.append(f"- **æ¶¨å¹…é˜ˆå€¼**: {resolve_confidence_param(config, 'rise_threshold', 0.04):.3%}")
                report_lines.append(f"- **æœ€å¤§è§‚å¯Ÿå¤©æ•°**: {config.get('strategy', {}).get('max_days', 20)}")
                report_lines.append(f"- **RSIè¶…å–é˜ˆå€¼**: {config.get('strategy', {}).get('rsi_oversold', 30)}")
                report_lines.append(f"- **RSIåä½é˜ˆå€¼**: {config.get('strategy', {}).get('rsi_low', 40)}")
                report_lines.append(f"- **æœ€ç»ˆç½®ä¿¡åº¦é˜ˆå€¼**: {final_threshold:.3f}")
                report_lines.append("")

                report_lines.append("> **å…è´£å£°æ˜**: æœ¬æŠ¥å‘Šç”±è„šæœ¬è‡ªåŠ¨ç”Ÿæˆï¼Œä»…ç”¨äºç­–ç•¥ä¸æ¨¡å‹è¯„ä¼°ï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚")

                with open(report_path, 'w', encoding='utf-8') as f:
                    f.write("\n".join(report_lines))
                logger.info(f"ğŸ“„ å›æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {os.path.relpath(report_path)}")

                # æ–°å¢ï¼šå¯¼å‡ºæ¯æ—¥æ˜ç»†CSVï¼ˆåŒ…å« used_thresholdï¼‰
                try:
                    csv_dir = os.path.join(base_results_dir, 'csv')
                    os.makedirs(csv_dir, exist_ok=True)
                    csv_path = os.path.join(csv_dir, f"daily_details_rolling_backtest_{ts_str}.csv")

                    csv_df = results_df.copy()
                    # æ’å…¥æ—¥æœŸåˆ—ï¼ˆæ ¼å¼åŒ–ï¼‰
                    try:
                        dates = pd.to_datetime(csv_df.index)
                        csv_df.insert(0, 'date', dates.strftime('%Y-%m-%d'))
                    except Exception:
                        csv_df.insert(0, 'date', csv_df.index.astype(str))

                    # è½¬æ¢ç­–ç•¥åŸå› ä¸æŒ‡æ ‡ä¸ºå­—ç¬¦ä¸²ï¼Œä¾¿äºCSVé˜…è¯»
                    if 'strategy_reasons' in csv_df.columns:
                        csv_df['strategy_reasons'] = csv_df['strategy_reasons'].apply(
                            lambda x: ' | '.join(x) if isinstance(x, (list, tuple)) else ('' if x is None else str(x))
                        )
                    # å…ˆä» strategy_indicators æå– trend_regimeï¼ˆåœ¨è½¬å­—ç¬¦ä¸²ä¹‹å‰è¿›è¡Œå±•å¹³ï¼‰
                    try:
                        if 'strategy_indicators' in csv_df.columns:
                            def _extract_trend_regime(v):
                                if isinstance(v, dict):
                                    return v.get('trend_regime', '')
                                # æŸäº›æƒ…å†µä¸‹å¯èƒ½å·²æ˜¯JSONå­—ç¬¦ä¸²
                                if isinstance(v, str):
                                    s = v.strip()
                                    if s.startswith('{') and s.endswith('}'):
                                        try:
                                            d = json.loads(s)
                                            if isinstance(d, dict):
                                                return d.get('trend_regime', '')
                                        except Exception:
                                            return ''
                                return ''
                            csv_df['trend_regime'] = csv_df['strategy_indicators'].apply(_extract_trend_regime)
                    except Exception:
                        # å‡ºé”™æ—¶ä¿æŒåˆ—ä¸ºç©ºï¼Œé¿å…ä¸­æ–­å¯¼å‡º
                        csv_df['trend_regime'] = ''
                    if 'strategy_indicators' in csv_df.columns:
                        csv_df['strategy_indicators'] = csv_df['strategy_indicators'].apply(
                            lambda d: json.dumps(d, ensure_ascii=False) if isinstance(d, dict) else ('' if d is None else str(d))
                        )

                    # æ ¼å¼åŒ–ç½®ä¿¡åº¦ä¿ç•™ä¸¤ä½å°æ•°
                    if 'confidence' in csv_df.columns:
                        try:
                            csv_df['confidence'] = csv_df['confidence'].apply(
                                lambda v: (f"{float(v):.2f}" if (v is not None and not pd.isna(v)) else '')
                            )
                        except Exception:
                            pass

                    # å°†æœªæ¥æœ€å¤§æ¶¨å¹…æ ¼å¼åŒ–ä¸ºä¸¤ä½å°æ•°çš„ç™¾åˆ†æ¯”
                    if 'future_max_rise' in csv_df.columns:
                        try:
                            csv_df['future_max_rise'] = csv_df['future_max_rise'].apply(
                                lambda v: (f"{float(v) * 100:.2f}%" if (v is not None and not pd.isna(v)) else '')
                            )
                        except Exception:
                            pass

                    # ä»…ä¿ç•™å…³å¿ƒçš„åˆ—ï¼ˆè‹¥ç¼ºå¤±åˆ™è‡ªåŠ¨è·³è¿‡ï¼‰
                    preferred_cols = ['date', 'predict_price', 'predicted_low_point', 'confidence',
                                      'used_threshold', 'actual_low_point', 'trend_regime', 'future_max_rise', 'days_to_target', 'prediction_correct']
                    cols = [c for c in preferred_cols if c in csv_df.columns]
                    
                    # åˆ›å»ºä¸­æ–‡åˆ—åæ˜ å°„ï¼ˆç§»é™¤ç­–ç•¥åŸå› ä¸ç­–ç•¥æŒ‡æ ‡ï¼‰
                    chinese_column_mapping = {
                        'date': 'æ—¥æœŸ',
                        'predict_price': 'é¢„æµ‹ä»·æ ¼',
                        'predicted_low_point': 'é¢„æµ‹ä½ç‚¹',
                        'confidence': 'ç½®ä¿¡åº¦',
                        'used_threshold': 'ä½¿ç”¨é˜ˆå€¼',
                        'actual_low_point': 'å®é™…ä½ç‚¹',
                        'trend_regime': 'è¶‹åŠ¿çŠ¶æ€',
                        'future_max_rise': 'æœªæ¥æœ€å¤§æ¶¨å¹…',
                        'days_to_target': 'è¾¾æ ‡ç”¨æ—¶(å¤©)',
                        'prediction_correct': 'é¢„æµ‹æ­£ç¡®'
                    }
                    
                    # é‡å‘½ååˆ—åä¸ºä¸­æ–‡
                    csv_df_chinese = csv_df[cols].copy()
                    csv_df_chinese.columns = [chinese_column_mapping.get(col, col) for col in csv_df_chinese.columns]
                    csv_df_chinese.to_csv(csv_path, index=False, encoding='utf-8-sig')
                    logger.info(f"ğŸ§¾ æ¯æ—¥æ˜ç»†å·²å¯¼å‡ºCSV: {os.path.relpath(csv_path)}")
                except Exception as e:
                    logger.warning(f"å¯¼å‡ºæ¯æ—¥æ˜ç»†CSVå¤±è´¥: {e}")

                # å‡†å¤‡è¿”å›ç»“æœä¸è¾“å‡º
                metrics = {
                    'success_rate': locals().get('success_rate', 0.0),
                    'precision': locals().get('precision', 0.0),
                    'recall': locals().get('recall', 0.0),
                    'specificity': locals().get('specificity', 0.0),
                    'balanced_acc': locals().get('balanced_acc', 0.0),
                    'f1': locals().get('f1', 0.0),
                    'tp': int(locals().get('tp', 0)),
                    'tn': int(locals().get('tn', 0)),
                    'fp': int(locals().get('fp', 0)),
                    'fn': int(locals().get('fn', 0)),
                    'total_predictions': int(locals().get('total_predictions_validated', 0)),
                    'training_count': int(locals().get('training_count', 0))
                }
                return {
                    'success': True,
                    'metrics': metrics,
                    'report_path': report_path
                }
            else:
                # å½“ä¸ç”ŸæˆæŠ¥å‘Šæ—¶ï¼Œä»éœ€è¦è¿”å›ç»“æœ
                metrics = {
                    'success_rate': locals().get('success_rate', 0.0),
                    'precision': locals().get('precision', 0.0),
                    'recall': locals().get('recall', 0.0),
                    'specificity': locals().get('specificity', 0.0),
                    'balanced_acc': locals().get('balanced_acc', 0.0),
                    'f1': locals().get('f1', 0.0),
                    'tp': int(locals().get('tp', 0)),
                    'tn': int(locals().get('tn', 0)),
                    'fp': int(locals().get('fp', 0)),
                    'fn': int(locals().get('fn', 0)),
                    'total_predictions': int(locals().get('total_predictions_validated', 0)),
                    'training_count': int(locals().get('training_count', 0))
                }
                return {
                    'success': True,
                    'metrics': metrics
                }
    except Exception as e:
        logger.error(f"æ»šåŠ¨å›æµ‹å‘ç”Ÿå¼‚å¸¸: {e}")
        return {
            'success': False,
            'error': str(e)
        }



def run_rolling_backtest_with_return(start_date_str: str, end_date_str: str, training_window_days: int = 365,
                                     reuse_model: bool = True, retrain_interval_days: int = None,
                                     generate_report: bool = True, report_dir: str = None):
    """
    å…¼å®¹å…¥å£ï¼šä¸ run_rolling_backtest ç›¸åŒï¼Œåªæ˜¯æ˜¾å¼è¿”å›å…¶ç»“æœï¼Œä¾›ç½‘æ ¼æµ‹è¯•è„šæœ¬è°ƒç”¨ã€‚
    """
    return run_rolling_backtest(
        start_date_str=start_date_str,
        end_date_str=end_date_str,
        training_window_days=training_window_days,
        reuse_model=reuse_model,
        retrain_interval_days=retrain_interval_days,
        generate_report=generate_report,
        report_dir=report_dir,
    )


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='è¿è¡Œæ»šåŠ¨å›æµ‹')
    parser.add_argument('--start_date', required=True, help='å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end_date', required=True, help='ç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--training_window_days', type=int, default=365, help='è®­ç»ƒçª—å£å¤©æ•°')
    parser.add_argument('--reuse_model', action='store_true', default=True, help='æ˜¯å¦é‡ç”¨æ¨¡å‹')
    parser.add_argument('--retrain_interval_days', type=int, help='é‡è®­ç»ƒé—´éš”å¤©æ•°')
    parser.add_argument('--no_report', action='store_true', help='ä¸ç”ŸæˆæŠ¥å‘Š')
    parser.add_argument('--report_dir', help='æŠ¥å‘Šç›®å½•')

    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    result = run_rolling_backtest(
        start_date_str=args.start_date,
        end_date_str=args.end_date,
        training_window_days=args.training_window_days,
        reuse_model=args.reuse_model,
        retrain_interval_days=args.retrain_interval_days,
        generate_report=not args.no_report,
        report_dir=args.report_dir,
    )
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    if result['success']:
        print(format_backtest_summary(result, project_root=project_root))
    else:
        print(f"å›æµ‹å¤±è´¥: {result['error']}")
        sys.exit(1)


