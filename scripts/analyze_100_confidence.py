#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
åˆ†æé«˜ç½®ä¿¡åº¦ä¿¡å·çš„å†å²è¡¨ç°
"""

import sys
import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.append(project_root)

from src.data.data_module import DataModule
from src.ai.ai_optimizer_improved import AIOptimizerImproved
from src.utils.config_loader import load_config
from src.utils.common import LoggerManager

def run_analysis():
    # 1. åˆå§‹åŒ–
    LoggerManager.setup_logging(level=logging.INFO)
    logger = logging.getLogger("Analysis")
    
    config = load_config()
    data_module = DataModule(config)
    ai_optimizer = AIOptimizerImproved(config)
    
    if not ai_optimizer._load_model():
        logger.error("âŒ æœªæ‰¾åˆ°å·²è®­ç»ƒæ¨¡å‹ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒï¼")
        return

    # 2. è·å–æ•°æ® (è¿‘2å¹´)
    end_date = datetime.now()
    start_date = end_date - timedelta(days=730) # 2å¹´
    
    logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®: {start_date.date()} è‡³ {end_date.date()}")
    
    # ä¸ºäº†ä¿è¯ç‰¹å¾è®¡ç®—ï¼ˆå¦‚MA60ï¼‰ï¼Œéœ€è¦é¢å¤–è·å–ä¸€äº›å‰ç½®æ•°æ®
    fetch_start = start_date - timedelta(days=100)
    
    full_data = data_module.get_history_data(
        fetch_start.strftime('%Y-%m-%d'), 
        end_date.strftime('%Y-%m-%d')
    )
    
    if full_data.empty:
        logger.error("æ•°æ®ä¸ºç©º")
        return
        
    full_data = data_module.preprocess_data(full_data)
    
    # è¿‡æ»¤å‡ºåˆ†æåŒºé—´çš„æ•°æ®ç´¢å¼•
    analysis_data = full_data[full_data['date'] >= start_date]
    if analysis_data.empty:
        logger.error("åˆ†æåŒºé—´å†…æ— æ•°æ®")
        return
        
    start_idx = analysis_data.index[0]
    end_idx = full_data.index[-1]
    
    logger.info(f"å¼€å§‹æ‰«æ {len(analysis_data)} ä¸ªäº¤æ˜“æ—¥...")
    
    signals = []
    
    # 3. é€æ—¥æ‰«æ
    # æ³¨æ„ï¼šæˆ‘ä»¬ä¸èƒ½ä½¿ç”¨æœªæ¥æ•°æ®ã€‚å¯¹äºæ¯ä¸€å¤© Tï¼Œåªèƒ½ä½¿ç”¨ T åŠä¹‹å‰çš„æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚
    # predict_low_point æ¥å—çš„æ˜¯ä¸€ä¸ª DataFrameï¼Œå®ƒé€šå¸¸å–æœ€åä¸€è¡Œä½œä¸º Tã€‚
    
    # è¿™é‡Œçš„å¾ªç¯å¯èƒ½ä¼šæ¯”è¾ƒæ…¢ï¼Œå› ä¸ºæ¯å¤©éƒ½è¦åˆ‡ç‰‡ã€‚
    # ä¼˜åŒ–ï¼šä¸éœ€æ¯æ¬¡åˆ‡ç‰‡æ‰€æœ‰å†å²ï¼Œåªéœ€ä¿ç•™è¶³å¤Ÿè®¡ç®—ç‰¹å¾çš„çª—å£å³å¯ã€‚
    # ä½† predict_low_point å†…éƒ¨å¯èƒ½ä¾èµ– rolling è®¡ç®—ï¼Œå¦‚æœä¼ å…¥å¤ªçŸ­å¯èƒ½ä¸å‡†ã€‚
    # ä¸è¿‡ full_data å·²ç»æ˜¯é¢„å¤„ç†è¿‡çš„ï¼ˆç‰¹å¾å·²è®¡ç®—å¥½ï¼‰ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥å–å•è¡Œï¼Ÿ
    # AIOptimizerImproved.predict_low_point å†…éƒ¨é€»è¾‘ï¼š
    # latest_features = features[-1:].reshape(1, -1)
    # å®ƒä¼šé‡æ–°æå–ç‰¹å¾ã€‚å¦‚æœä¼ å…¥çš„æ˜¯å•è¡Œï¼Œç‰¹å¾æå–å¯èƒ½ä¼šå¤±è´¥ï¼ˆå› ä¸ºç¼ºå†å²ï¼‰ã€‚
    # ä½†æ˜¯ï¼Œæˆ‘ä»¬å·²ç»å¯¹ full_data åšäº† preprocess_dataï¼Œç‰¹å¾åˆ—ï¼ˆå¦‚ rsi, ma5ï¼‰å·²ç»å­˜åœ¨äº†ã€‚
    # æ£€æŸ¥ predict_low_point æºç ï¼šå®ƒä¼šè°ƒç”¨ self.feature_engineer.create_features(data)ã€‚
    # å¦‚æœ data å·²ç»æœ‰ç‰¹å¾åˆ—ï¼Œcreate_features ä¼šæ€ä¹ˆå¤„ç†ï¼Ÿ
    # é€šå¸¸å®ƒä¼šé‡æ–°è®¡ç®—ã€‚å¦‚æœåªä¼ ä¸€è¡Œï¼Œrolling è®¡ç®—ä¼šå˜æˆ NaNã€‚
    
    # æ‰€ä»¥å¿…é¡»ä¼ å…¥å†å²çª—å£ã€‚
    
    window_size = 60 # è¶³å¤Ÿè®¡ç®— MA60 ç­‰æŒ‡æ ‡
    
    for i in range(start_idx, end_idx - 10): # ç•™10å¤©çœ‹ç»“æœ
        current_date = full_data.loc[i, 'date']
        
        # æ„é€ å†å²çª—å£
        window_start = max(0, i - window_size)
        history_slice = full_data.iloc[window_start : i+1].copy()
        
        # é¢„æµ‹
        # ç¦ç”¨æ—¥å¿—ä»¥é˜²åˆ·å±
        logging.getLogger("AIOptimizer").setLevel(logging.WARNING)
        
        try:
            res = ai_optimizer.predict_low_point(history_slice)
            confidence = res.get('confidence', 0.0)
            
            # é™ä½é˜ˆå€¼ï¼Œåˆ†ææ–°æ¨¡å‹çš„é«˜ç½®ä¿¡åº¦åŒºåŸŸ
            if confidence >= 0.50: 
                # è®°å½•ä¿¡å·
                # T+1 å¼€ç›˜ä¹°å…¥
                entry_price = full_data.loc[i+1, 'open']
                # T+10 æ”¶ç›˜å–å‡º (æˆ–è€…æŒæœ‰10å¤©åçš„æ”¶ç›˜ä»·)
                exit_idx = i + 1 + 10
                if exit_idx < len(full_data):
                    exit_price = full_data.loc[exit_idx, 'close']
                    ret = (exit_price - entry_price) / entry_price
                    
                    signals.append({
                        'date': current_date.strftime('%Y-%m-%d'),
                        'confidence': confidence,
                        'entry_price': entry_price,
                        'exit_price': exit_price,
                        'return': ret
                    })
                    print(f"âœ… å‘ç°ä¿¡å·: {current_date.strftime('%Y-%m-%d')} | ç½®ä¿¡åº¦: {confidence:.2f} | æ”¶ç›Š: {ret:+.2%}")
                    
        except Exception as e:
            # logger.error(f"Error on {current_date}: {e}")
            pass

    # 4. ç»Ÿè®¡ç»“æœ
    print("\n" + "="*40)
    print("ğŸ“Š é«˜ç½®ä¿¡åº¦ä¿¡å·å†å²è¡¨ç° (Confidence >= 0.50)")
    print("="*40)
    
    if not signals:
        print("æœªå‘ç°ä»»ä½•ç½®ä¿¡åº¦ >= 0.50 çš„ä¿¡å·ã€‚")
        return

    df = pd.DataFrame(signals)
    df = df.sort_values('confidence', ascending=False) # æŒ‰ç½®ä¿¡åº¦æ’åº
    
    win_rate = len(df[df['return'] > 0]) / len(df)
    avg_ret = df['return'].mean()
    max_ret = df['return'].max()
    min_ret = df['return'].min()
    
    print(f"ä¿¡å·æ€»æ•°: {len(df)}")
    print(f"èƒœç‡:     {win_rate:.2%}")
    print(f"å¹³å‡æ”¶ç›Š: {avg_ret:+.2%} (10å¤©æŒæœ‰)")
    print(f"æœ€å¤§æ”¶ç›Š: {max_ret:+.2%}")
    print(f"æœ€å¤§äºæŸ: {min_ret:+.2%}")
    print("-" * 40)
    print("è¯¦ç»†è®°å½• (Top 20):")
    print(df.head(20).to_string(index=False))

if __name__ == "__main__":
    run_analysis()
