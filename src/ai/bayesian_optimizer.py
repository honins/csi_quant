#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
è´å¶æ–¯ä¼˜åŒ–æ¨¡å—
è´Ÿè´£ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œæ™ºèƒ½å‚æ•°æœç´¢
"""

import logging
import numpy as np
import pandas as pd
from typing import Dict, Any, List

# è´å¶æ–¯ä¼˜åŒ–ç›¸å…³å¯¼å…¥
try:
    from skopt import gp_minimize
    from skopt.space import Real, Integer
    from skopt.utils import use_named_args
    BAYESIAN_AVAILABLE = True
except ImportError:
    BAYESIAN_AVAILABLE = False


class BayesianOptimizer:
    """è´å¶æ–¯ä¼˜åŒ–å™¨ç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è´å¶æ–¯ä¼˜åŒ–å™¨
        
        å‚æ•°:
        config: é…ç½®ä¿¡æ¯
        """
        self.config = config
        self.logger = logging.getLogger(__name__)

    def is_available(self) -> bool:
        """
        æ£€æŸ¥è´å¶æ–¯ä¼˜åŒ–æ˜¯å¦å¯ç”¨
        
        è¿”å›:
        bool: æ˜¯å¦å¯ç”¨
        """
        return BAYESIAN_AVAILABLE

    def optimize_parameters(self, data: pd.DataFrame, objective_func, 
                          current_params: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œå‚æ•°æœç´¢
        
        å‚æ•°:
        data: å†å²æ•°æ®
        objective_func: ç›®æ ‡å‡½æ•°ï¼Œæ¥å—å‚æ•°å­—å…¸å¹¶è¿”å›å¾—åˆ†
        current_params: å½“å‰å‚æ•°ï¼Œç”¨äºæ„å»ºæ™ºèƒ½æœç´¢èŒƒå›´
        
        è¿”å›:
        dict: ä¼˜åŒ–ç»“æœ
        """
        self.logger.info("ğŸ” å¼€å§‹è´å¶æ–¯ä¼˜åŒ–å‚æ•°æœç´¢...")
        
        if not BAYESIAN_AVAILABLE:
            self.logger.error("âŒ scikit-optimizeæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–")
            return {'success': False, 'error': 'scikit-optimizeæœªå®‰è£…'}
        
        try:
            # è·å–è´å¶æ–¯ä¼˜åŒ–é…ç½®
            ai_config = self.config.get('ai', {})
            bayesian_config = ai_config.get('bayesian_optimization', {})
            
            if not bayesian_config.get('enabled', True):
                self.logger.info("è´å¶æ–¯ä¼˜åŒ–å·²ç¦ç”¨ï¼Œè·³è¿‡")
                return {'success': False, 'error': 'è´å¶æ–¯ä¼˜åŒ–å·²ç¦ç”¨'}
            
            # é…ç½®å‚æ•°
            n_calls = bayesian_config.get('n_calls', 100)
            n_initial_points = bayesian_config.get('n_initial_points', 20)
            acq_func = bayesian_config.get('acq_func', 'EI')
            xi = bayesian_config.get('xi', 0.01)
            kappa = bayesian_config.get('kappa', 1.96)
            n_jobs = bayesian_config.get('n_jobs', 1)
            random_state = bayesian_config.get('random_state', 42)
            
            self.logger.info(f"è´å¶æ–¯ä¼˜åŒ–é…ç½®:")
            self.logger.info(f"  - è°ƒç”¨æ¬¡æ•°: {n_calls}")
            self.logger.info(f"  - åˆå§‹ç‚¹æ•°: {n_initial_points}")
            self.logger.info(f"  - é‡‡é›†å‡½æ•°: {acq_func}")
            
            # å®šä¹‰åŸºäºå½“å‰å‚æ•°çš„æ™ºèƒ½æœç´¢ç©ºé—´
            dimensions, param_names = self._build_adaptive_parameter_space(current_params)
            
            if len(dimensions) == 0:
                self.logger.error("âŒ æœªå®šä¹‰ä¼˜åŒ–å‚æ•°ç©ºé—´")
                return {'success': False, 'error': 'æœªå®šä¹‰ä¼˜åŒ–å‚æ•°ç©ºé—´'}
            
            self.logger.info(f"å‚æ•°ç©ºé—´ç»´åº¦: {len(dimensions)}")
            for i, dim in enumerate(dimensions):
                self.logger.info(f"  - {param_names[i]}: [{dim.low}, {dim.high}]")
            
            # è®°å½•è¯„ä¼°å†å²
            evaluation_history = []
            
            @use_named_args(dimensions)
            def objective(**params):
                """ç›®æ ‡å‡½æ•°ï¼šæœ€å°åŒ–è´Ÿå¾—åˆ†ï¼ˆå› ä¸ºgp_minimizeæ˜¯æœ€å°åŒ–ï¼‰"""
                try:
                    # è°ƒç”¨å¤–éƒ¨ç›®æ ‡å‡½æ•°
                    score = objective_func(params)
                    
                    # è®°å½•è¯„ä¼°å†å²
                    evaluation_history.append({
                        'params': params.copy(),
                        'score': score,
                        'iteration': len(evaluation_history) + 1
                    })
                    
                    if len(evaluation_history) % 10 == 0:
                        self.logger.info(f"è´å¶æ–¯ä¼˜åŒ–è¿›åº¦: {len(evaluation_history)}/{n_calls}, å½“å‰å¾—åˆ†: {score:.4f}")
                    
                    # è¿”å›è´Ÿå¾—åˆ†ï¼ˆå› ä¸ºè¦æœ€å°åŒ–ï¼‰
                    return -score
                    
                except Exception as e:
                    self.logger.error(f"ç›®æ ‡å‡½æ•°è¯„ä¼°å¤±è´¥: {str(e)}")
                    return 1.0  # è¿”å›æœ€å·®å¾—åˆ†
            
            # è¿è¡Œè´å¶æ–¯ä¼˜åŒ–
            self.logger.info("ğŸš€ å¼€å§‹è´å¶æ–¯ä¼˜åŒ–...")
            
            # æ ¹æ®é‡‡é›†å‡½æ•°è®¾ç½®å‚æ•°
            gp_kwargs = {
                'func': objective,
                'dimensions': dimensions,
                'n_calls': n_calls,
                'n_initial_points': n_initial_points,
                'acq_func': acq_func,
                'random_state': random_state,
                'n_jobs': n_jobs,
                'verbose': False
            }
            
            # æ ¹æ®é‡‡é›†å‡½æ•°ç±»å‹æ·»åŠ ç‰¹å®šå‚æ•°
            if acq_func == 'EI':
                gp_kwargs['xi'] = xi
            elif acq_func == 'LCB':
                gp_kwargs['kappa'] = kappa
            
            result = gp_minimize(**gp_kwargs)
            
            # æå–æœ€ä¼˜å‚æ•°
            best_params = {}
            for i, param_name in enumerate(param_names):
                best_params[param_name] = result.x[i]
            
            best_score = -result.fun  # è½¬æ¢å›æ­£å¾—åˆ†
            
            self.logger.info("âœ… è´å¶æ–¯ä¼˜åŒ–å®Œæˆ")
            self.logger.info(f"   - æœ€ä¼˜å¾—åˆ†: {best_score:.4f}")
            self.logger.info(f"   - æ€»è¯„ä¼°æ¬¡æ•°: {len(evaluation_history)}")
            self.logger.info(f"   - æœ€ä¼˜å‚æ•°:")
            for param, value in best_params.items():
                self.logger.info(f"     - {param}: {value:.4f}")
            
            # åˆ†ææ”¶æ•›æƒ…å†µ
            scores = [eval_record['score'] for eval_record in evaluation_history]
            best_scores = np.maximum.accumulate(scores)
            improvement_rate = (best_scores[-1] - best_scores[n_initial_points]) / max(best_scores[n_initial_points], 0.001)
            
            self.logger.info(f"   - æ”¹è¿›ç‡: {improvement_rate:.2%}")
            self.logger.info(f"   - æœ€ç»ˆæ”¶æ•›å¾—åˆ†: {best_scores[-1]:.4f}")
            
            return {
                'success': True,
                'best_params': best_params,
                'best_score': best_score,
                'n_evaluations': len(evaluation_history),
                'improvement_rate': improvement_rate,
                'evaluation_history': evaluation_history,
                'convergence_scores': best_scores.tolist(),
                'optimization_result': result
            }
            
        except Exception as e:
            self.logger.error(f"âŒ è´å¶æ–¯ä¼˜åŒ–å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}

    def _build_adaptive_parameter_space(self, current_params: Dict[str, Any]) -> tuple:
        """
        åŸºäºå½“å‰å‚æ•°æ„å»ºè‡ªé€‚åº”å‚æ•°ç©ºé—´
        
        å‚æ•°:
        current_params: å½“å‰æœ€ä¼˜å‚æ•°
        
        è¿”å›:
        tuple: (dimensionsåˆ—è¡¨, å‚æ•°ååˆ—è¡¨)
        """
        dimensions = []
        param_names = []
        
        self.logger.info("ğŸ¯ æ„å»ºåŸºäºå½“å‰å‚æ•°çš„è‡ªé€‚åº”æœç´¢ç©ºé—´...")
        
        # ä»é…ç½®ä¸­è¯»å–åŸºç¡€å‚æ•°èŒƒå›´
        optimization_ranges = self.config.get('ai', {}).get('optimization_ranges', {})
        
        # æ™ºèƒ½æœç´¢åŠå¾„
        search_factor = self.config.get('ai', {}).get('bayesian_optimization', {}).get('search_factor', 0.3)
        
        for param_name, param_range in optimization_ranges.items():
            base_min = param_range.get('min', 0.0)
            base_max = param_range.get('max', 1.0)
            current_value = current_params.get(param_name, (base_min + base_max) / 2)
            
            # åŸºäºå½“å‰å€¼åŠ¨æ€è°ƒæ•´æœç´¢èŒƒå›´
            range_width = base_max - base_min
            adaptive_radius = range_width * search_factor
            
            adaptive_min = max(base_min, current_value - adaptive_radius)
            adaptive_max = min(base_max, current_value + adaptive_radius)
            
            dimensions.append(Real(adaptive_min, adaptive_max, name=param_name))
            param_names.append(param_name)
            
            self.logger.info(f"   - {param_name}: å½“å‰å€¼ {current_value:.3f}, æœç´¢èŒƒå›´ [{adaptive_min:.3f}, {adaptive_max:.3f}]")
        
        # RSIç›¸å…³å‚æ•°çš„è‡ªé€‚åº”èŒƒå›´
        base_rsi_oversold = current_params.get('rsi_oversold_threshold', 30)
        base_rsi_low = current_params.get('rsi_low_threshold', 40)
        base_final_threshold = current_params.get('final_threshold', 0.5)
        
        # RSIå‚æ•°æœç´¢åŠå¾„
        rsi_radius = 4  # RSIå‚æ•°çš„æœç´¢åŠå¾„
        threshold_radius = 0.15  # final_thresholdçš„æœç´¢åŠå¾„
        
        # è‡ªé€‚åº”RSI oversoldèŒƒå›´
        rsi_oversold_min = max(25, base_rsi_oversold - rsi_radius)
        rsi_oversold_max = min(35, base_rsi_oversold + rsi_radius)
        
        # è‡ªé€‚åº”RSI lowèŒƒå›´
        rsi_low_min = max(35, base_rsi_low - rsi_radius)
        rsi_low_max = min(45, base_rsi_low + rsi_radius)
        
        # è‡ªé€‚åº”final_thresholdèŒƒå›´
        final_threshold_min = max(0.3, base_final_threshold - threshold_radius)
        final_threshold_max = min(0.7, base_final_threshold + threshold_radius)
        
        dimensions.extend([
            Integer(rsi_oversold_min, rsi_oversold_max, name='rsi_oversold_threshold'),
            Integer(rsi_low_min, rsi_low_max, name='rsi_low_threshold'),
            Real(final_threshold_min, final_threshold_max, name='final_threshold')
        ])
        param_names.extend(['rsi_oversold_threshold', 'rsi_low_threshold', 'final_threshold'])
        
        self.logger.info(f"   - rsi_oversold_threshold: å½“å‰å€¼ {base_rsi_oversold}, æœç´¢èŒƒå›´ [{rsi_oversold_min}, {rsi_oversold_max}]")
        self.logger.info(f"   - rsi_low_threshold: å½“å‰å€¼ {base_rsi_low}, æœç´¢èŒƒå›´ [{rsi_low_min}, {rsi_low_max}]")
        self.logger.info(f"   - final_threshold: å½“å‰å€¼ {base_final_threshold:.3f}, æœç´¢èŒƒå›´ [{final_threshold_min:.3f}, {final_threshold_max:.3f}]")
        
        return dimensions, param_names

    def _build_parameter_space(self, param_ranges: Dict[str, Dict[str, Any]]) -> tuple:
        """
        æ„å»ºä¼ ç»Ÿçš„å›ºå®šå‚æ•°ç©ºé—´ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        
        å‚æ•°:
        param_ranges: å‚æ•°èŒƒå›´é…ç½®
        
        è¿”å›:
        tuple: (dimensionsåˆ—è¡¨, å‚æ•°ååˆ—è¡¨)
        """
        dimensions = []
        param_names = []
        
        # ä»é…ç½®ä¸­è¯»å–å‚æ•°èŒƒå›´
        optimization_ranges = self.config.get('ai', {}).get('optimization_ranges', {})
        
        for param_name, param_range in optimization_ranges.items():
            min_val = param_range.get('min', 0.0)
            max_val = param_range.get('max', 1.0)
            
            dimensions.append(Real(min_val, max_val, name=param_name))
            param_names.append(param_name)
        
        # æ·»åŠ RSIç›¸å…³å‚æ•°
        dimensions.extend([
            Integer(25, 35, name='rsi_oversold_threshold'),
            Integer(35, 45, name='rsi_low_threshold'),
            Real(0.3, 0.7, name='final_threshold')
        ])
        param_names.extend(['rsi_oversold_threshold', 'rsi_low_threshold', 'final_threshold'])
        
        return dimensions, param_names 