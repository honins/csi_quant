#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
è¿‡æ‹Ÿåˆæ£€æµ‹æ¨¡å—
æä¾›å¤šç§è¿‡æ‹Ÿåˆæ£€æµ‹æ–¹æ³•ï¼Œç¡®ä¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›
"""

import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Optional



class OverfittingDetector:
    """è¿‡æ‹Ÿåˆæ£€æµ‹å™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # è¿‡æ‹Ÿåˆæ£€æµ‹é˜ˆå€¼
        self.validation_threshold = config.get('validation', {}).get('overfitting_threshold', 0.9)
        self.confidence_std_threshold = config.get('validation', {}).get('confidence_std_threshold', 0.05)
        self.zero_confidence_threshold = config.get('validation', {}).get('zero_confidence_threshold', 0.5)
        
    def detect_overfitting(self, 
                          train_score: float,
                          val_score: float,
                          test_score: Optional[float] = None,
                          val_predictions: List[float] = None,
                          train_predictions: List[float] = None) -> Dict:
        """
        ç»¼åˆè¿‡æ‹Ÿåˆæ£€æµ‹
        
        å‚æ•°:
        train_score: è®­ç»ƒé›†å¾—åˆ†
        val_score: éªŒè¯é›†å¾—åˆ†
        test_score: æµ‹è¯•é›†å¾—åˆ†ï¼ˆå¯é€‰ï¼‰
        val_predictions: éªŒè¯é›†é¢„æµ‹ç½®ä¿¡åº¦åˆ—è¡¨
        train_predictions: è®­ç»ƒé›†é¢„æµ‹ç½®ä¿¡åº¦åˆ—è¡¨
        
        è¿”å›:
        dict: æ£€æµ‹ç»“æœ
        """
        self.logger.info("ğŸ” å¼€å§‹ç»¼åˆè¿‡æ‹Ÿåˆæ£€æµ‹...")
        
        results = {
            'overfitting_detected': False,
            'warnings': [],
            'metrics': {},
            'recommendations': []
        }
        
        # 1. åŸºç¡€å¾—åˆ†å·®å¼‚æ£€æµ‹
        score_check = self._check_score_degradation(train_score, val_score, test_score)
        results['metrics'].update(score_check['metrics'])
        
        if score_check['overfitting']:
            results['overfitting_detected'] = True
            results['warnings'].extend(score_check['warnings'])
            results['recommendations'].extend(score_check['recommendations'])
        
        # 2. ç½®ä¿¡åº¦åˆ†å¸ƒæ£€æµ‹
        if val_predictions is not None:
            confidence_check = self._check_confidence_distribution(val_predictions, train_predictions)
            results['metrics'].update(confidence_check['metrics'])
            
            if confidence_check['overfitting']:
                results['overfitting_detected'] = True
                results['warnings'].extend(confidence_check['warnings'])
                results['recommendations'].extend(confidence_check['recommendations'])
        
        # 3. å­¦ä¹ æ›²çº¿æ£€æµ‹ï¼ˆå¦‚æœæœ‰å†å²æ•°æ®ï¼‰
        learning_curve_check = self._check_learning_curve_pattern()
        if learning_curve_check['overfitting']:
            results['overfitting_detected'] = True
            results['warnings'].extend(learning_curve_check['warnings'])
            results['recommendations'].extend(learning_curve_check['recommendations'])
        
        # è¾“å‡ºæ£€æµ‹ç»“æœ
        self._log_detection_results(results)
        
        return results
    
    def _check_score_degradation(self, train_score: float, val_score: float, test_score: Optional[float] = None) -> Dict:
        """æ£€æµ‹å¾—åˆ†é€€åŒ–"""
        results = {
            'overfitting': False,
            'warnings': [],
            'recommendations': [],
            'metrics': {
                'train_score': train_score,
                'val_score': val_score,
                'score_ratio': val_score / train_score if train_score > 0 else 0
            }
        }
        
        # éªŒè¯é›†vsè®­ç»ƒé›†å¾—åˆ†æ¯”ç‡
        score_ratio = val_score / train_score if train_score > 0 else 0
        
        if score_ratio < self.validation_threshold:
            results['overfitting'] = True
            results['warnings'].append(f"éªŒè¯é›†å¾—åˆ†æ˜æ˜¾ä½äºè®­ç»ƒé›†: {val_score:.4f} vs {train_score:.4f} (æ¯”ç‡: {score_ratio:.3f})")
            results['recommendations'].append("å‡å°‘æ¨¡å‹å¤æ‚åº¦ï¼Œå¢åŠ æ­£åˆ™åŒ–")
        
        # å¦‚æœæœ‰æµ‹è¯•é›†å¾—åˆ†ï¼Œè¿›è¡Œé¢å¤–æ£€æµ‹
        if test_score is not None:
            results['metrics']['test_score'] = test_score
            val_test_ratio = test_score / val_score if val_score > 0 else 0
            results['metrics']['val_test_ratio'] = val_test_ratio
            
            if val_test_ratio < 0.85:  # æµ‹è¯•é›†å¾—åˆ†åº”è¯¥æ¥è¿‘éªŒè¯é›†
                results['overfitting'] = True
                results['warnings'].append(f"æµ‹è¯•é›†å¾—åˆ†æ˜¾è‘—ä½äºéªŒè¯é›†: {test_score:.4f} vs {val_score:.4f}")
                results['recommendations'].append("æ¨¡å‹å¯èƒ½è¿‡æ‹ŸåˆéªŒè¯é›†ï¼Œå»ºè®®é‡æ–°è®¾è®¡éªŒè¯ç­–ç•¥")
        
        return results
    
    def _check_confidence_distribution(self, val_predictions: List[float], train_predictions: List[float] = None) -> Dict:
        """æ£€æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒå¼‚å¸¸"""
        results = {
            'overfitting': False,
            'warnings': [],
            'recommendations': [],
            'metrics': {}
        }
        
        val_array = np.array(val_predictions)
        
        # è®¡ç®—ç½®ä¿¡åº¦ç»Ÿè®¡
        val_std = np.std(val_array)
        val_mean = np.mean(val_array)
        zero_ratio = np.sum(val_array == 0.0) / len(val_array)
        extreme_ratio = np.sum((val_array == 0.0) | (val_array == 1.0)) / len(val_array)
        
        results['metrics'].update({
            'val_confidence_std': val_std,
            'val_confidence_mean': val_mean,
            'zero_confidence_ratio': zero_ratio,
            'extreme_confidence_ratio': extreme_ratio
        })
        
        # æ£€æµ‹ç½®ä¿¡åº¦æ ‡å‡†å·®è¿‡å°
        if val_std < self.confidence_std_threshold:
            results['overfitting'] = True
            results['warnings'].append(f"éªŒè¯é›†ç½®ä¿¡åº¦æ ‡å‡†å·®è¿‡å°: {val_std:.4f}")
            results['recommendations'].append("æ¨¡å‹è¾“å‡ºè¿‡äºæç«¯ï¼Œå¢åŠ æ¨¡å‹æ­£åˆ™åŒ–æˆ–å‡å°‘ç‰¹å¾æ•°é‡")
        
        # æ£€æµ‹é›¶ç½®ä¿¡åº¦æ¯”ä¾‹è¿‡é«˜
        if zero_ratio > self.zero_confidence_threshold:
            results['overfitting'] = True
            results['warnings'].append(f"éªŒè¯é›†é›¶ç½®ä¿¡åº¦æ¯”ä¾‹è¿‡é«˜: {zero_ratio:.1%}")
            results['recommendations'].append("æ¨¡å‹å¯èƒ½è¿‡åº¦æ‹Ÿåˆè®­ç»ƒæ•°æ®æ¨¡å¼ï¼Œå»ºè®®é‡æ–°è®­ç»ƒ")
        
        # æ£€æµ‹æç«¯ç½®ä¿¡åº¦æ¯”ä¾‹
        if extreme_ratio > 0.8:
            results['overfitting'] = True
            results['warnings'].append(f"éªŒè¯é›†æç«¯ç½®ä¿¡åº¦(0æˆ–1)æ¯”ä¾‹è¿‡é«˜: {extreme_ratio:.1%}")
            results['recommendations'].append("æ¨¡å‹è¿‡äºè‡ªä¿¡ï¼Œç¼ºä¹æ³›åŒ–èƒ½åŠ›")
        
        # å¦‚æœæœ‰è®­ç»ƒé›†ç½®ä¿¡åº¦ï¼Œè¿›è¡Œå¯¹æ¯”åˆ†æ
        if train_predictions is not None:
            train_array = np.array(train_predictions)
            train_std = np.std(train_array)
            train_mean = np.mean(train_array)
            
            results['metrics'].update({
                'train_confidence_std': train_std,
                'train_confidence_mean': train_mean,
                'std_ratio': val_std / train_std if train_std > 0 else 0
            })
            
            # è®­ç»ƒé›†å’ŒéªŒè¯é›†ç½®ä¿¡åº¦åˆ†å¸ƒå·®å¼‚
            if train_std > 0 and val_std / train_std < 0.5:
                results['overfitting'] = True
                results['warnings'].append(f"éªŒè¯é›†ç½®ä¿¡åº¦æ–¹å·®æ˜¾è‘—å°äºè®­ç»ƒé›†: {val_std:.4f} vs {train_std:.4f}")
                results['recommendations'].append("æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šè¡¨ç°å¼‚å¸¸ä¸€è‡´ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ")
        
        return results
    
    def _check_learning_curve_pattern(self) -> Dict:
        """æ£€æµ‹å­¦ä¹ æ›²çº¿æ¨¡å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        results = {
            'overfitting': False,
            'warnings': [],
            'recommendations': []
        }
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ å­¦ä¹ æ›²çº¿åˆ†æ
        # æš‚æ—¶è¿”å›ç©ºç»“æœ
        return results
    
    def _log_detection_results(self, results: Dict):
        """è®°å½•æ£€æµ‹ç»“æœ"""
        if results['overfitting_detected']:
            self.logger.warning("ğŸš¨ æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆé£é™©ï¼")
            for warning in results['warnings']:
                self.logger.warning(f"   âš ï¸ {warning}")
            
            self.logger.info("ğŸ’¡ å»ºè®®æªæ–½:")
            for rec in results['recommendations']:
                self.logger.info(f"   ğŸ“ {rec}")
        else:
            self.logger.info("âœ… æœªæ£€æµ‹åˆ°æ˜æ˜¾è¿‡æ‹Ÿåˆ")
        
        # è¾“å‡ºå…³é”®æŒ‡æ ‡
        metrics = results['metrics']
        self.logger.info("ğŸ“Š æ£€æµ‹æŒ‡æ ‡:")
        for key, value in metrics.items():
            if isinstance(value, float):
                self.logger.info(f"   {key}: {value:.4f}")
            else:
                self.logger.info(f"   {key}: {value}")


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    
    def __init__(self, patience: int = 20, min_delta: float = 0.005):
        self.patience = patience
        self.min_delta = min_delta
        self.best_score = -np.inf
        self.wait = 0
        self.stopped_epoch = 0
        
    def __call__(self, val_score: float, epoch: int = 0) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ
        
        å‚æ•°:
        val_score: å½“å‰éªŒè¯é›†å¾—åˆ†
        epoch: å½“å‰è½®æ¬¡
        
        è¿”å›:
        bool: æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ
        """
        if val_score > self.best_score + self.min_delta:
            self.best_score = val_score
            self.wait = 0
            return False
        else:
            self.wait += 1
            if self.wait >= self.patience:
                self.stopped_epoch = epoch
                return True
            return False
    
    def get_best_score(self) -> float:
        """è·å–æœ€ä½³å¾—åˆ†"""
        return self.best_score
    
    def get_stopped_epoch(self) -> int:
        """è·å–åœæ­¢çš„è½®æ¬¡"""
        return self.stopped_epoch


def validate_data_split(train_data: pd.DataFrame, 
                       val_data: pd.DataFrame, 
                       test_data: pd.DataFrame,
                       date_column: str = 'date') -> Dict:
    """
    éªŒè¯æ•°æ®åˆ†å‰²çš„æ­£ç¡®æ€§
    
    å‚æ•°:
    train_data: è®­ç»ƒæ•°æ®
    val_data: éªŒè¯æ•°æ®
    test_data: æµ‹è¯•æ•°æ®
    date_column: æ—¥æœŸåˆ—å
    
    è¿”å›:
    dict: éªŒè¯ç»“æœ
    """
    results = {
        'valid': True,
        'issues': []
    }
    
    # æ£€æŸ¥æ•°æ®é‡å 
    train_indices = set(train_data.index)
    val_indices = set(val_data.index)
    test_indices = set(test_data.index)
    
    if train_indices & val_indices:
        results['valid'] = False
        results['issues'].append("è®­ç»ƒé›†å’ŒéªŒè¯é›†å­˜åœ¨æ•°æ®é‡å ")
    
    if train_indices & test_indices:
        results['valid'] = False
        results['issues'].append("è®­ç»ƒé›†å’Œæµ‹è¯•é›†å­˜åœ¨æ•°æ®é‡å ")
    
    if val_indices & test_indices:
        results['valid'] = False
        results['issues'].append("éªŒè¯é›†å’Œæµ‹è¯•é›†å­˜åœ¨æ•°æ®é‡å ")
    
    # æ£€æŸ¥æ—¶é—´é¡ºåºï¼ˆå¦‚æœæœ‰æ—¥æœŸåˆ—ï¼‰
    if date_column in train_data.columns:
        train_max_date = train_data[date_column].max()
        val_min_date = val_data[date_column].min()
        val_max_date = val_data[date_column].max()
        test_min_date = test_data[date_column].min()
        
        if train_max_date >= val_min_date:
            results['valid'] = False
            results['issues'].append(f"æ—¶é—´åºåˆ—é¡ºåºé”™è¯¯: è®­ç»ƒé›†æœ€æ–°æ—¥æœŸ({train_max_date}) >= éªŒè¯é›†æœ€æ—©æ—¥æœŸ({val_min_date})")
        
        if val_max_date >= test_min_date:
            results['valid'] = False
            results['issues'].append(f"æ—¶é—´åºåˆ—é¡ºåºé”™è¯¯: éªŒè¯é›†æœ€æ–°æ—¥æœŸ({val_max_date}) >= æµ‹è¯•é›†æœ€æ—©æ—¥æœŸ({test_min_date})")
    
    return results