#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ”¹è¿›ç‰ˆAIä¼˜åŒ–å™¨
é›†æˆå¢é‡å­¦ä¹ ã€ç‰¹å¾æƒé‡ä¼˜åŒ–å’Œè¶‹åŠ¿ç¡®è®¤æŒ‡æ ‡
å·²åºŸå¼ƒç½®ä¿¡åº¦å¹³æ»‘åŠŸèƒ½ï¼Œç›´æ¥ä½¿ç”¨AIæ¨¡å‹åŸå§‹è¾“å‡º
"""

import logging
import os
import pickle
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from typing import Dict, Any, Tuple, List, Optional
import json
import yaml

# æ³¨é‡Šï¼šä»¥ä¸‹ConfidenceSmootherç±»å·²åºŸå¼ƒï¼Œä¸å†ä½¿ç”¨å¹³æ»‘å¤„ç†
# ç°åœ¨ç›´æ¥ä½¿ç”¨æ¨¡å‹çš„åŸå§‹è¾“å‡ºï¼Œä¿æŒä¿¡æ¯å®Œæ•´æ€§


class AIOptimizerImproved:
    """æ”¹è¿›ç‰ˆAIä¼˜åŒ–å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–æ¨¡å‹ç›¸å…³å±æ€§
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
        self.models_dir = os.path.join(project_root, 'models')
        os.makedirs(self.models_dir, exist_ok=True)
        
        self.model = None
        self.scaler = None
        self.feature_names = None
        self.model_type = config.get('ai', {}).get('model_type', 'machine_learning')
        
        # å¢é‡å­¦ä¹ é…ç½®
        incremental_config = config.get('ai', {}).get('incremental_learning', {})
        self.incremental_enabled = incremental_config.get('enabled', True)
        self.retrain_threshold = incremental_config.get('retrain_threshold', 0.1)  # æ¨¡å‹æ€§èƒ½ä¸‹é™é˜ˆå€¼
        self.max_incremental_updates = incremental_config.get('max_updates', 10)  # æœ€å¤§å¢é‡æ›´æ–°æ¬¡æ•°
        self.incremental_count = 0
        
        # ç§»é™¤ç½®ä¿¡åº¦å¹³æ»‘å™¨ - ä½¿ç”¨æ¨¡å‹åŸå§‹è¾“å‡º
        # self.confidence_smoother = ConfidenceSmoother(config)
        
        self.logger.info("æ”¹è¿›ç‰ˆAIä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def prepare_features_improved(self, data: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:
        """
        æ”¹è¿›çš„ç‰¹å¾å‡†å¤‡ï¼Œè°ƒæ•´æƒé‡å’Œå¢åŠ è¶‹åŠ¿ç¡®è®¤æŒ‡æ ‡
        
        å‚æ•°:
        data: å†å²æ•°æ®
        
        è¿”å›:
        tuple: (ç‰¹å¾çŸ©é˜µ, ç‰¹å¾åç§°åˆ—è¡¨)
        """
        self.logger.info("å‡†å¤‡æ”¹è¿›çš„æœºå™¨å­¦ä¹ ç‰¹å¾")
        
        # è®¡ç®—é¢å¤–çš„è¶‹åŠ¿ç¡®è®¤æŒ‡æ ‡
        data = self._calculate_trend_indicators(data)
        
        # é‡æ–°è®¾è®¡ç‰¹å¾åˆ—ï¼Œé™ä½çŸ­æœŸæŒ‡æ ‡æƒé‡ï¼Œå¢åŠ è¶‹åŠ¿ç¡®è®¤æŒ‡æ ‡
        feature_columns = [
            # é•¿æœŸè¶‹åŠ¿æŒ‡æ ‡ï¼ˆé«˜æƒé‡ï¼‰
            'ma20', 'ma60',  # é•¿æœŸå‡çº¿
            'trend_strength_20', 'trend_strength_60',  # è¶‹åŠ¿å¼ºåº¦
            'price_position_20', 'price_position_60',  # ä»·æ ¼åœ¨å‡çº¿ç³»ç»Ÿä¸­çš„ä½ç½®
            
            # ä¸­æœŸè¶‹åŠ¿æŒ‡æ ‡ï¼ˆä¸­ç­‰æƒé‡ï¼‰
            'ma10', 'dist_ma10', 'dist_ma20',
            'rsi', 'macd', 'signal',
            'bb_upper', 'bb_lower',
            'volatility_normalized',  # æ ‡å‡†åŒ–æ³¢åŠ¨ç‡
            
            # çŸ­æœŸæŒ‡æ ‡ï¼ˆé™ä½æƒé‡ï¼‰
            'ma5', 'dist_ma5', 'hist',
            'price_change_5d', 'price_change_10d',
            
            # æˆäº¤é‡å’Œæ³¢åŠ¨ç‡ï¼ˆå¹³è¡¡æƒé‡ï¼‰
            'volume_trend', 'volume_strength',  # æˆäº¤é‡è¶‹åŠ¿
            'volatility'
        ]
        
        # è¿‡æ»¤å­˜åœ¨çš„åˆ—
        available_columns = [col for col in feature_columns if col in data.columns]
        
        if len(available_columns) == 0:
            self.logger.warning("æ²¡æœ‰å¯ç”¨çš„ç‰¹å¾åˆ—")
            return np.array([]), []
        
        # æå–ç‰¹å¾å¹¶åº”ç”¨æƒé‡
        features_df = data[available_columns].fillna(0).copy()
        features = self._apply_feature_weights(features_df, available_columns)
        
        self.logger.info("æ”¹è¿›ç‰¹å¾å‡†å¤‡å®Œæˆï¼Œç‰¹å¾æ•°é‡: %d, æ ·æœ¬æ•°é‡: %d", 
                        len(available_columns), len(features))
        
        return features, available_columns
    
    def _calculate_trend_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—è¶‹åŠ¿ç¡®è®¤æŒ‡æ ‡
        
        å‚æ•°:
        data: åŸå§‹æ•°æ®
        
        è¿”å›:
        pd.DataFrame: æ·»åŠ äº†è¶‹åŠ¿æŒ‡æ ‡çš„æ•°æ®
        """
        # è¶‹åŠ¿å¼ºåº¦æŒ‡æ ‡ï¼ˆåŸºäºçº¿æ€§å›å½’æ–œç‡ï¼‰
        for period in [20, 60]:
            slopes = []
            for i in range(len(data)):
                if i >= period - 1:
                    prices = data['close'].iloc[i-period+1:i+1].values
                    x = np.arange(period)
                    slope = np.polyfit(x, prices, 1)[0]
                    # æ ‡å‡†åŒ–æ–œç‡
                    normalized_slope = slope / prices.mean()
                    slopes.append(normalized_slope)
                else:
                    slopes.append(0)
            data[f'trend_strength_{period}'] = slopes
        
        # ä»·æ ¼åœ¨å‡çº¿ç³»ç»Ÿä¸­çš„ä½ç½®
        data['price_position_20'] = (data['close'] - data['ma20']) / data['ma20']
        data['price_position_60'] = (data['close'] - data['ma60']) / data['ma60']
        
        # æ ‡å‡†åŒ–æ³¢åŠ¨ç‡
        data['volatility_normalized'] = data['volatility'] / data['volatility'].rolling(60).mean()
        
        # æˆäº¤é‡è¶‹åŠ¿æŒ‡æ ‡
        data['volume_ma20'] = data['volume'].rolling(20).mean()
        data['volume_trend'] = (data['volume'] - data['volume_ma20']) / data['volume_ma20']
        
        # æˆäº¤é‡å¼ºåº¦ï¼ˆç›¸å¯¹äºå†å²ï¼‰
        data['volume_strength'] = data['volume'] / data['volume'].rolling(60).mean()
        
        return data
    
    def _apply_feature_weights(self, features_df: pd.DataFrame, feature_names: List[str]) -> np.ndarray:
        """
        åº”ç”¨ç‰¹å¾æƒé‡ï¼Œé™ä½çŸ­æœŸæŒ‡æ ‡å½±å“
        
        å‚æ•°:
        features_df: ç‰¹å¾æ•°æ®æ¡†
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        
        è¿”å›:
        np.ndarray: åŠ æƒåçš„ç‰¹å¾çŸ©é˜µ
        """
        # å®šä¹‰ç‰¹å¾æƒé‡
        feature_weights = {
            # é•¿æœŸè¶‹åŠ¿æŒ‡æ ‡ï¼ˆé«˜æƒé‡ï¼‰
            'ma20': 1.5, 'ma60': 1.5,
            'trend_strength_20': 2.0, 'trend_strength_60': 2.0,
            'price_position_20': 1.8, 'price_position_60': 1.8,
            
            # ä¸­æœŸæŒ‡æ ‡ï¼ˆæ­£å¸¸æƒé‡ï¼‰
            'ma10': 1.0, 'dist_ma10': 1.2, 'dist_ma20': 1.2,
            'rsi': 1.0, 'macd': 1.0, 'signal': 1.0,
            'bb_upper': 1.0, 'bb_lower': 1.0,
            'volatility_normalized': 1.0,
            
            # çŸ­æœŸæŒ‡æ ‡ï¼ˆé™ä½æƒé‡ï¼‰
            'ma5': 0.6, 'dist_ma5': 0.6, 'hist': 0.7,
            'price_change_5d': 0.5, 'price_change_10d': 0.7,
            
            # æˆäº¤é‡æŒ‡æ ‡ï¼ˆå¹³è¡¡æƒé‡ï¼‰
            'volume_trend': 1.1, 'volume_strength': 1.1,
            'volatility': 0.9
        }
        
        # åº”ç”¨æƒé‡
        weighted_features = features_df.copy()
        for feature in feature_names:
            weight = feature_weights.get(feature, 1.0)
            weighted_features[feature] = weighted_features[feature] * weight
        
        return weighted_features.values
    
    def incremental_train(self, new_data: pd.DataFrame, strategy_module) -> Dict[str, Any]:
        """
        å¢é‡è®­ç»ƒæ¨¡å‹
        
        å‚æ•°:
        new_data: æ–°å¢æ•°æ®
        strategy_module: ç­–ç•¥æ¨¡å—
        
        è¿”å›:
        dict: è®­ç»ƒç»“æœ
        """
        self.logger.info("å¼€å§‹å¢é‡è®­ç»ƒæ¨¡å‹")
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å®Œå…¨é‡è®­ç»ƒ
            if self.model is None or self.incremental_count >= self.max_incremental_updates:
                self.logger.info("è§¦å‘å®Œå…¨é‡è®­ç»ƒæ¡ä»¶")
                return self.full_train(new_data, strategy_module)
            
            # å‡†å¤‡æ–°æ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾
            new_features, feature_names = self.prepare_features_improved(new_data)
            new_labels = self._prepare_labels(new_data, strategy_module)
            
            if len(new_features) == 0 or len(new_labels) == 0:
                self.logger.warning("æ–°æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å¢é‡è®­ç»ƒ")
                return {'success': False, 'error': 'æ–°æ•°æ®ä¸ºç©º'}
            
            # æ£€æŸ¥ç‰¹å¾ä¸€è‡´æ€§
            if self.feature_names and feature_names != self.feature_names:
                self.logger.warning("ç‰¹å¾ä¸ä¸€è‡´ï¼Œè¿›è¡Œå®Œå…¨é‡è®­ç»ƒ")
                return self.full_train(new_data, strategy_module)
            
            # ä½¿ç”¨æœ€è¿‘çš„æ•°æ®è¿›è¡Œå¢é‡æ›´æ–°
            recent_features = new_features[-10:]  # æœ€è¿‘10å¤©çš„æ•°æ®
            recent_labels = new_labels[-10:]
            
            if len(recent_features) > 0:
                # å¯¹æ–°æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆä½¿ç”¨å·²æœ‰çš„scalerï¼‰
                if self.scaler is not None:
                    recent_features_scaled = self.scaler.transform(recent_features)
                else:
                    self.logger.warning("ç¼ºå°‘scalerï¼Œè¿›è¡Œå®Œå…¨é‡è®­ç»ƒ")
                    return self.full_train(new_data, strategy_module)
                
                # ä½¿ç”¨warm_startè¿›è¡Œå¢é‡å­¦ä¹ 
                if hasattr(self.model.named_steps['classifier'], 'n_estimators'):
                    classifier = self.model.named_steps['classifier']
                    classifier.n_estimators += 10  # å¢åŠ æ ‘çš„æ•°é‡
                    classifier.warm_start = True
                    
                    # é‡æ–°è®­ç»ƒï¼ˆè¿™é‡Œå®é™…ä¸Šæ˜¯å¢é‡çš„ï¼‰
                    self.model.named_steps['classifier'].fit(recent_features_scaled, recent_labels)
                    
                    self.incremental_count += 1
                    self.logger.info(f"å¢é‡è®­ç»ƒå®Œæˆï¼Œæ›´æ–°æ¬¡æ•°: {self.incremental_count}")
                    
                    return {
                        'success': True,
                        'method': 'incremental',
                        'update_count': self.incremental_count,
                        'new_samples': len(recent_features)
                    }
                else:
                    self.logger.warning("æ¨¡å‹ä¸æ”¯æŒå¢é‡å­¦ä¹ ï¼Œè¿›è¡Œå®Œå…¨é‡è®­ç»ƒ")
                    return self.full_train(new_data, strategy_module)
            
            return {'success': False, 'error': 'æ²¡æœ‰è¶³å¤Ÿçš„æ–°æ•°æ®è¿›è¡Œå¢é‡è®­ç»ƒ'}
            
        except Exception as e:
            self.logger.error(f"å¢é‡è®­ç»ƒå¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿›è¡Œå®Œå…¨é‡è®­ç»ƒ
            return self.full_train(new_data, strategy_module)
    
    def full_train(self, data: pd.DataFrame, strategy_module) -> Dict[str, Any]:
        """
        å®Œå…¨é‡è®­ç»ƒæ¨¡å‹
        
        å‚æ•°:
        data: è®­ç»ƒæ•°æ®
        strategy_module: ç­–ç•¥æ¨¡å—
        
        è¿”å›:
        dict: è®­ç»ƒç»“æœ
        """
        self.logger.info("å¼€å§‹å®Œå…¨é‡è®­ç»ƒæ¨¡å‹")
        
        try:
            # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
            features, feature_names = self.prepare_features_improved(data)
            labels = self._prepare_labels(data, strategy_module)
            
            if len(features) == 0 or len(labels) == 0:
                self.logger.error("ç‰¹å¾æˆ–æ ‡ç­¾ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
                return {'success': False, 'error': 'ç‰¹å¾æˆ–æ ‡ç­¾ä¸ºç©º'}
            
            # æ•°æ®åˆ†å‰²
            min_length = min(len(features), len(labels))
            features = features[:min_length]
            labels = labels[:min_length]
            aligned_data = data.iloc[:min_length].copy()
            
            split_ratio = self.config.get("ai", {}).get("train_test_split_ratio", 0.8)
            split_index = int(len(features) * split_ratio)
            
            X_train = features[:split_index]
            y_train = labels[:split_index]
            train_dates = aligned_data["date"].iloc[:split_index]
            
            # è®¡ç®—æ ·æœ¬æƒé‡
            sample_weights = self._calculate_sample_weights(train_dates)
            
            # åˆ›å»ºæ–°æ¨¡å‹
            self.scaler = StandardScaler()
            X_train_scaled = self.scaler.fit_transform(X_train)
            
            # ä½¿ç”¨æ”¹è¿›çš„RandomForestå‚æ•°
            classifier = RandomForestClassifier(
                n_estimators=150,  # å¢åŠ æ ‘çš„æ•°é‡
                max_depth=12,      # é€‚å½“å¢åŠ æ·±åº¦
                min_samples_split=8,  # å¢åŠ åˆ†å‰²æ ·æœ¬æ•°
                min_samples_leaf=3,   # å¢åŠ å¶å­èŠ‚ç‚¹æ ·æœ¬æ•°
                random_state=42,
                class_weight='balanced',
                warm_start=True,  # æ”¯æŒå¢é‡å­¦ä¹ 
                n_jobs=-1         # å¹¶è¡Œè®­ç»ƒ
            )
            
            # è®­ç»ƒåˆ†ç±»å™¨
            classifier.fit(X_train_scaled, y_train, sample_weight=sample_weights)
            
            # åˆ›å»ºå®Œæ•´çš„pipeline
            self.model = Pipeline([
                ('scaler', self.scaler),
                ('classifier', classifier)
            ])
            
            self.feature_names = feature_names
            self.incremental_count = 0  # é‡ç½®å¢é‡è®¡æ•°
            
            # ä¿å­˜æ¨¡å‹
            self._save_model()
            
            self.logger.info("å®Œå…¨é‡è®­ç»ƒå®Œæˆ")
            return {
                'success': True,
                'method': 'full_retrain',
                'train_samples': len(X_train),
                'feature_count': len(feature_names)
            }
            
        except Exception as e:
            self.logger.error(f"å®Œå…¨é‡è®­ç»ƒå¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def predict_low_point(self, data: pd.DataFrame, prediction_date: str = None) -> Dict[str, Any]:
        """
        é¢„æµ‹ç›¸å¯¹ä½ç‚¹ï¼ˆå¸¦ç½®ä¿¡åº¦å¹³æ»‘ï¼‰
        
        å‚æ•°:
        data: å¸‚åœºæ•°æ®
        prediction_date: é¢„æµ‹æ—¥æœŸï¼ˆç”¨äºç½®ä¿¡åº¦å¹³æ»‘ï¼‰
        
        è¿”å›:
        dict: é¢„æµ‹ç»“æœ
        """
        self.logger.info("é¢„æµ‹ç›¸å¯¹ä½ç‚¹ï¼ˆæ”¹è¿›ç‰ˆï¼‰")
        
        try:
            # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœæœªåŠ è½½ï¼‰
            if self.model is None:
                if not self._load_model():
                    return {
                        'is_low_point': False,
                        'confidence': 0.0,
                        'smoothed_confidence': 0.0,
                        'error': 'æ¨¡å‹æœªè®­ç»ƒä¸”æ— æ³•åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹'
                    }
            
            if len(data) == 0:
                return {
                    'is_low_point': False,
                    'confidence': 0.0,
                    'smoothed_confidence': 0.0,
                    'error': 'æ•°æ®ä¸ºç©º'
                }
            
            # å‡†å¤‡ç‰¹å¾
            features, feature_names = self.prepare_features_improved(data)
            
            if len(features) == 0:
                return {
                    'is_low_point': False,
                    'confidence': 0.0,
                    'final_confidence': 0.0,
                    'error': 'æ— æ³•æå–ç‰¹å¾'
                }
            
            # ä½¿ç”¨æœ€æ–°æ•°æ®è¿›è¡Œé¢„æµ‹
            latest_features = features[-1:].reshape(1, -1)
            
            # è·å–é¢„æµ‹æ¦‚ç‡ï¼ˆä¸ä½¿ç”¨predictæ–¹æ³•ï¼Œé¿å…å†…ç½®é˜ˆå€¼å½±å“ï¼‰
            prediction_proba = self.model.predict_proba(latest_features)[0]
            
            # è·å–åŸå§‹ç½®ä¿¡åº¦ï¼ˆä¸å†è¿›è¡Œå¹³æ»‘å¤„ç†ï¼‰
            raw_confidence = prediction_proba[1] if len(prediction_proba) > 1 else 0.0
            
            # ç›´æ¥ä½¿ç”¨åŸå§‹ç½®ä¿¡åº¦ï¼Œä¸è¿›è¡Œå¹³æ»‘å¤„ç†
            final_confidence = raw_confidence
            
            # ä½¿ç”¨é…ç½®çš„é˜ˆå€¼å’ŒåŸå§‹ç½®ä¿¡åº¦è¿›è¡Œæœ€ç»ˆé¢„æµ‹
            confidence_config = self.config.get('strategy', {}).get('confidence_weights', {})
            final_threshold = confidence_config.get('final_threshold', 0.5)
            
            # åŸºäºåŸå§‹ç½®ä¿¡åº¦å’Œé…ç½®é˜ˆå€¼è¿›è¡Œé¢„æµ‹
            is_low_point = final_confidence >= final_threshold
            
            result = {
                'is_low_point': bool(is_low_point),
                'confidence': float(raw_confidence),
                'final_confidence': float(final_confidence),  # ç°åœ¨ç­‰äºåŸå§‹ç½®ä¿¡åº¦
                'prediction_proba': prediction_proba.tolist(),
                'feature_count': len(feature_names),
                'model_type': type(self.model.named_steps['classifier']).__name__,
                'threshold_used': final_threshold
            }
            
            # è¾“å‡ºé¢„æµ‹ç»“æœ
            self.logger.info("----------------------------------------------------")
            self.logger.info("AIé¢„æµ‹ç»“æœï¼ˆæ— å¹³æ»‘ï¼‰: \033[1m%s\033[0m", 
                           "ç›¸å¯¹ä½ç‚¹" if is_low_point else "éç›¸å¯¹ä½ç‚¹")
            self.logger.info("åŸå§‹ç½®ä¿¡åº¦: \033[1m%.4f\033[0m, é˜ˆå€¼: \033[1m%.2f\033[0m", 
                           raw_confidence, final_threshold)
            self.logger.info("----------------------------------------------------")
            
            return result
            
        except Exception as e:
            self.logger.error(f"é¢„æµ‹ç›¸å¯¹ä½ç‚¹å¤±è´¥: {e}")
            return {
                'is_low_point': False,
                'confidence': 0.0,
                'final_confidence': 0.0,
                'error': str(e)
            }
    
    def _prepare_labels(self, data: pd.DataFrame, strategy_module) -> np.ndarray:
        """å‡†å¤‡æ ‡ç­¾"""
        backtest_results = strategy_module.backtest(data)
        return backtest_results['is_low_point'].astype(int).values
    
    def _calculate_sample_weights(self, dates: pd.Series) -> np.ndarray:
        """è®¡ç®—æ ·æœ¬æƒé‡"""
        weights = np.ones(len(dates))
        if len(dates) == 0:
            return weights
        
        latest_date = dates.max()
        decay_rate = self.config.get("ai", {}).get("data_decay_rate", 0.4)
        
        for i, date in enumerate(dates):
            time_diff = (latest_date - date).days / 365.25
            weight = np.exp(-decay_rate * time_diff)
            weights[i] = weight
        
        return weights
    
    def _save_model(self) -> bool:
        """ä¿å­˜æ¨¡å‹"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # ä¿å­˜æ¨¡å‹
            model_path = os.path.join(self.models_dir, f'improved_model_{timestamp}.pkl')
            with open(model_path, 'wb') as f:
                pickle.dump({
                    'model': self.model,
                    'feature_names': self.feature_names,
                    'incremental_count': self.incremental_count,
                    'scaler': self.scaler
                }, f)
            
            # ä¿å­˜æœ€æ–°æ¨¡å‹è·¯å¾„
            latest_path = os.path.join(self.models_dir, 'latest_improved_model.txt')
            with open(latest_path, 'w') as f:
                f.write(model_path)
            
            self.logger.info(f"æ”¹è¿›æ¨¡å‹ä¿å­˜æˆåŠŸ: {model_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ”¹è¿›æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def _load_model(self) -> bool:
        """åŠ è½½æ¨¡å‹ï¼ˆå®‰å…¨ç‰ˆæœ¬ï¼‰"""
        try:
            latest_path = os.path.join(self.models_dir, 'latest_improved_model.txt')
            
            if not os.path.exists(latest_path):
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°å·²ä¿å­˜çš„æ”¹è¿›æ¨¡å‹")
                return False
            
            with open(latest_path, 'r') as f:
                model_path = f.read().strip()
            
            # å®‰å…¨æ£€æŸ¥ï¼šéªŒè¯æ¨¡å‹æ–‡ä»¶è·¯å¾„
            if not os.path.abspath(model_path).startswith(os.path.abspath(self.models_dir)):
                self.logger.error(f"æ¨¡å‹æ–‡ä»¶è·¯å¾„ä¸å®‰å…¨: {model_path}")
                return False
            
            # å®‰å…¨æ£€æŸ¥ï¼šéªŒè¯æ–‡ä»¶å¤§å°ï¼ˆé˜²æ­¢è¿‡å¤§çš„æ¶æ„æ–‡ä»¶ï¼‰
            max_file_size = 500 * 1024 * 1024  # 500MBé™åˆ¶
            if os.path.getsize(model_path) > max_file_size:
                self.logger.error(f"æ¨¡å‹æ–‡ä»¶è¿‡å¤§ï¼Œå¯èƒ½å­˜åœ¨å®‰å…¨é£é™©: {model_path}")
                return False
            
            with open(model_path, 'rb') as f:
                # ä½¿ç”¨å—é™çš„pickleåŠ è½½å™¨ï¼ˆé™åˆ¶å¯å¯¼å…¥çš„æ¨¡å—ï¼‰
                import pickle
                import builtins
                
                # åˆ›å»ºå®‰å…¨çš„unpickler
                logger = self.logger  # ä¿å­˜loggerå¼•ç”¨
                class SafeUnpickler(pickle.Unpickler):
                    def find_class(self, module, name):
                        # åªå…è®¸åŠ è½½ç‰¹å®šçš„å®‰å…¨æ¨¡å—
                        safe_modules = {
                            'sklearn.ensemble._forest',
                            'sklearn.ensemble',
                            'sklearn.pipeline',
                            'sklearn.preprocessing._data',
                            'sklearn.preprocessing',
                            'numpy',
                            'pandas.core.frame',
                            'pandas',
                            'builtins'
                        }
                        
                        if module in safe_modules or module.startswith('numpy') or module.startswith('sklearn'):
                            return getattr(__import__(module, fromlist=[name]), name)
                        else:
                            logger.warning(f"æ‹’ç»åŠ è½½ä¸å®‰å…¨çš„æ¨¡å—: {module}.{name}")
                            raise pickle.PicklingError(f"Unsafe module: {module}")
                
                # ä½¿ç”¨å®‰å…¨çš„unpickleråŠ è½½æ•°æ®
                safe_unpickler = SafeUnpickler(f)
                data = safe_unpickler.load()
                
                # éªŒè¯åŠ è½½çš„æ•°æ®ç»“æ„
                required_keys = ['model', 'feature_names']
                if not isinstance(data, dict) or not all(key in data for key in required_keys):
                    self.logger.error("æ¨¡å‹æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
                    return False
                
                self.model = data['model']
                self.feature_names = data['feature_names']
                self.incremental_count = data.get('incremental_count', 0)
                self.scaler = data.get('scaler')
            
            self.logger.info(f"æ”¹è¿›æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æ”¹è¿›æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def get_feature_importance(self) -> Dict[str, float]:
        """
        è·å–ç‰¹å¾é‡è¦æ€§
        
        è¿”å›:
        dict: ç‰¹å¾é‡è¦æ€§å­—å…¸ï¼ŒæŒ‰é‡è¦æ€§é™åºæ’åˆ—
        """
        try:
            if self.model is None:
                self.logger.warning("æ¨¡å‹æœªè®­ç»ƒï¼Œå°è¯•åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹")
                if not self._load_model():
                    self.logger.error("æ— æ³•è·å–ç‰¹å¾é‡è¦æ€§ï¼šæ¨¡å‹æœªè®­ç»ƒä¸”æ— æ³•åŠ è½½")
                    return {}
            
            if self.feature_names is None:
                self.logger.error("ç‰¹å¾åç§°æœªè®¾ç½®ï¼Œæ— æ³•è·å–ç‰¹å¾é‡è¦æ€§")
                return {}
            
            # ä»Pipelineä¸­è·å–åˆ†ç±»å™¨
            if hasattr(self.model, 'named_steps') and 'classifier' in self.model.named_steps:
                classifier = self.model.named_steps['classifier']
            else:
                # å¦‚æœæ¨¡å‹ä¸æ˜¯Pipelineï¼Œç›´æ¥ä½¿ç”¨
                classifier = self.model
            
            # æ£€æŸ¥åˆ†ç±»å™¨æ˜¯å¦æœ‰feature_importances_å±æ€§
            if hasattr(classifier, 'feature_importances_'):
                importances = classifier.feature_importances_
                
                # åˆ›å»ºç‰¹å¾é‡è¦æ€§å­—å…¸
                feature_importance = dict(zip(self.feature_names, importances))
                
                # æŒ‰é‡è¦æ€§é™åºæ’åˆ—
                sorted_importance = dict(sorted(
                    feature_importance.items(), 
                    key=lambda x: x[1], 
                    reverse=True
                ))
                
                self.logger.info(f"æˆåŠŸè·å– {len(sorted_importance)} ä¸ªç‰¹å¾çš„é‡è¦æ€§")
                return sorted_importance
            else:
                self.logger.warning(f"åˆ†ç±»å™¨ {type(classifier).__name__} ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§")
                return {}
                
        except Exception as e:
            self.logger.error(f"è·å–ç‰¹å¾é‡è¦æ€§å¤±è´¥: {e}")
            return {}
    
    def run_complete_optimization(self, data: pd.DataFrame, strategy_module) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„AIä¼˜åŒ–æµç¨‹ï¼ˆåŒ…å«å‚æ•°ä¼˜åŒ– + æ¨¡å‹è®­ç»ƒï¼‰
        
        å‚æ•°:
        data: å†å²æ•°æ®
        strategy_module: ç­–ç•¥æ¨¡å—
        
        è¿”å›:
        dict: ä¼˜åŒ–ç»“æœ
        """
        self.logger.info("ğŸš€ å¼€å§‹å®Œæ•´çš„AIä¼˜åŒ–æµç¨‹ï¼ˆæ”¹è¿›ç‰ˆï¼‰")
        
        try:
            optimization_result = {
                'success': False,
                'strategy_optimization': {},
                'model_training': {},
                'final_evaluation': {},
                'errors': []
            }
            
            # 1. ç­–ç•¥å‚æ•°ä¼˜åŒ–
            self.logger.info("ğŸ”§ æ­¥éª¤1: ç­–ç•¥å‚æ•°ä¼˜åŒ–")
            strategy_result = self.optimize_strategy_parameters_improved(strategy_module, data)
            optimization_result['strategy_optimization'] = strategy_result
            
            if strategy_result['success']:
                # æ›´æ–°ç­–ç•¥æ¨¡å—å‚æ•°
                strategy_module.update_params(strategy_result['best_params'])
                self.logger.info(f"âœ… ç­–ç•¥å‚æ•°ä¼˜åŒ–å®Œæˆ: {strategy_result['best_params']}")
            else:
                self.logger.warning("âš ï¸ ç­–ç•¥å‚æ•°ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°ç»§ç»­")
                optimization_result['errors'].append("ç­–ç•¥å‚æ•°ä¼˜åŒ–å¤±è´¥")
            
            # 2. æ”¹è¿›ç‰ˆæ¨¡å‹è®­ç»ƒ
            self.logger.info("ğŸ¤– æ­¥éª¤2: æ”¹è¿›ç‰ˆæ¨¡å‹è®­ç»ƒ")
            model_result = self.full_train(data, strategy_module)
            optimization_result['model_training'] = model_result
            
            if not model_result['success']:
                self.logger.error("âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥")
                optimization_result['errors'].append("æ¨¡å‹è®­ç»ƒå¤±è´¥")
                return optimization_result
            
            # 3. æœ€ç»ˆè¯„ä¼°
            self.logger.info("ğŸ“Š æ­¥éª¤3: æœ€ç»ˆæ€§èƒ½è¯„ä¼°")
            evaluation_result = self.evaluate_optimized_system(data, strategy_module)
            optimization_result['final_evaluation'] = evaluation_result
            
            # 4. ä¿å­˜ä¼˜åŒ–ç»“æœ
            if strategy_result['success']:
                self.save_optimized_params(strategy_result['best_params'])
            
            optimization_result['success'] = model_result['success']
            
            self.logger.info("ğŸ‰ å®Œæ•´AIä¼˜åŒ–æµç¨‹å®Œæˆ")
            return optimization_result
            
        except Exception as e:
            self.logger.error(f"å®Œæ•´AIä¼˜åŒ–æµç¨‹å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'strategy_optimization': {},
                'model_training': {},
                'final_evaluation': {}
            }
    
    def optimize_strategy_parameters_improved(self, strategy_module, data: pd.DataFrame) -> Dict[str, Any]:
        """
        æ”¹è¿›ç‰ˆç­–ç•¥å‚æ•°ä¼˜åŒ–ï¼ˆä½¿ç”¨ä¸¥æ ¼ä¸‰å±‚æ•°æ®åˆ†å‰²ï¼‰
        
        å‚æ•°:
        strategy_module: ç­–ç•¥æ¨¡å—
        data: å†å²æ•°æ®
        
        è¿”å›:
        dict: ä¼˜åŒ–ç»“æœ
        """
        self.logger.info("å¼€å§‹æ”¹è¿›ç‰ˆç­–ç•¥å‚æ•°ä¼˜åŒ–ï¼ˆä¸¥æ ¼ä¸‰å±‚åˆ†å‰²ï¼‰")
        
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
            if len(data) < 100:
                return {
                    'success': False,
                    'error': 'æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå‚æ•°ä¼˜åŒ–'
                }
            
            # ä»é…ç½®æ–‡ä»¶è·å–ä¸‰å±‚æ•°æ®åˆ†å‰²æ¯”ä¾‹
            validation_config = self.config.get('ai', {}).get('validation', {})
            train_ratio = validation_config.get('train_ratio', 0.65)
            val_ratio = validation_config.get('validation_ratio', 0.2) 
            test_ratio = validation_config.get('test_ratio', 0.15)
            
            # éªŒè¯æ¯”ä¾‹æ€»å’Œ
            total_ratio = train_ratio + val_ratio + test_ratio
            if abs(total_ratio - 1.0) > 0.01:
                self.logger.warning(f"æ•°æ®åˆ†å‰²æ¯”ä¾‹æ€»å’Œä¸ç­‰äº1.0: {total_ratio:.3f}ï¼Œè‡ªåŠ¨è°ƒæ•´")
                # é‡æ–°å½’ä¸€åŒ–
                train_ratio = train_ratio / total_ratio
                val_ratio = val_ratio / total_ratio 
                test_ratio = test_ratio / total_ratio
            
            # è®¡ç®—åˆ†å‰²ç‚¹
            train_end = int(len(data) * train_ratio)
            val_end = int(len(data) * (train_ratio + val_ratio))
            
            # ä¸¥æ ¼ä¸‰å±‚æ•°æ®åˆ†å‰²
            train_data = data.iloc[:train_end].copy()
            validation_data = data.iloc[train_end:val_end].copy()
            test_data = data.iloc[val_end:].copy()
            
            self.logger.info(f"ä¸¥æ ¼ä¸‰å±‚æ•°æ®åˆ†å‰²:")
            self.logger.info(f"   ğŸ“Š è®­ç»ƒé›†: {len(train_data)}æ¡ ({train_ratio:.1%}) - ä»…ç”¨äºå‚æ•°ä¼˜åŒ–")
            self.logger.info(f"   ğŸ“ˆ éªŒè¯é›†: {len(validation_data)}æ¡ ({val_ratio:.1%}) - ç”¨äºæ¨¡å‹éªŒè¯å’Œè¿‡æ‹Ÿåˆæ£€æµ‹")
            self.logger.info(f"   ğŸ”’ æµ‹è¯•é›†: {len(test_data)}æ¡ ({test_ratio:.1%}) - å®Œå…¨é”å®šï¼Œä»…æœ€ç»ˆè¯„ä¼°")
            
            # å®šä¹‰ä¼˜åŒ–å‚æ•°èŒƒå›´
            param_ranges = self.config.get('optimization', {}).get('param_ranges', {})
            
            # ä½¿ç”¨ç½‘æ ¼æœç´¢è¿›è¡Œå‚æ•°ä¼˜åŒ–ï¼ˆä»…åœ¨è®­ç»ƒé›†ä¸Šï¼‰
            self.logger.info("ğŸ”§ æ­¥éª¤1: åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå‚æ•°æœç´¢...")
            best_params, best_score = self._grid_search_optimization(
                strategy_module, train_data, param_ranges
            )
            
            # åœ¨éªŒè¯é›†ä¸ŠéªŒè¯æœ€ä½³å‚æ•°
            self.logger.info("ğŸ“ˆ æ­¥éª¤2: åœ¨éªŒè¯é›†ä¸ŠéªŒè¯æœ€ä½³å‚æ•°...")
            strategy_module.update_params(best_params)
            val_backtest = strategy_module.backtest(validation_data)
            val_evaluation = strategy_module.evaluate_strategy(val_backtest)
            val_score = val_evaluation['score']
            val_success_rate = val_evaluation.get('success_rate', 0)
            val_total_points = val_evaluation.get('total_points', 0)
            val_avg_rise = val_evaluation.get('avg_rise', 0)
            
            # æ£€æŸ¥è¿‡æ‹Ÿåˆ
            overfitting_threshold = 0.8  # éªŒè¯é›†å¾—åˆ†åº”è¯¥è‡³å°‘æ˜¯è®­ç»ƒé›†å¾—åˆ†çš„80%
            overfitting_passed = val_score >= best_score * overfitting_threshold
            
            # åœ¨å®Œå…¨é”å®šçš„æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°
            self.logger.info("ğŸ”’ æ­¥éª¤3: åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
            test_backtest = strategy_module.backtest(test_data)
            test_evaluation = strategy_module.evaluate_strategy(test_backtest)
            test_score = test_evaluation['score']
            test_success_rate = test_evaluation.get('success_rate', 0)
            test_total_points = test_evaluation.get('total_points', 0)
            test_avg_rise = test_evaluation.get('avg_rise', 0)
            
            # è¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›
            generalization_ratio = test_score / val_score if val_score > 0 else 0
            generalization_passed = generalization_ratio >= 0.85  # æµ‹è¯•é›†å¾—åˆ†åº”è¯¥æ¥è¿‘éªŒè¯é›†
            
            self.logger.info(f"âœ… ä¸‰å±‚éªŒè¯ç»“æœ:")
            self.logger.info(f"   ğŸ“Š è®­ç»ƒé›†å¾—åˆ†: {best_score:.4f}")
            self.logger.info(f"   ğŸ“ˆ éªŒè¯é›†å¾—åˆ†: {val_score:.4f} | æˆåŠŸç‡: {val_success_rate:.2%} | è¯†åˆ«ç‚¹æ•°: {val_total_points} | å¹³å‡æ¶¨å¹…: {val_avg_rise:.2%}")
            self.logger.info(f"   ğŸ”’ æµ‹è¯•é›†å¾—åˆ†: {test_score:.4f} | æˆåŠŸç‡: {test_success_rate:.2%} | è¯†åˆ«ç‚¹æ•°: {test_total_points} | å¹³å‡æ¶¨å¹…: {test_avg_rise:.2%}")
            self.logger.info(f"   ğŸ›¡ï¸ è¿‡æ‹Ÿåˆæ£€æµ‹: {'âœ… é€šè¿‡' if overfitting_passed else 'âš ï¸ è­¦å‘Š'}")
            self.logger.info(f"   ğŸ¯ æ³›åŒ–èƒ½åŠ›: {'âœ… è‰¯å¥½' if generalization_passed else 'âš ï¸ ä¸€èˆ¬'} (æ¯”ç‡: {generalization_ratio:.3f})")
            
            return {
                'success': True,
                'best_params': best_params,
                'best_score': best_score,
                'validation_score': val_score,
                'validation_success_rate': val_success_rate,
                'validation_total_points': val_total_points,
                'validation_avg_rise': val_avg_rise,
                'test_score': test_score,
                'test_success_rate': test_success_rate,
                'test_total_points': test_total_points,
                'test_avg_rise': test_avg_rise,
                'overfitting_passed': overfitting_passed,
                'generalization_passed': generalization_passed,
                'generalization_ratio': generalization_ratio,
                'optimization_method': 'grid_search_improved_3layer',
                'data_split': {
                    'train_ratio': train_ratio,
                    'validation_ratio': val_ratio,
                    'test_ratio': test_ratio,
                    'train_samples': len(train_data),
                    'validation_samples': len(validation_data),
                    'test_samples': len(test_data)
                }
            }
            
        except Exception as e:
            self.logger.error(f"ç­–ç•¥å‚æ•°ä¼˜åŒ–å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _grid_search_optimization(self, strategy_module, train_data: pd.DataFrame, param_ranges: dict) -> tuple:
        """
        ç½‘æ ¼æœç´¢ä¼˜åŒ–
        
        å‚æ•°:
        strategy_module: ç­–ç•¥æ¨¡å—
        train_data: è®­ç»ƒæ•°æ®
        param_ranges: å‚æ•°èŒƒå›´
        
        è¿”å›:
        tuple: (æœ€ä½³å‚æ•°, æœ€ä½³å¾—åˆ†)
        """
        self.logger.info("å¼€å§‹ç½‘æ ¼æœç´¢ä¼˜åŒ–")
        
        # å®šä¹‰é»˜è®¤å‚æ•°èŒƒå›´
        default_ranges = {
            'rsi_oversold_threshold': {'min': 25, 'max': 35, 'step': 2},
            'rsi_low_threshold': {'min': 35, 'max': 45, 'step': 2},
            'final_threshold': {'min': 0.3, 'max': 0.7, 'step': 0.1}
        }
        
        # åˆå¹¶ç”¨æˆ·é…ç½®å’Œé»˜è®¤é…ç½®
        search_ranges = {**default_ranges, **param_ranges}
        
        # ç”Ÿæˆå‚æ•°ç»„åˆ
        param_combinations = []
        param_names = list(search_ranges.keys())
        
        def generate_range(param_config):
            start = param_config['min']
            end = param_config['max']
            step = param_config['step']
            return [start + i * step for i in range(int((end - start) / step) + 1)]
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆï¼ˆé™åˆ¶æ•°é‡ä»¥é¿å…è¿‡é•¿æ—¶é—´ï¼‰
        from itertools import product
        
        ranges = [generate_range(search_ranges[param]) for param in param_names]
        all_combinations = list(product(*ranges))
        
        # é™åˆ¶æœç´¢æ•°é‡
        max_combinations = 50
        if len(all_combinations) > max_combinations:
            import random
            random.seed(42)
            all_combinations = random.sample(all_combinations, max_combinations)
        
        self.logger.info(f"å°†æµ‹è¯• {len(all_combinations)} ä¸ªå‚æ•°ç»„åˆ")
        
        best_score = -float('inf')
        best_params = {}
        
        for i, combination in enumerate(all_combinations):
            # æ„å»ºå‚æ•°å­—å…¸
            params = dict(zip(param_names, combination))
            
            try:
                # æ›´æ–°å‚æ•°å¹¶æµ‹è¯•
                strategy_module.update_params(params)
                backtest_results = strategy_module.backtest(train_data)
                evaluation = strategy_module.evaluate_strategy(backtest_results)
                score = evaluation['score']
                
                if score > best_score:
                    best_score = score
                    best_params = params.copy()
                
                if (i + 1) % 10 == 0:
                    self.logger.info(f"å·²æµ‹è¯• {i + 1}/{len(all_combinations)} ä¸ªç»„åˆï¼Œå½“å‰æœ€ä½³å¾—åˆ†: {best_score:.4f}")
                    
            except Exception as e:
                self.logger.warning(f"å‚æ•°ç»„åˆ {params} æµ‹è¯•å¤±è´¥: {e}")
                continue
        
        self.logger.info(f"ç½‘æ ¼æœç´¢å®Œæˆï¼Œæœ€ä½³å¾—åˆ†: {best_score:.4f}")
        return best_params, best_score
    
    def evaluate_optimized_system(self, data: pd.DataFrame, strategy_module) -> Dict[str, Any]:
        """
        è¯„ä¼°ä¼˜åŒ–åçš„ç³»ç»Ÿ
        
        å‚æ•°:
        data: æµ‹è¯•æ•°æ®
        strategy_module: ç­–ç•¥æ¨¡å—
        
        è¿”å›:
        dict: è¯„ä¼°ç»“æœ
        """
        self.logger.info("è¯„ä¼°ä¼˜åŒ–åçš„ç³»ç»Ÿ")
        
        try:
            # ç­–ç•¥è¯„ä¼°
            backtest_results = strategy_module.backtest(data)
            strategy_evaluation = strategy_module.evaluate_strategy(backtest_results)
            
            # AIæ¨¡å‹é¢„æµ‹è¯„ä¼°
            prediction_result = self.predict_low_point(data)
            
            return {
                'success': True,
                'strategy_score': strategy_evaluation['score'],
                'strategy_success_rate': strategy_evaluation['success_rate'],
                'identified_points': strategy_evaluation['total_points'],
                'avg_rise': strategy_evaluation['avg_rise'],
                'ai_confidence': prediction_result.get('final_confidence', 0),
                'ai_prediction': prediction_result.get('is_low_point', False)
            }
            
        except Exception as e:
            self.logger.error(f"ç³»ç»Ÿè¯„ä¼°å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def save_optimized_params(self, params: dict):
        """
        ä¿å­˜ä¼˜åŒ–åçš„å‚æ•°åˆ°é…ç½®æ–‡ä»¶ï¼ˆä¿ç•™æ³¨é‡Šç‰ˆï¼‰
        
        å‚æ•°:
        params: ä¼˜åŒ–åçš„å‚æ•°
        """
        try:
            # å°è¯•ä½¿ç”¨ä¿ç•™æ³¨é‡Šçš„ä¿å­˜å™¨
            try:
                from src.utils.config_saver import ConfigSaver
                saver = ConfigSaver()
                saver.save_optimized_params(params)
                self.logger.info("å‚æ•°å·²ä¿å­˜ï¼ˆä¿ç•™æ³¨é‡Šç‰ˆæœ¬ï¼‰")
                return
            except Exception as e:
                self.logger.warning(f"ä¿ç•™æ³¨é‡Šç‰ˆæœ¬ä¿å­˜å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼: {e}")
                self._save_params_fallback(params)
        except Exception as e:
            self.logger.error(f"ä¿å­˜ä¼˜åŒ–å‚æ•°å¤±è´¥: {e}")
    
    def _save_params_fallback(self, params: dict):
        """
        ä¼ ç»Ÿçš„å‚æ•°ä¿å­˜æ–¹å¼ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        
        å‚æ•°:
        params: ä¼˜åŒ–åçš„å‚æ•°
        """
        try:
            config_path = 'config/config_improved.yaml'
            
            # è¯»å–ç°æœ‰é…ç½®
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # æ›´æ–°å‚æ•°
            for param_name, param_value in params.items():
                if param_name == 'final_threshold':
                    if 'strategy' not in config:
                        config['strategy'] = {}
                    if 'confidence_weights' not in config['strategy']:
                        config['strategy']['confidence_weights'] = {}
                    config['strategy']['confidence_weights']['final_threshold'] = param_value
                elif param_name in ['rsi_oversold_threshold', 'rsi_low_threshold']:
                    if 'strategy' not in config:
                        config['strategy'] = {}
                    if 'confidence_weights' not in config['strategy']:
                        config['strategy']['confidence_weights'] = {}
                    config['strategy']['confidence_weights'][param_name] = param_value
                else:
                    # å…¶ä»–å‚æ•°æŒ‰åŸæœ‰é€»è¾‘å¤„ç†
                    if 'strategy' not in config:
                        config['strategy'] = {}
                    config['strategy'][param_name] = param_value
            
            # ä¿å­˜é…ç½®
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
            
            self.logger.info("å‚æ•°å·²ä¿å­˜åˆ°é…ç½®æ–‡ä»¶ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰")
            
        except Exception as e:
            self.logger.error(f"ä¼ ç»Ÿæ–¹å¼ä¿å­˜å‚æ•°å¤±è´¥: {e}") 