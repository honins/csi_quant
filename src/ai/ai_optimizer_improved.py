#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ”¹è¿›ç‰ˆAIä¼˜åŒ–å™¨
å®ç°å¢é‡å­¦ä¹ ã€ç½®ä¿¡åº¦å¹³æ»‘ã€æƒé‡è°ƒæ•´å’Œè¶‹åŠ¿ç¡®è®¤æŒ‡æ ‡
"""

import os
import logging
import json
import pickle
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, Any, List, Tuple, Optional
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline


class ConfidenceSmoother:
    """ç½®ä¿¡åº¦å¹³æ»‘å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # å¹³æ»‘å‚æ•°
        smooth_config = config.get('ai', {}).get('confidence_smoothing', {})
        self.enabled = smooth_config.get('enabled', True)
        self.ema_alpha = smooth_config.get('ema_alpha', 0.3)  # EMAå¹³æ»‘ç³»æ•°
        self.max_daily_change = smooth_config.get('max_daily_change', 0.25)  # åŸºç¡€æœ€å¤§æ—¥å˜åŒ–
        self.history_path = os.path.join('models', 'confidence_history.json')
        
        # åŠ¨æ€è°ƒæ•´é…ç½®
        dynamic_config = smooth_config.get('dynamic_adjustment', {})
        self.dynamic_enabled = dynamic_config.get('enabled', True)
        self.min_limit = dynamic_config.get('min_limit', 0.15)
        self.max_limit = dynamic_config.get('max_limit', 0.50)
        
        # å„å› å­é…ç½®
        self.volatility_config = dynamic_config.get('volatility_factor', {})
        self.price_config = dynamic_config.get('price_factor', {})
        self.volume_config = dynamic_config.get('volume_factor', {})
        self.confidence_config = dynamic_config.get('confidence_factor', {})
        
        # è°ƒè¯•é…ç½®
        self.debug_mode = smooth_config.get('debug_mode', False)
        self.log_adjustments = smooth_config.get('log_adjustments', True)
        
        # åŠ è½½å†å²ç½®ä¿¡åº¦
        self.confidence_history = self._load_confidence_history()
        
    def smooth_confidence(self, raw_confidence: float, date: str, market_data: pd.DataFrame = None) -> float:
        """
        å¹³æ»‘ç½®ä¿¡åº¦ï¼ˆæ”¹è¿›ç‰ˆï¼šåŠ¨æ€è°ƒæ•´é™åˆ¶ï¼‰
        
        å‚æ•°:
        raw_confidence: åŸå§‹ç½®ä¿¡åº¦
        date: é¢„æµ‹æ—¥æœŸ
        market_data: å¸‚åœºæ•°æ®ï¼ˆç”¨äºè®¡ç®—æ³¢åŠ¨æ€§ï¼‰
        
        è¿”å›:
        float: å¹³æ»‘åçš„ç½®ä¿¡åº¦
        """
        if not self.enabled:
            return raw_confidence
            
        try:
            # è·å–ä¸Šä¸€ä¸ªäº¤æ˜“æ—¥çš„ç½®ä¿¡åº¦
            last_confidence = self._get_last_confidence()
            
            if last_confidence is None:
                # ç¬¬ä¸€æ¬¡é¢„æµ‹ï¼Œç›´æ¥è¿”å›
                smoothed_confidence = raw_confidence
            else:
                # è®¡ç®—åŠ¨æ€æœ€å¤§å˜åŒ–é™åˆ¶
                dynamic_max_change = self._calculate_dynamic_max_change(market_data, raw_confidence, last_confidence)
                
                # åº”ç”¨EMAå¹³æ»‘
                smoothed_confidence = (
                    self.ema_alpha * raw_confidence + 
                    (1 - self.ema_alpha) * last_confidence
                )
                
                # é™åˆ¶æœ€å¤§å˜åŒ–å¹…åº¦ï¼ˆä½¿ç”¨åŠ¨æ€é™åˆ¶ï¼‰
                change = smoothed_confidence - last_confidence
                if abs(change) > dynamic_max_change:
                    if change > 0:
                        smoothed_confidence = last_confidence + dynamic_max_change
                    else:
                        smoothed_confidence = last_confidence - dynamic_max_change
                        
                self.logger.info(f"ç½®ä¿¡åº¦å¹³æ»‘: {raw_confidence:.4f} â†’ {smoothed_confidence:.4f} "
                               f"(å˜åŒ–: {smoothed_confidence-last_confidence:+.4f}, "
                               f"é™åˆ¶: Â±{dynamic_max_change:.3f})")
            
            # ç¡®ä¿åœ¨æœ‰æ•ˆèŒƒå›´å†…
            smoothed_confidence = max(0.0, min(1.0, smoothed_confidence))
            
            # ä¿å­˜åˆ°å†å²è®°å½•
            self._save_confidence(date, raw_confidence, smoothed_confidence)
            
            return smoothed_confidence
            
        except Exception as e:
            self.logger.error(f"ç½®ä¿¡åº¦å¹³æ»‘å¤±è´¥: {e}")
            return raw_confidence
    
    def _calculate_dynamic_max_change(self, market_data: pd.DataFrame, raw_confidence: float, last_confidence: float) -> float:
        """
        è®¡ç®—åŠ¨æ€æœ€å¤§å˜åŒ–é™åˆ¶ï¼ˆé…ç½®åŒ–ç‰ˆæœ¬ï¼‰
        
        å‚æ•°:
        market_data: å¸‚åœºæ•°æ®
        raw_confidence: åŸå§‹ç½®ä¿¡åº¦
        last_confidence: ä¸Šæ¬¡ç½®ä¿¡åº¦
        
        è¿”å›:
        float: åŠ¨æ€æœ€å¤§å˜åŒ–é™åˆ¶
        """
        base_limit = self.max_daily_change  # åŸºç¡€é™åˆ¶
        
        # å¦‚æœæœªå¯ç”¨åŠ¨æ€è°ƒæ•´æˆ–æ²¡æœ‰å¸‚åœºæ•°æ®ï¼Œä½¿ç”¨åŸºç¡€é™åˆ¶
        if not self.dynamic_enabled or market_data is None or len(market_data) < 20:
            return base_limit
        
        try:
            volatility_factor = 1.0
            price_factor = 1.0
            volume_factor = 1.0
            change_factor = 1.0
            
            # 1. è®¡ç®—å¸‚åœºæ³¢åŠ¨æ€§å› å­
            if self.volatility_config.get('enabled', True):
                recent_volatility = market_data['volatility'].tail(5).mean() if 'volatility' in market_data.columns else 0
                historical_volatility = market_data['volatility'].tail(60).mean() if 'volatility' in market_data.columns else recent_volatility
                
                if historical_volatility > 0:
                    volatility_ratio = recent_volatility / historical_volatility
                    max_mult = self.volatility_config.get('max_multiplier', 2.0)
                    min_mult = self.volatility_config.get('min_multiplier', 0.5)
                    volatility_factor = min(max_mult, max(min_mult, volatility_ratio))
            
            # 2. è®¡ç®—ä»·æ ¼å˜åŒ–å› å­
            if self.price_config.get('enabled', True) and 'close' in market_data.columns and len(market_data) >= 2:
                latest_price = market_data['close'].iloc[-1]
                prev_price = market_data['close'].iloc[-2]
                price_change = abs(latest_price - prev_price) / prev_price
                
                sensitivity = self.price_config.get('sensitivity', 10)
                max_mult = self.price_config.get('max_multiplier', 2.0)
                price_factor = min(max_mult, 1.0 + price_change * sensitivity)
            
            # 3. è®¡ç®—æˆäº¤é‡å› å­
            if self.volume_config.get('enabled', True) and 'volume' in market_data.columns and len(market_data) >= 20:
                recent_volume = market_data['volume'].tail(3).mean()
                avg_volume = market_data['volume'].tail(20).mean()
                
                if avg_volume > 0:
                    volume_ratio = recent_volume / avg_volume
                    panic_threshold = self.volume_config.get('panic_threshold', 1.5)
                    low_threshold = self.volume_config.get('low_threshold', 0.7)
                    max_mult = self.volume_config.get('max_multiplier', 1.8)
                    
                    # æˆäº¤é‡å¼‚å¸¸æ—¶ï¼ˆææ…Œæˆ–ç‹‚çƒ­ï¼‰æ”¾å®½é™åˆ¶
                    if volume_ratio > panic_threshold or volume_ratio < low_threshold:
                        volume_factor = min(max_mult, 1.0 + abs(volume_ratio - 1.0))
            
            # 4. è®¡ç®—ç½®ä¿¡åº¦å˜åŒ–å¹…åº¦å› å­
            if self.confidence_config.get('enabled', True):
                confidence_change = abs(raw_confidence - last_confidence)
                threshold = self.confidence_config.get('large_change_threshold', 0.5)
                max_mult = self.confidence_config.get('max_multiplier', 1.5)
                
                if confidence_change > threshold:
                    change_factor = min(max_mult, 1.0 + (confidence_change - threshold))
            
            # 5. ç»¼åˆè®¡ç®—åŠ¨æ€é™åˆ¶
            dynamic_limit = base_limit * volatility_factor * price_factor * volume_factor * change_factor
            
            # ä½¿ç”¨é…ç½®çš„ä¸Šä¸‹ç•Œ
            dynamic_limit = max(self.min_limit, min(self.max_limit, dynamic_limit))
            
            # è®°å½•è°ƒæ•´ä¿¡æ¯
            if self.log_adjustments:
                log_level = logging.DEBUG if not self.debug_mode else logging.INFO
                self.logger.log(log_level, 
                              f"åŠ¨æ€é™åˆ¶è®¡ç®—: åŸºç¡€={base_limit:.3f}, "
                              f"æ³¢åŠ¨={volatility_factor:.2f}, ä»·æ ¼={price_factor:.2f}, "
                              f"æˆäº¤é‡={volume_factor:.2f}, å˜åŒ–={change_factor:.2f}, "
                              f"æœ€ç»ˆ={dynamic_limit:.3f}")
            
            return dynamic_limit
            
        except Exception as e:
            self.logger.warning(f"åŠ¨æ€é™åˆ¶è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€é™åˆ¶: {e}")
            return base_limit
    
    def _load_confidence_history(self) -> List[Dict]:
        """åŠ è½½ç½®ä¿¡åº¦å†å²"""
        try:
            if os.path.exists(self.history_path):
                with open(self.history_path, 'r') as f:
                    return json.load(f)
        except Exception as e:
            self.logger.warning(f"åŠ è½½ç½®ä¿¡åº¦å†å²å¤±è´¥: {e}")
        return []
    
    def _get_last_confidence(self) -> Optional[float]:
        """è·å–æœ€è¿‘çš„ç½®ä¿¡åº¦"""
        if self.confidence_history:
            return self.confidence_history[-1].get('smoothed_confidence')
        return None
    
    def _save_confidence(self, date: str, raw: float, smoothed: float):
        """ä¿å­˜ç½®ä¿¡åº¦è®°å½•"""
        try:
            # æ·»åŠ æ–°è®°å½•
            self.confidence_history.append({
                'date': str(date),
                'raw_confidence': float(raw),
                'smoothed_confidence': float(smoothed),
                'timestamp': datetime.now().isoformat()
            })
            
            # åªä¿ç•™æœ€è¿‘30å¤©çš„è®°å½•
            if len(self.confidence_history) > 30:
                self.confidence_history = self.confidence_history[-30:]
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            os.makedirs(os.path.dirname(self.history_path), exist_ok=True)
            with open(self.history_path, 'w') as f:
                json.dump(self.confidence_history, f, indent=2)
                
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç½®ä¿¡åº¦å†å²å¤±è´¥: {e}")


class AIOptimizerImproved:
    """æ”¹è¿›ç‰ˆAIä¼˜åŒ–å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–æ¨¡å‹ç›¸å…³å±æ€§
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
        self.models_dir = os.path.join(project_root, 'models')
        os.makedirs(self.models_dir, exist_ok=True)
        
        self.model = None
        self.scaler = None
        self.feature_names = None
        self.model_type = config.get('ai', {}).get('model_type', 'machine_learning')
        
        # å¢é‡å­¦ä¹ é…ç½®
        incremental_config = config.get('ai', {}).get('incremental_learning', {})
        self.incremental_enabled = incremental_config.get('enabled', True)
        self.retrain_threshold = incremental_config.get('retrain_threshold', 0.1)  # æ¨¡å‹æ€§èƒ½ä¸‹é™é˜ˆå€¼
        self.max_incremental_updates = incremental_config.get('max_updates', 10)  # æœ€å¤§å¢é‡æ›´æ–°æ¬¡æ•°
        self.incremental_count = 0
        
        # ç½®ä¿¡åº¦å¹³æ»‘å™¨
        self.confidence_smoother = ConfidenceSmoother(config)
        
        self.logger.info("æ”¹è¿›ç‰ˆAIä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def prepare_features_improved(self, data: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:
        """
        æ”¹è¿›çš„ç‰¹å¾å‡†å¤‡ï¼Œè°ƒæ•´æƒé‡å’Œå¢åŠ è¶‹åŠ¿ç¡®è®¤æŒ‡æ ‡
        
        å‚æ•°:
        data: å†å²æ•°æ®
        
        è¿”å›:
        tuple: (ç‰¹å¾çŸ©é˜µ, ç‰¹å¾åç§°åˆ—è¡¨)
        """
        self.logger.info("å‡†å¤‡æ”¹è¿›çš„æœºå™¨å­¦ä¹ ç‰¹å¾")
        
        # è®¡ç®—é¢å¤–çš„è¶‹åŠ¿ç¡®è®¤æŒ‡æ ‡
        data = self._calculate_trend_indicators(data)
        
        # é‡æ–°è®¾è®¡ç‰¹å¾åˆ—ï¼Œé™ä½çŸ­æœŸæŒ‡æ ‡æƒé‡ï¼Œå¢åŠ è¶‹åŠ¿ç¡®è®¤æŒ‡æ ‡
        feature_columns = [
            # é•¿æœŸè¶‹åŠ¿æŒ‡æ ‡ï¼ˆé«˜æƒé‡ï¼‰
            'ma20', 'ma60',  # é•¿æœŸå‡çº¿
            'trend_strength_20', 'trend_strength_60',  # è¶‹åŠ¿å¼ºåº¦
            'price_position_20', 'price_position_60',  # ä»·æ ¼åœ¨å‡çº¿ç³»ç»Ÿä¸­çš„ä½ç½®
            
            # ä¸­æœŸè¶‹åŠ¿æŒ‡æ ‡ï¼ˆä¸­ç­‰æƒé‡ï¼‰
            'ma10', 'dist_ma10', 'dist_ma20',
            'rsi', 'macd', 'signal',
            'bb_upper', 'bb_lower',
            'volatility_normalized',  # æ ‡å‡†åŒ–æ³¢åŠ¨ç‡
            
            # çŸ­æœŸæŒ‡æ ‡ï¼ˆé™ä½æƒé‡ï¼‰
            'ma5', 'dist_ma5', 'hist',
            'price_change_5d', 'price_change_10d',
            
            # æˆäº¤é‡å’Œæ³¢åŠ¨ç‡ï¼ˆå¹³è¡¡æƒé‡ï¼‰
            'volume_trend', 'volume_strength',  # æˆäº¤é‡è¶‹åŠ¿
            'volatility'
        ]
        
        # è¿‡æ»¤å­˜åœ¨çš„åˆ—
        available_columns = [col for col in feature_columns if col in data.columns]
        
        if len(available_columns) == 0:
            self.logger.warning("æ²¡æœ‰å¯ç”¨çš„ç‰¹å¾åˆ—")
            return np.array([]), []
        
        # æå–ç‰¹å¾å¹¶åº”ç”¨æƒé‡
        features_df = data[available_columns].fillna(0).copy()
        features = self._apply_feature_weights(features_df, available_columns)
        
        self.logger.info("æ”¹è¿›ç‰¹å¾å‡†å¤‡å®Œæˆï¼Œç‰¹å¾æ•°é‡: %d, æ ·æœ¬æ•°é‡: %d", 
                        len(available_columns), len(features))
        
        return features, available_columns
    
    def _calculate_trend_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—è¶‹åŠ¿ç¡®è®¤æŒ‡æ ‡
        
        å‚æ•°:
        data: åŸå§‹æ•°æ®
        
        è¿”å›:
        pd.DataFrame: æ·»åŠ äº†è¶‹åŠ¿æŒ‡æ ‡çš„æ•°æ®
        """
        # è¶‹åŠ¿å¼ºåº¦æŒ‡æ ‡ï¼ˆåŸºäºçº¿æ€§å›å½’æ–œç‡ï¼‰
        for period in [20, 60]:
            slopes = []
            for i in range(len(data)):
                if i >= period - 1:
                    prices = data['close'].iloc[i-period+1:i+1].values
                    x = np.arange(period)
                    slope = np.polyfit(x, prices, 1)[0]
                    # æ ‡å‡†åŒ–æ–œç‡
                    normalized_slope = slope / prices.mean()
                    slopes.append(normalized_slope)
                else:
                    slopes.append(0)
            data[f'trend_strength_{period}'] = slopes
        
        # ä»·æ ¼åœ¨å‡çº¿ç³»ç»Ÿä¸­çš„ä½ç½®
        data['price_position_20'] = (data['close'] - data['ma20']) / data['ma20']
        data['price_position_60'] = (data['close'] - data['ma60']) / data['ma60']
        
        # æ ‡å‡†åŒ–æ³¢åŠ¨ç‡
        data['volatility_normalized'] = data['volatility'] / data['volatility'].rolling(60).mean()
        
        # æˆäº¤é‡è¶‹åŠ¿æŒ‡æ ‡
        data['volume_ma20'] = data['volume'].rolling(20).mean()
        data['volume_trend'] = (data['volume'] - data['volume_ma20']) / data['volume_ma20']
        
        # æˆäº¤é‡å¼ºåº¦ï¼ˆç›¸å¯¹äºå†å²ï¼‰
        data['volume_strength'] = data['volume'] / data['volume'].rolling(60).mean()
        
        return data
    
    def _apply_feature_weights(self, features_df: pd.DataFrame, feature_names: List[str]) -> np.ndarray:
        """
        åº”ç”¨ç‰¹å¾æƒé‡ï¼Œé™ä½çŸ­æœŸæŒ‡æ ‡å½±å“
        
        å‚æ•°:
        features_df: ç‰¹å¾æ•°æ®æ¡†
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        
        è¿”å›:
        np.ndarray: åŠ æƒåçš„ç‰¹å¾çŸ©é˜µ
        """
        # å®šä¹‰ç‰¹å¾æƒé‡
        feature_weights = {
            # é•¿æœŸè¶‹åŠ¿æŒ‡æ ‡ï¼ˆé«˜æƒé‡ï¼‰
            'ma20': 1.5, 'ma60': 1.5,
            'trend_strength_20': 2.0, 'trend_strength_60': 2.0,
            'price_position_20': 1.8, 'price_position_60': 1.8,
            
            # ä¸­æœŸæŒ‡æ ‡ï¼ˆæ­£å¸¸æƒé‡ï¼‰
            'ma10': 1.0, 'dist_ma10': 1.2, 'dist_ma20': 1.2,
            'rsi': 1.0, 'macd': 1.0, 'signal': 1.0,
            'bb_upper': 1.0, 'bb_lower': 1.0,
            'volatility_normalized': 1.0,
            
            # çŸ­æœŸæŒ‡æ ‡ï¼ˆé™ä½æƒé‡ï¼‰
            'ma5': 0.6, 'dist_ma5': 0.6, 'hist': 0.7,
            'price_change_5d': 0.5, 'price_change_10d': 0.7,
            
            # æˆäº¤é‡æŒ‡æ ‡ï¼ˆå¹³è¡¡æƒé‡ï¼‰
            'volume_trend': 1.1, 'volume_strength': 1.1,
            'volatility': 0.9
        }
        
        # åº”ç”¨æƒé‡
        weighted_features = features_df.copy()
        for feature in feature_names:
            weight = feature_weights.get(feature, 1.0)
            weighted_features[feature] = weighted_features[feature] * weight
        
        return weighted_features.values
    
    def incremental_train(self, new_data: pd.DataFrame, strategy_module) -> Dict[str, Any]:
        """
        å¢é‡è®­ç»ƒæ¨¡å‹
        
        å‚æ•°:
        new_data: æ–°å¢æ•°æ®
        strategy_module: ç­–ç•¥æ¨¡å—
        
        è¿”å›:
        dict: è®­ç»ƒç»“æœ
        """
        self.logger.info("å¼€å§‹å¢é‡è®­ç»ƒæ¨¡å‹")
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å®Œå…¨é‡è®­ç»ƒ
            if self.model is None or self.incremental_count >= self.max_incremental_updates:
                self.logger.info("è§¦å‘å®Œå…¨é‡è®­ç»ƒæ¡ä»¶")
                return self.full_train(new_data, strategy_module)
            
            # å‡†å¤‡æ–°æ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾
            new_features, feature_names = self.prepare_features_improved(new_data)
            new_labels = self._prepare_labels(new_data, strategy_module)
            
            if len(new_features) == 0 or len(new_labels) == 0:
                self.logger.warning("æ–°æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å¢é‡è®­ç»ƒ")
                return {'success': False, 'error': 'æ–°æ•°æ®ä¸ºç©º'}
            
            # æ£€æŸ¥ç‰¹å¾ä¸€è‡´æ€§
            if self.feature_names and feature_names != self.feature_names:
                self.logger.warning("ç‰¹å¾ä¸ä¸€è‡´ï¼Œè¿›è¡Œå®Œå…¨é‡è®­ç»ƒ")
                return self.full_train(new_data, strategy_module)
            
            # ä½¿ç”¨æœ€è¿‘çš„æ•°æ®è¿›è¡Œå¢é‡æ›´æ–°
            recent_features = new_features[-10:]  # æœ€è¿‘10å¤©çš„æ•°æ®
            recent_labels = new_labels[-10:]
            
            if len(recent_features) > 0:
                # å¯¹æ–°æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆä½¿ç”¨å·²æœ‰çš„scalerï¼‰
                if self.scaler is not None:
                    recent_features_scaled = self.scaler.transform(recent_features)
                else:
                    self.logger.warning("ç¼ºå°‘scalerï¼Œè¿›è¡Œå®Œå…¨é‡è®­ç»ƒ")
                    return self.full_train(new_data, strategy_module)
                
                # ä½¿ç”¨warm_startè¿›è¡Œå¢é‡å­¦ä¹ 
                if hasattr(self.model.named_steps['classifier'], 'n_estimators'):
                    classifier = self.model.named_steps['classifier']
                    classifier.n_estimators += 10  # å¢åŠ æ ‘çš„æ•°é‡
                    classifier.warm_start = True
                    
                    # é‡æ–°è®­ç»ƒï¼ˆè¿™é‡Œå®é™…ä¸Šæ˜¯å¢é‡çš„ï¼‰
                    self.model.named_steps['classifier'].fit(recent_features_scaled, recent_labels)
                    
                    self.incremental_count += 1
                    self.logger.info(f"å¢é‡è®­ç»ƒå®Œæˆï¼Œæ›´æ–°æ¬¡æ•°: {self.incremental_count}")
                    
                    return {
                        'success': True,
                        'method': 'incremental',
                        'update_count': self.incremental_count,
                        'new_samples': len(recent_features)
                    }
                else:
                    self.logger.warning("æ¨¡å‹ä¸æ”¯æŒå¢é‡å­¦ä¹ ï¼Œè¿›è¡Œå®Œå…¨é‡è®­ç»ƒ")
                    return self.full_train(new_data, strategy_module)
            
            return {'success': False, 'error': 'æ²¡æœ‰è¶³å¤Ÿçš„æ–°æ•°æ®è¿›è¡Œå¢é‡è®­ç»ƒ'}
            
        except Exception as e:
            self.logger.error(f"å¢é‡è®­ç»ƒå¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿›è¡Œå®Œå…¨é‡è®­ç»ƒ
            return self.full_train(new_data, strategy_module)
    
    def full_train(self, data: pd.DataFrame, strategy_module) -> Dict[str, Any]:
        """
        å®Œå…¨é‡è®­ç»ƒæ¨¡å‹
        
        å‚æ•°:
        data: è®­ç»ƒæ•°æ®
        strategy_module: ç­–ç•¥æ¨¡å—
        
        è¿”å›:
        dict: è®­ç»ƒç»“æœ
        """
        self.logger.info("å¼€å§‹å®Œå…¨é‡è®­ç»ƒæ¨¡å‹")
        
        try:
            # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
            features, feature_names = self.prepare_features_improved(data)
            labels = self._prepare_labels(data, strategy_module)
            
            if len(features) == 0 or len(labels) == 0:
                self.logger.error("ç‰¹å¾æˆ–æ ‡ç­¾ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹")
                return {'success': False, 'error': 'ç‰¹å¾æˆ–æ ‡ç­¾ä¸ºç©º'}
            
            # æ•°æ®åˆ†å‰²
            min_length = min(len(features), len(labels))
            features = features[:min_length]
            labels = labels[:min_length]
            aligned_data = data.iloc[:min_length].copy()
            
            split_ratio = self.config.get("ai", {}).get("train_test_split_ratio", 0.8)
            split_index = int(len(features) * split_ratio)
            
            X_train = features[:split_index]
            y_train = labels[:split_index]
            train_dates = aligned_data["date"].iloc[:split_index]
            
            # è®¡ç®—æ ·æœ¬æƒé‡
            sample_weights = self._calculate_sample_weights(train_dates)
            
            # åˆ›å»ºæ–°æ¨¡å‹
            self.scaler = StandardScaler()
            X_train_scaled = self.scaler.fit_transform(X_train)
            
            # ä½¿ç”¨æ”¹è¿›çš„RandomForestå‚æ•°
            classifier = RandomForestClassifier(
                n_estimators=150,  # å¢åŠ æ ‘çš„æ•°é‡
                max_depth=12,      # é€‚å½“å¢åŠ æ·±åº¦
                min_samples_split=8,  # å¢åŠ åˆ†å‰²æ ·æœ¬æ•°
                min_samples_leaf=3,   # å¢åŠ å¶å­èŠ‚ç‚¹æ ·æœ¬æ•°
                random_state=42,
                class_weight='balanced',
                warm_start=True,  # æ”¯æŒå¢é‡å­¦ä¹ 
                n_jobs=-1         # å¹¶è¡Œè®­ç»ƒ
            )
            
            # è®­ç»ƒåˆ†ç±»å™¨
            classifier.fit(X_train_scaled, y_train, sample_weight=sample_weights)
            
            # åˆ›å»ºå®Œæ•´çš„pipeline
            self.model = Pipeline([
                ('scaler', self.scaler),
                ('classifier', classifier)
            ])
            
            self.feature_names = feature_names
            self.incremental_count = 0  # é‡ç½®å¢é‡è®¡æ•°
            
            # ä¿å­˜æ¨¡å‹
            self._save_model()
            
            self.logger.info("å®Œå…¨é‡è®­ç»ƒå®Œæˆ")
            return {
                'success': True,
                'method': 'full_retrain',
                'train_samples': len(X_train),
                'feature_count': len(feature_names)
            }
            
        except Exception as e:
            self.logger.error(f"å®Œå…¨é‡è®­ç»ƒå¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def predict_low_point(self, data: pd.DataFrame, prediction_date: str = None) -> Dict[str, Any]:
        """
        é¢„æµ‹ç›¸å¯¹ä½ç‚¹ï¼ˆå¸¦ç½®ä¿¡åº¦å¹³æ»‘ï¼‰
        
        å‚æ•°:
        data: å¸‚åœºæ•°æ®
        prediction_date: é¢„æµ‹æ—¥æœŸï¼ˆç”¨äºç½®ä¿¡åº¦å¹³æ»‘ï¼‰
        
        è¿”å›:
        dict: é¢„æµ‹ç»“æœ
        """
        self.logger.info("é¢„æµ‹ç›¸å¯¹ä½ç‚¹ï¼ˆæ”¹è¿›ç‰ˆï¼‰")
        
        try:
            # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœæœªåŠ è½½ï¼‰
            if self.model is None:
                if not self._load_model():
                    return {
                        'is_low_point': False,
                        'confidence': 0.0,
                        'smoothed_confidence': 0.0,
                        'error': 'æ¨¡å‹æœªè®­ç»ƒä¸”æ— æ³•åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹'
                    }
            
            if len(data) == 0:
                return {
                    'is_low_point': False,
                    'confidence': 0.0,
                    'smoothed_confidence': 0.0,
                    'error': 'æ•°æ®ä¸ºç©º'
                }
            
            # å‡†å¤‡ç‰¹å¾
            features, feature_names = self.prepare_features_improved(data)
            
            if len(features) == 0:
                return {
                    'is_low_point': False,
                    'confidence': 0.0,
                    'smoothed_confidence': 0.0,
                    'error': 'æ— æ³•æå–ç‰¹å¾'
                }
            
            # ä½¿ç”¨æœ€æ–°æ•°æ®è¿›è¡Œé¢„æµ‹
            latest_features = features[-1:].reshape(1, -1)
            
            # è·å–é¢„æµ‹æ¦‚ç‡ï¼ˆä¸ä½¿ç”¨predictæ–¹æ³•ï¼Œé¿å…å†…ç½®é˜ˆå€¼å½±å“ï¼‰
            prediction_proba = self.model.predict_proba(latest_features)[0]
            
            # è·å–åŸå§‹ç½®ä¿¡åº¦
            raw_confidence = prediction_proba[1] if len(prediction_proba) > 1 else 0.0
            
            # åº”ç”¨ç½®ä¿¡åº¦å¹³æ»‘ï¼ˆä¼ é€’å¸‚åœºæ•°æ®ï¼‰
            if prediction_date:
                smoothed_confidence = self.confidence_smoother.smooth_confidence(
                    raw_confidence, prediction_date, data
                )
            else:
                smoothed_confidence = raw_confidence
            
            # ä½¿ç”¨é…ç½®çš„é˜ˆå€¼å’Œå¹³æ»‘åçš„ç½®ä¿¡åº¦è¿›è¡Œæœ€ç»ˆé¢„æµ‹
            confidence_config = self.config.get('strategy', {}).get('confidence_weights', {})
            final_threshold = confidence_config.get('final_threshold', 0.5)
            
            # åŸºäºå¹³æ»‘åçš„ç½®ä¿¡åº¦å’Œé…ç½®é˜ˆå€¼è¿›è¡Œé¢„æµ‹
            is_low_point = smoothed_confidence >= final_threshold
            
            result = {
                'is_low_point': bool(is_low_point),
                'confidence': float(raw_confidence),
                'smoothed_confidence': float(smoothed_confidence),
                'prediction_proba': prediction_proba.tolist(),
                'feature_count': len(feature_names),
                'model_type': type(self.model.named_steps['classifier']).__name__,
                'threshold_used': final_threshold
            }
            
            # è¾“å‡ºé¢„æµ‹ç»“æœ
            self.logger.info("----------------------------------------------------")
            self.logger.info("AIé¢„æµ‹ç»“æœï¼ˆæ”¹è¿›ç‰ˆï¼‰: \033[1m%s\033[0m", 
                           "ç›¸å¯¹ä½ç‚¹" if is_low_point else "éç›¸å¯¹ä½ç‚¹")
            self.logger.info("åŸå§‹ç½®ä¿¡åº¦: \033[1m%.4f\033[0m, å¹³æ»‘ç½®ä¿¡åº¦: \033[1m%.4f\033[0m, é˜ˆå€¼: \033[1m%.2f\033[0m", 
                           raw_confidence, smoothed_confidence, final_threshold)
            self.logger.info("----------------------------------------------------")
            
            return result
            
        except Exception as e:
            self.logger.error(f"é¢„æµ‹ç›¸å¯¹ä½ç‚¹å¤±è´¥: {e}")
            return {
                'is_low_point': False,
                'confidence': 0.0,
                'smoothed_confidence': 0.0,
                'error': str(e)
            }
    
    def _prepare_labels(self, data: pd.DataFrame, strategy_module) -> np.ndarray:
        """å‡†å¤‡æ ‡ç­¾"""
        backtest_results = strategy_module.backtest(data)
        return backtest_results['is_low_point'].astype(int).values
    
    def _calculate_sample_weights(self, dates: pd.Series) -> np.ndarray:
        """è®¡ç®—æ ·æœ¬æƒé‡"""
        weights = np.ones(len(dates))
        if len(dates) == 0:
            return weights
        
        latest_date = dates.max()
        decay_rate = self.config.get("ai", {}).get("data_decay_rate", 0.4)
        
        for i, date in enumerate(dates):
            time_diff = (latest_date - date).days / 365.25
            weight = np.exp(-decay_rate * time_diff)
            weights[i] = weight
        
        return weights
    
    def _save_model(self) -> bool:
        """ä¿å­˜æ¨¡å‹"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # ä¿å­˜æ¨¡å‹
            model_path = os.path.join(self.models_dir, f'improved_model_{timestamp}.pkl')
            with open(model_path, 'wb') as f:
                pickle.dump({
                    'model': self.model,
                    'feature_names': self.feature_names,
                    'incremental_count': self.incremental_count,
                    'scaler': self.scaler
                }, f)
            
            # ä¿å­˜æœ€æ–°æ¨¡å‹è·¯å¾„
            latest_path = os.path.join(self.models_dir, 'latest_improved_model.txt')
            with open(latest_path, 'w') as f:
                f.write(model_path)
            
            self.logger.info(f"æ”¹è¿›æ¨¡å‹ä¿å­˜æˆåŠŸ: {model_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ”¹è¿›æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def _load_model(self) -> bool:
        """åŠ è½½æ¨¡å‹"""
        try:
            latest_path = os.path.join(self.models_dir, 'latest_improved_model.txt')
            
            if not os.path.exists(latest_path):
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°å·²ä¿å­˜çš„æ”¹è¿›æ¨¡å‹")
                return False
            
            with open(latest_path, 'r') as f:
                model_path = f.read().strip()
            
            with open(model_path, 'rb') as f:
                data = pickle.load(f)
                self.model = data['model']
                self.feature_names = data['feature_names']
                self.incremental_count = data.get('incremental_count', 0)
                self.scaler = data.get('scaler')
            
            self.logger.info(f"æ”¹è¿›æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æ”¹è¿›æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def run_complete_optimization(self, data: pd.DataFrame, strategy_module) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„AIä¼˜åŒ–æµç¨‹ï¼ˆåŒ…å«å‚æ•°ä¼˜åŒ– + æ¨¡å‹è®­ç»ƒï¼‰
        
        å‚æ•°:
        data: å†å²æ•°æ®
        strategy_module: ç­–ç•¥æ¨¡å—
        
        è¿”å›:
        dict: ä¼˜åŒ–ç»“æœ
        """
        self.logger.info("ğŸš€ å¼€å§‹å®Œæ•´çš„AIä¼˜åŒ–æµç¨‹ï¼ˆæ”¹è¿›ç‰ˆï¼‰")
        
        try:
            optimization_result = {
                'success': False,
                'strategy_optimization': {},
                'model_training': {},
                'final_evaluation': {},
                'errors': []
            }
            
            # 1. ç­–ç•¥å‚æ•°ä¼˜åŒ–
            self.logger.info("ğŸ”§ æ­¥éª¤1: ç­–ç•¥å‚æ•°ä¼˜åŒ–")
            strategy_result = self.optimize_strategy_parameters_improved(strategy_module, data)
            optimization_result['strategy_optimization'] = strategy_result
            
            if strategy_result['success']:
                # æ›´æ–°ç­–ç•¥æ¨¡å—å‚æ•°
                strategy_module.update_params(strategy_result['best_params'])
                self.logger.info(f"âœ… ç­–ç•¥å‚æ•°ä¼˜åŒ–å®Œæˆ: {strategy_result['best_params']}")
            else:
                self.logger.warning("âš ï¸ ç­–ç•¥å‚æ•°ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°ç»§ç»­")
                optimization_result['errors'].append("ç­–ç•¥å‚æ•°ä¼˜åŒ–å¤±è´¥")
            
            # 2. æ”¹è¿›ç‰ˆæ¨¡å‹è®­ç»ƒ
            self.logger.info("ğŸ¤– æ­¥éª¤2: æ”¹è¿›ç‰ˆæ¨¡å‹è®­ç»ƒ")
            model_result = self.full_train(data, strategy_module)
            optimization_result['model_training'] = model_result
            
            if not model_result['success']:
                self.logger.error("âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥")
                optimization_result['errors'].append("æ¨¡å‹è®­ç»ƒå¤±è´¥")
                return optimization_result
            
            # 3. æœ€ç»ˆè¯„ä¼°
            self.logger.info("ğŸ“Š æ­¥éª¤3: æœ€ç»ˆæ€§èƒ½è¯„ä¼°")
            evaluation_result = self.evaluate_optimized_system(data, strategy_module)
            optimization_result['final_evaluation'] = evaluation_result
            
            # 4. ä¿å­˜ä¼˜åŒ–ç»“æœ
            if strategy_result['success']:
                self.save_optimized_params(strategy_result['best_params'])
            
            optimization_result['success'] = model_result['success']
            
            self.logger.info("ğŸ‰ å®Œæ•´AIä¼˜åŒ–æµç¨‹å®Œæˆ")
            return optimization_result
            
        except Exception as e:
            self.logger.error(f"å®Œæ•´AIä¼˜åŒ–æµç¨‹å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'strategy_optimization': {},
                'model_training': {},
                'final_evaluation': {}
            }
    
    def optimize_strategy_parameters_improved(self, strategy_module, data: pd.DataFrame) -> Dict[str, Any]:
        """
        æ”¹è¿›ç‰ˆç­–ç•¥å‚æ•°ä¼˜åŒ–ï¼ˆä½¿ç”¨ä¸¥æ ¼ä¸‰å±‚æ•°æ®åˆ†å‰²ï¼‰
        
        å‚æ•°:
        strategy_module: ç­–ç•¥æ¨¡å—
        data: å†å²æ•°æ®
        
        è¿”å›:
        dict: ä¼˜åŒ–ç»“æœ
        """
        self.logger.info("å¼€å§‹æ”¹è¿›ç‰ˆç­–ç•¥å‚æ•°ä¼˜åŒ–ï¼ˆä¸¥æ ¼ä¸‰å±‚åˆ†å‰²ï¼‰")
        
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
            if len(data) < 100:
                return {
                    'success': False,
                    'error': 'æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå‚æ•°ä¼˜åŒ–'
                }
            
            # ä»é…ç½®æ–‡ä»¶è·å–ä¸‰å±‚æ•°æ®åˆ†å‰²æ¯”ä¾‹
            validation_config = self.config.get('ai', {}).get('validation', {})
            train_ratio = validation_config.get('train_ratio', 0.65)
            val_ratio = validation_config.get('validation_ratio', 0.2) 
            test_ratio = validation_config.get('test_ratio', 0.15)
            
            # éªŒè¯æ¯”ä¾‹æ€»å’Œ
            total_ratio = train_ratio + val_ratio + test_ratio
            if abs(total_ratio - 1.0) > 0.01:
                self.logger.warning(f"æ•°æ®åˆ†å‰²æ¯”ä¾‹æ€»å’Œä¸ç­‰äº1.0: {total_ratio:.3f}ï¼Œè‡ªåŠ¨è°ƒæ•´")
                # é‡æ–°å½’ä¸€åŒ–
                train_ratio = train_ratio / total_ratio
                val_ratio = val_ratio / total_ratio 
                test_ratio = test_ratio / total_ratio
            
            # è®¡ç®—åˆ†å‰²ç‚¹
            train_end = int(len(data) * train_ratio)
            val_end = int(len(data) * (train_ratio + val_ratio))
            
            # ä¸¥æ ¼ä¸‰å±‚æ•°æ®åˆ†å‰²
            train_data = data.iloc[:train_end].copy()
            validation_data = data.iloc[train_end:val_end].copy()
            test_data = data.iloc[val_end:].copy()
            
            self.logger.info(f"ä¸¥æ ¼ä¸‰å±‚æ•°æ®åˆ†å‰²:")
            self.logger.info(f"   ğŸ“Š è®­ç»ƒé›†: {len(train_data)}æ¡ ({train_ratio:.1%}) - ä»…ç”¨äºå‚æ•°ä¼˜åŒ–")
            self.logger.info(f"   ğŸ“ˆ éªŒè¯é›†: {len(validation_data)}æ¡ ({val_ratio:.1%}) - ç”¨äºæ¨¡å‹éªŒè¯å’Œè¿‡æ‹Ÿåˆæ£€æµ‹")
            self.logger.info(f"   ğŸ”’ æµ‹è¯•é›†: {len(test_data)}æ¡ ({test_ratio:.1%}) - å®Œå…¨é”å®šï¼Œä»…æœ€ç»ˆè¯„ä¼°")
            
            # å®šä¹‰ä¼˜åŒ–å‚æ•°èŒƒå›´
            param_ranges = self.config.get('optimization', {}).get('param_ranges', {})
            
            # ä½¿ç”¨ç½‘æ ¼æœç´¢è¿›è¡Œå‚æ•°ä¼˜åŒ–ï¼ˆä»…åœ¨è®­ç»ƒé›†ä¸Šï¼‰
            self.logger.info("ğŸ”§ æ­¥éª¤1: åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå‚æ•°æœç´¢...")
            best_params, best_score = self._grid_search_optimization(
                strategy_module, train_data, param_ranges
            )
            
            # åœ¨éªŒè¯é›†ä¸ŠéªŒè¯æœ€ä½³å‚æ•°
            self.logger.info("ğŸ“ˆ æ­¥éª¤2: åœ¨éªŒè¯é›†ä¸ŠéªŒè¯æœ€ä½³å‚æ•°...")
            strategy_module.update_params(best_params)
            val_backtest = strategy_module.backtest(validation_data)
            val_evaluation = strategy_module.evaluate_strategy(val_backtest)
            val_score = val_evaluation['score']
            val_success_rate = val_evaluation.get('success_rate', 0)
            val_total_points = val_evaluation.get('total_points', 0)
            val_avg_rise = val_evaluation.get('avg_rise', 0)
            
            # æ£€æŸ¥è¿‡æ‹Ÿåˆ
            overfitting_threshold = 0.8  # éªŒè¯é›†å¾—åˆ†åº”è¯¥è‡³å°‘æ˜¯è®­ç»ƒé›†å¾—åˆ†çš„80%
            overfitting_passed = val_score >= best_score * overfitting_threshold
            
            # åœ¨å®Œå…¨é”å®šçš„æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°
            self.logger.info("ğŸ”’ æ­¥éª¤3: åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
            test_backtest = strategy_module.backtest(test_data)
            test_evaluation = strategy_module.evaluate_strategy(test_backtest)
            test_score = test_evaluation['score']
            test_success_rate = test_evaluation.get('success_rate', 0)
            test_total_points = test_evaluation.get('total_points', 0)
            test_avg_rise = test_evaluation.get('avg_rise', 0)
            
            # è¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›
            generalization_ratio = test_score / val_score if val_score > 0 else 0
            generalization_passed = generalization_ratio >= 0.85  # æµ‹è¯•é›†å¾—åˆ†åº”è¯¥æ¥è¿‘éªŒè¯é›†
            
            self.logger.info(f"âœ… ä¸‰å±‚éªŒè¯ç»“æœ:")
            self.logger.info(f"   ğŸ“Š è®­ç»ƒé›†å¾—åˆ†: {best_score:.4f}")
            self.logger.info(f"   ğŸ“ˆ éªŒè¯é›†å¾—åˆ†: {val_score:.4f} | æˆåŠŸç‡: {val_success_rate:.2%} | è¯†åˆ«ç‚¹æ•°: {val_total_points} | å¹³å‡æ¶¨å¹…: {val_avg_rise:.2%}")
            self.logger.info(f"   ğŸ”’ æµ‹è¯•é›†å¾—åˆ†: {test_score:.4f} | æˆåŠŸç‡: {test_success_rate:.2%} | è¯†åˆ«ç‚¹æ•°: {test_total_points} | å¹³å‡æ¶¨å¹…: {test_avg_rise:.2%}")
            self.logger.info(f"   ğŸ›¡ï¸ è¿‡æ‹Ÿåˆæ£€æµ‹: {'âœ… é€šè¿‡' if overfitting_passed else 'âš ï¸ è­¦å‘Š'}")
            self.logger.info(f"   ğŸ¯ æ³›åŒ–èƒ½åŠ›: {'âœ… è‰¯å¥½' if generalization_passed else 'âš ï¸ ä¸€èˆ¬'} (æ¯”ç‡: {generalization_ratio:.3f})")
            
            return {
                'success': True,
                'best_params': best_params,
                'best_score': best_score,
                'validation_score': val_score,
                'validation_success_rate': val_success_rate,
                'validation_total_points': val_total_points,
                'validation_avg_rise': val_avg_rise,
                'test_score': test_score,
                'test_success_rate': test_success_rate,
                'test_total_points': test_total_points,
                'test_avg_rise': test_avg_rise,
                'overfitting_passed': overfitting_passed,
                'generalization_passed': generalization_passed,
                'generalization_ratio': generalization_ratio,
                'optimization_method': 'grid_search_improved_3layer',
                'data_split': {
                    'train_ratio': train_ratio,
                    'validation_ratio': val_ratio,
                    'test_ratio': test_ratio,
                    'train_samples': len(train_data),
                    'validation_samples': len(validation_data),
                    'test_samples': len(test_data)
                }
            }
            
        except Exception as e:
            self.logger.error(f"ç­–ç•¥å‚æ•°ä¼˜åŒ–å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _grid_search_optimization(self, strategy_module, train_data: pd.DataFrame, param_ranges: dict) -> tuple:
        """
        ç½‘æ ¼æœç´¢ä¼˜åŒ–
        
        å‚æ•°:
        strategy_module: ç­–ç•¥æ¨¡å—
        train_data: è®­ç»ƒæ•°æ®
        param_ranges: å‚æ•°èŒƒå›´
        
        è¿”å›:
        tuple: (æœ€ä½³å‚æ•°, æœ€ä½³å¾—åˆ†)
        """
        self.logger.info("å¼€å§‹ç½‘æ ¼æœç´¢ä¼˜åŒ–")
        
        # å®šä¹‰é»˜è®¤å‚æ•°èŒƒå›´
        default_ranges = {
            'rsi_oversold_threshold': {'min': 25, 'max': 35, 'step': 2},
            'rsi_low_threshold': {'min': 35, 'max': 45, 'step': 2},
            'final_threshold': {'min': 0.3, 'max': 0.7, 'step': 0.1}
        }
        
        # åˆå¹¶ç”¨æˆ·é…ç½®å’Œé»˜è®¤é…ç½®
        search_ranges = {**default_ranges, **param_ranges}
        
        # ç”Ÿæˆå‚æ•°ç»„åˆ
        param_combinations = []
        param_names = list(search_ranges.keys())
        
        def generate_range(param_config):
            start = param_config['min']
            end = param_config['max']
            step = param_config['step']
            return [start + i * step for i in range(int((end - start) / step) + 1)]
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆï¼ˆé™åˆ¶æ•°é‡ä»¥é¿å…è¿‡é•¿æ—¶é—´ï¼‰
        from itertools import product
        
        ranges = [generate_range(search_ranges[param]) for param in param_names]
        all_combinations = list(product(*ranges))
        
        # é™åˆ¶æœç´¢æ•°é‡
        max_combinations = 50
        if len(all_combinations) > max_combinations:
            import random
            random.seed(42)
            all_combinations = random.sample(all_combinations, max_combinations)
        
        self.logger.info(f"å°†æµ‹è¯• {len(all_combinations)} ä¸ªå‚æ•°ç»„åˆ")
        
        best_score = -float('inf')
        best_params = {}
        
        for i, combination in enumerate(all_combinations):
            # æ„å»ºå‚æ•°å­—å…¸
            params = dict(zip(param_names, combination))
            
            try:
                # æ›´æ–°å‚æ•°å¹¶æµ‹è¯•
                strategy_module.update_params(params)
                backtest_results = strategy_module.backtest(train_data)
                evaluation = strategy_module.evaluate_strategy(backtest_results)
                score = evaluation['score']
                
                if score > best_score:
                    best_score = score
                    best_params = params.copy()
                
                if (i + 1) % 10 == 0:
                    self.logger.info(f"å·²æµ‹è¯• {i + 1}/{len(all_combinations)} ä¸ªç»„åˆï¼Œå½“å‰æœ€ä½³å¾—åˆ†: {best_score:.4f}")
                    
            except Exception as e:
                self.logger.warning(f"å‚æ•°ç»„åˆ {params} æµ‹è¯•å¤±è´¥: {e}")
                continue
        
        self.logger.info(f"ç½‘æ ¼æœç´¢å®Œæˆï¼Œæœ€ä½³å¾—åˆ†: {best_score:.4f}")
        return best_params, best_score
    
    def evaluate_optimized_system(self, data: pd.DataFrame, strategy_module) -> Dict[str, Any]:
        """
        è¯„ä¼°ä¼˜åŒ–åçš„ç³»ç»Ÿ
        
        å‚æ•°:
        data: æµ‹è¯•æ•°æ®
        strategy_module: ç­–ç•¥æ¨¡å—
        
        è¿”å›:
        dict: è¯„ä¼°ç»“æœ
        """
        self.logger.info("è¯„ä¼°ä¼˜åŒ–åçš„ç³»ç»Ÿ")
        
        try:
            # ç­–ç•¥è¯„ä¼°
            backtest_results = strategy_module.backtest(data)
            strategy_evaluation = strategy_module.evaluate_strategy(backtest_results)
            
            # AIæ¨¡å‹é¢„æµ‹è¯„ä¼°
            prediction_result = self.predict_low_point(data)
            
            return {
                'success': True,
                'strategy_score': strategy_evaluation['score'],
                'strategy_success_rate': strategy_evaluation['success_rate'],
                'identified_points': strategy_evaluation['total_points'],
                'avg_rise': strategy_evaluation['avg_rise'],
                'ai_confidence': prediction_result.get('smoothed_confidence', 0),
                'ai_prediction': prediction_result.get('is_low_point', False)
            }
            
        except Exception as e:
            self.logger.error(f"ç³»ç»Ÿè¯„ä¼°å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def save_optimized_params(self, params: dict):
        """
        ä¿å­˜ä¼˜åŒ–åçš„å‚æ•°åˆ°é…ç½®æ–‡ä»¶
        
        å‚æ•°:
        params: ä¼˜åŒ–åçš„å‚æ•°
        """
        try:
            # ä¿å­˜åˆ°æ”¹è¿›ç‰ˆé…ç½®æ–‡ä»¶
            config_path = os.path.join(
                os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
                'config', 'config_improved.yaml'
            )
            
            if os.path.exists(config_path):
                import yaml
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                
                # æ›´æ–°ç­–ç•¥å‚æ•°
                if 'strategy' not in config:
                    config['strategy'] = {}
                
                for key, value in params.items():
                    config['strategy'][key] = value
                
                # ä¿å­˜æ›´æ–°åçš„é…ç½®
                with open(config_path, 'w', encoding='utf-8') as f:
                    yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
                
                self.logger.info(f"ä¼˜åŒ–å‚æ•°å·²ä¿å­˜åˆ°: {config_path}")
            else:
                self.logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
                
        except Exception as e:
            self.logger.error(f"ä¿å­˜ä¼˜åŒ–å‚æ•°å¤±è´¥: {e}") 