#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
AIä¼˜åŒ–å™¨
é›†æˆå¢é‡å­¦ä¹ ã€ç‰¹å¾æƒé‡ä¼˜åŒ–å’Œè¶‹åŠ¿ç¡®è®¤æŒ‡æ ‡
"""

import logging
import os
import pickle
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, precision_recall_curve
from typing import Dict, Any, Tuple, List, Optional
import json
import yaml
from itertools import product
import sys
import time

# è´å¶æ–¯ä¼˜åŒ–ç›¸å…³å¯¼å…¥

from skopt import gp_minimize
from skopt.space import Real, Integer
from skopt.utils import use_named_args
BAYESIAN_AVAILABLE = True

# å¯¼å…¥å·¥å…·å‡½æ•°
from src.utils.utils import resolve_confidence_param




class AIOptimizerImproved:
    """AIä¼˜åŒ–å™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # åˆå§‹åŒ–æ¨¡å‹ç›¸å…³å±æ€§
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
        self.models_dir = os.path.join(project_root, 'models')
        os.makedirs(self.models_dir, exist_ok=True)

        self.model = None
        self.scaler = None
        self.feature_names = None
        self.model_type = config.get('ai', {}).get('model_type', 'machine_learning')

        # å¢é‡å­¦ä¹ é…ç½®
        incremental_config = config.get('ai', {}).get('incremental_learning', {})
        self.incremental_enabled = incremental_config.get('enabled', True)
        self.retrain_threshold = incremental_config.get('retrain_threshold', 0.1)  # æ¨¡å‹æ€§èƒ½ä¸‹é™é˜ˆå€¼
        self.max_incremental_updates = incremental_config.get('max_updates', 10)  # æœ€å¤§å¢é‡æ›´æ–°æ¬¡æ•°
        self.incremental_count = 0

        # ç§»é™¤ç½®ä¿¡åº¦å¤„ç†å™¨ - ä½¿ç”¨æ¨¡å‹åŸå§‹è¾“å‡º
        # self.confidence_processor = ConfidenceProcessor(config)

        self.logger.info("AIä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")

    def prepare_features_improved(self, data: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:
        """
        æ”¹è¿›çš„ç‰¹å¾å‡†å¤‡ï¼Œè°ƒæ•´æƒé‡å’Œå¢åŠ è¶‹åŠ¿ç¡®è®¤æŒ‡æ ‡
        
        å‚æ•°:
        data: å†å²æ•°æ®
        
        è¿”å›:
        tuple: (ç‰¹å¾çŸ©é˜µ, ç‰¹å¾åç§°åˆ—è¡¨)
        """
        self.logger.info("å‡†å¤‡æ”¹è¿›çš„æœºå™¨å­¦ä¹ ç‰¹å¾")

        # æ£€æŸ¥æ•°æ®ä¸­å·²æœ‰çš„æŠ€æœ¯æŒ‡æ ‡
        self.logger.info(f"è¾“å…¥æ•°æ®åˆ—: {list(data.columns)}")

        # è®¡ç®—é¢å¤–çš„è¶‹åŠ¿ç¡®è®¤æŒ‡æ ‡
        data = self._calculate_trend_indicators(data)

        # ğŸ”§ ç¡®ä¿é‡è¦æŠ€æœ¯æŒ‡æ ‡å­˜åœ¨ä¸”æœ‰æ•ˆï¼ˆå¦‚æœDataModuleå·²è®¡ç®—åˆ™ä¿ç•™ï¼Œå¦åˆ™é‡æ–°è®¡ç®—ï¼‰
        data = self._ensure_technical_indicators(data)

        # ğŸ¯ ä¼˜åŒ–ï¼šä½¿ç”¨ç²¾é€‰çš„é«˜æ•ˆç‰¹å¾ï¼Œä¿ç•™é‡è¦çš„æŠ€æœ¯æŒ‡æ ‡åŒ…æ‹¬RSI
        # åŸºäºç‰¹å¾é‡è¦æ€§åˆ†æ + ä¿ç•™å…³é”®æŠ€æœ¯æŒ‡æ ‡
        optimized_feature_columns = [
            # ğŸ”¥ æ ¸å¿ƒè¶‹åŠ¿æŒ‡æ ‡ï¼ˆæœ€é«˜é‡è¦æ€§ï¼š0.21 + 0.11 = 32%ï¼‰
            'trend_strength_60', 'trend_strength_20',

            # ğŸ”¥ æˆäº¤é‡æŒ‡æ ‡ï¼ˆé«˜é‡è¦æ€§ï¼š0.10 + 0.07 = 17%ï¼‰
            'volume_strength', 'volume_trend',

            # âš¡ å‡çº¿ç³»ç»Ÿï¼ˆä¸­é«˜é‡è¦æ€§ï¼š0.06 + 0.06 + 0.05 = 17%ï¼‰
            'ma5', 'ma10', 'ma20',

            # âš¡ ä»·æ ¼åŠ¨é‡å’Œå‡çº¿è·ç¦»ï¼ˆä¸­ç­‰é‡è¦æ€§ï¼š0.05 + 0.05 + 0.05 = 15%ï¼‰
            'price_change_5d', 'dist_ma20', 'macd',

            # âš¡ è¡¥å……ç‰¹å¾ï¼ˆè¾ƒä½ä½†æœ‰æ•ˆï¼š0.05 + 0.05 + 0.04 + 0.04 = 18%ï¼‰
            'dist_ma10', 'dist_ma5', 'ma60', 'price_change_10d',

            # ğŸ“Š é‡è¦æŠ€æœ¯æŒ‡æ ‡ï¼ˆä¿ç•™ç”¨äºç­–ç•¥ä¸€è‡´æ€§ï¼‰
            'rsi',  # RSIå¿…é¡»ä¿ç•™
            'bb_upper', 'bb_lower',  # å¸ƒæ—å¸¦ä¸Šä¸‹è½¨
            'signal', 'hist'  # MACDä¿¡å·å’ŒæŸ±çŠ¶çº¿
        ]

        # âŒ ç§»é™¤çš„çœŸæ­£å™ªéŸ³ç‰¹å¾ï¼š
        # 'price_position_20', 'price_position_60', 'volatility', 'volatility_normalized'

        # ğŸš¨ é‡è¦ï¼šç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å¡«å……åˆç†çš„é»˜è®¤å€¼
        for col in optimized_feature_columns:
            if col not in data.columns:
                self.logger.warning(f"ç¼ºå°‘ç‰¹å¾ {col}ï¼Œå°†å¡«å……é»˜è®¤å€¼")
                # æ ¹æ®ç‰¹å¾ç±»å‹å¡«å……åˆç†çš„é»˜è®¤å€¼
                if 'ma' in col or 'price' in col.lower():
                    # å‡çº¿å’Œä»·æ ¼ç›¸å…³ï¼šä½¿ç”¨æ”¶ç›˜ä»·
                    data[col] = data['close'] if 'close' in data.columns else 0.0
                elif col in ['rsi']:
                    # RSIï¼šå¡«å……ä¸­æ€§å€¼50
                    data[col] = 50.0
                elif 'dist_' in col:
                    # è·ç¦»ç›¸å…³ï¼šå¡«å……0ï¼ˆè¡¨ç¤ºåœ¨å‡çº¿ä¸Šï¼‰
                    data[col] = 0.0
                elif 'volume' in col.lower():
                    # æˆäº¤é‡ç›¸å…³ï¼šå¡«å……1.0ï¼ˆè¡¨ç¤ºæ­£å¸¸ï¼‰
                    data[col] = 1.0
                elif 'volatility' in col.lower():
                    # æ³¢åŠ¨ç‡ç›¸å…³ï¼šå¡«å……é€‚ä¸­å€¼
                    data[col] = 0.02 if 'normalized' not in col else 1.0
                else:
                    # å…¶ä»–ç‰¹å¾ï¼šå¡«å……0
                    data[col] = 0.0

        # ä½¿ç”¨ç²¾é€‰ç‰¹å¾é›†åˆ
        available_columns = optimized_feature_columns.copy()

        if len(available_columns) == 0:
            self.logger.warning("æ²¡æœ‰å¯ç”¨çš„ç‰¹å¾åˆ—")
            return np.array([]), []

        # æå–ç‰¹å¾å¹¶åº”ç”¨æƒé‡
        features_df = data[available_columns].fillna(0).copy()
        features = self._apply_feature_weights(features_df, available_columns)

        self.logger.info("æ”¹è¿›ç‰¹å¾å‡†å¤‡å®Œæˆï¼Œç‰¹å¾æ•°é‡: %d, æ ·æœ¬æ•°é‡: %d",
                         len(available_columns), len(features))

        return features, available_columns

    def _calculate_trend_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—è¶‹åŠ¿ç¡®è®¤æŒ‡æ ‡
        
        å‚æ•°:
        data: åŸå§‹æ•°æ®
        
        è¿”å›:
        pd.DataFrame: æ·»åŠ äº†è¶‹åŠ¿æŒ‡æ ‡çš„æ•°æ®
        """
        # è¶‹åŠ¿å¼ºåº¦æŒ‡æ ‡ï¼ˆåŸºäºçº¿æ€§å›å½’æ–œç‡ï¼‰
        for period in [20, 60]:
            slopes = []
            for i in range(len(data)):
                if i >= period - 1:
                    prices = data['close'].iloc[i - period + 1:i + 1].values
                    x = np.arange(period)
                    slope = np.polyfit(x, prices, 1)[0]
                    # æ ‡å‡†åŒ–æ–œç‡
                    normalized_slope = slope / prices.mean()
                    slopes.append(normalized_slope)
                else:
                    slopes.append(0)
            data[f'trend_strength_{period}'] = slopes

        # ä»·æ ¼åœ¨å‡çº¿ç³»ç»Ÿä¸­çš„ä½ç½®
        # ç¡®ä¿ma20å’Œma60åˆ—å­˜åœ¨ä¸”ä¸ä¸ºNaN
        if 'ma20' in data.columns and data['ma20'].notna().any():
            data['price_position_20'] = (data['close'] - data['ma20']) / data['ma20']
        else:
            data['price_position_20'] = 0

        if 'ma60' in data.columns and data['ma60'].notna().any():
            data['price_position_60'] = (data['close'] - data['ma60']) / data['ma60']
        else:
            data['price_position_60'] = 0

        # æ ‡å‡†åŒ–æ³¢åŠ¨ç‡
        if 'volatility' in data.columns and data['volatility'].notna().any():
            volatility_mean = data['volatility'].rolling(60).mean()
            data['volatility_normalized'] = data['volatility'] / volatility_mean
            # å¤„ç†é™¤é›¶æƒ…å†µ
            data['volatility_normalized'] = data['volatility_normalized'].fillna(1.0)
        else:
            data['volatility_normalized'] = 1.0

        # æˆäº¤é‡è¶‹åŠ¿æŒ‡æ ‡
        volume_ma20 = data['volume'].rolling(20).mean()
        data['volume_trend'] = (data['volume'] - volume_ma20) / volume_ma20
        # å¤„ç†é™¤é›¶æƒ…å†µ
        data['volume_trend'] = data['volume_trend'].fillna(0)

        # æˆäº¤é‡å¼ºåº¦ï¼ˆç›¸å¯¹äºå†å²ï¼‰
        volume_ma60 = data['volume'].rolling(60).mean()
        data['volume_strength'] = data['volume'] / volume_ma60
        # å¤„ç†é™¤é›¶æƒ…å†µ
        data['volume_strength'] = data['volume_strength'].fillna(1.0)

        return data

    def _ensure_technical_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        ç¡®ä¿é‡è¦æŠ€æœ¯æŒ‡æ ‡å­˜åœ¨ä¸”æœ‰æ•ˆ
        
        å‚æ•°:
        data: åŸå§‹æ•°æ®
        
        è¿”å›:
        pd.DataFrame: åŒ…å«æ‰€æœ‰å¿…è¦æŠ€æœ¯æŒ‡æ ‡çš„æ•°æ®
        """
        self.logger.info("ğŸ”§ ç¡®ä¿æŠ€æœ¯æŒ‡æ ‡å®Œæ•´æ€§")

        # æ£€æŸ¥å¹¶è®¡ç®—RSI
        if 'rsi' not in data.columns or data['rsi'].isna().all():
            self.logger.info("é‡æ–°è®¡ç®—RSIæŒ‡æ ‡")
            delta = data['close'].diff()
            gain = delta.where(delta > 0, 0)
            loss = -delta.where(delta < 0, 0)
            avg_gain = gain.rolling(14).mean()
            avg_loss = loss.rolling(14).mean()

            # ä¿®å¤é™¤é›¶é”™è¯¯
            rs = np.where(avg_loss == 0, np.inf, avg_gain / avg_loss)
            rs = np.where(avg_gain == 0, 0, rs)
            data['rsi'] = 100 - (100 / (1 + rs))

        # æ£€æŸ¥å¹¶è®¡ç®—MACD
        if 'macd' not in data.columns or data['macd'].isna().all():
            self.logger.info("é‡æ–°è®¡ç®—MACDæŒ‡æ ‡")
            exp1 = data['close'].ewm(span=12, adjust=False).mean()
            exp2 = data['close'].ewm(span=26, adjust=False).mean()
            data['macd'] = exp1 - exp2
            data['signal'] = data['macd'].ewm(span=9, adjust=False).mean()
            data['hist'] = data['macd'] - data['signal']

        # æ£€æŸ¥å¹¶è®¡ç®—å¸ƒæ—å¸¦
        if 'bb_upper' not in data.columns or data['bb_upper'].isna().all():
            self.logger.info("é‡æ–°è®¡ç®—å¸ƒæ—å¸¦æŒ‡æ ‡")
            # ç¡®ä¿ma20å­˜åœ¨
            if 'ma20' not in data.columns:
                data['ma20'] = data['close'].rolling(20).mean()
            data['bb_upper'] = data['ma20'] + (data['close'].rolling(20).std() * 2)
            data['bb_lower'] = data['ma20'] - (data['close'].rolling(20).std() * 2)

        # æ£€æŸ¥ç§»åŠ¨å¹³å‡çº¿
        if 'ma5' not in data.columns:
            data['ma5'] = data['close'].rolling(5).mean()
        if 'ma10' not in data.columns:
            data['ma10'] = data['close'].rolling(10).mean()
        if 'ma20' not in data.columns:
            data['ma20'] = data['close'].rolling(20).mean()
        if 'ma60' not in data.columns:
            data['ma60'] = data['close'].rolling(60).mean()

        # æ£€æŸ¥ä»·æ ¼ä¸å‡çº¿è·ç¦»
        if 'dist_ma5' not in data.columns:
            data['dist_ma5'] = (data['close'] - data['ma5']) / data['ma5']
        if 'dist_ma10' not in data.columns:
            data['dist_ma10'] = (data['close'] - data['ma10']) / data['ma10']
        if 'dist_ma20' not in data.columns:
            data['dist_ma20'] = (data['close'] - data['ma20']) / data['ma20']

        # æ£€æŸ¥ä»·æ ¼å˜åŒ–ç‡
        if 'price_change_5d' not in data.columns:
            data['price_change_5d'] = data['close'].pct_change(5)
        if 'price_change_10d' not in data.columns:
            data['price_change_10d'] = data['close'].pct_change(10)

        # éªŒè¯å…³é”®æŒ‡æ ‡
        key_indicators = ['rsi', 'macd', 'signal', 'hist', 'bb_upper', 'bb_lower']
        for indicator in key_indicators:
            if indicator in data.columns:
                valid_count = data[indicator].notna().sum()
                total_count = len(data)
                self.logger.info(f"âœ… {indicator}: {valid_count}/{total_count} æœ‰æ•ˆå€¼")
            else:
                self.logger.warning(f"âŒ {indicator} ä¸å­˜åœ¨")

        return data

    def _apply_feature_weights(self, features_df: pd.DataFrame, feature_names: List[str]) -> np.ndarray:
        """
        åº”ç”¨ç‰¹å¾æƒé‡ï¼Œé™ä½çŸ­æœŸæŒ‡æ ‡å½±å“
        
        å‚æ•°:
        features_df: ç‰¹å¾æ•°æ®æ¡†
        feature_names: ç‰¹å¾åç§°åˆ—è¡¨
        
        è¿”å›:
        np.ndarray: åŠ æƒåçš„ç‰¹å¾çŸ©é˜µ
        """
        # å®šä¹‰ç‰¹å¾æƒé‡
        feature_weights = {
            # é•¿æœŸè¶‹åŠ¿æŒ‡æ ‡ï¼ˆé«˜æƒé‡ï¼‰
            'ma20': 1.5, 'ma60': 1.5,
            'trend_strength_20': 2.0, 'trend_strength_60': 2.0,
            'price_position_20': 1.8, 'price_position_60': 1.8,

            # ä¸­æœŸæŒ‡æ ‡ï¼ˆæ­£å¸¸æƒé‡ï¼‰
            'ma10': 1.0, 'dist_ma10': 1.2, 'dist_ma20': 1.2,
            'rsi': 1.0, 'macd': 1.0, 'signal': 1.0,
            'bb_upper': 1.0, 'bb_lower': 1.0,
            'volatility_normalized': 1.0,

            # çŸ­æœŸæŒ‡æ ‡ï¼ˆé™ä½æƒé‡ï¼‰
            'ma5': 0.6, 'dist_ma5': 0.6, 'hist': 0.7,
            'price_change_5d': 0.5, 'price_change_10d': 0.7,

            # æˆäº¤é‡æŒ‡æ ‡ï¼ˆå¹³è¡¡æƒé‡ï¼‰
            'volume_trend': 1.1, 'volume_strength': 1.1,
            'volatility': 0.9
        }

        # åº”ç”¨æƒé‡
        weighted_features = features_df.copy()
        for feature in feature_names:
            weight = feature_weights.get(feature, 1.0)
            weighted_features[feature] = weighted_features[feature] * weight

        return weighted_features.values

    def incremental_train(self, new_data: pd.DataFrame, strategy_module) -> Dict[str, Any]:
        """
        å¢é‡è®­ç»ƒæ¨¡å‹
        
        å‚æ•°:
        new_data: æ–°å¢æ•°æ®
        strategy_module: ç­–ç•¥æ¨¡å—
        
        è¿”å›:
        dict: è®­ç»ƒç»“æœ
        """
        self.logger.info("å¼€å§‹å¢é‡è®­ç»ƒæ¨¡å‹")

        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å®Œå…¨é‡è®­ç»ƒ
            if self.model is None or self.incremental_count >= self.max_incremental_updates:
                self.logger.info("è§¦å‘å®Œå…¨é‡è®­ç»ƒæ¡ä»¶")
                return self.full_train(new_data, strategy_module)

            # å‡†å¤‡æ–°æ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾
            new_features, feature_names = self.prepare_features_improved(new_data)
            new_labels = self._prepare_labels(new_data, strategy_module)

            if len(new_features) == 0 or len(new_labels) == 0:
                self.logger.warning("æ–°æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡å¢é‡è®­ç»ƒ")
                return {'success': False, 'error': 'æ–°æ•°æ®ä¸ºç©º'}

            # æ£€æŸ¥ç‰¹å¾ä¸€è‡´æ€§
            if self.feature_names and feature_names != self.feature_names:
                self.logger.warning("ç‰¹å¾ä¸ä¸€è‡´ï¼Œè¿›è¡Œå®Œå…¨é‡è®­ç»ƒ")
                return self.full_train(new_data, strategy_module)

            # ä½¿ç”¨æœ€è¿‘çš„æ•°æ®è¿›è¡Œå¢é‡æ›´æ–°
            recent_features = new_features[-10:]  # æœ€è¿‘10å¤©çš„æ•°æ®
            recent_labels = new_labels[-10:]

            if len(recent_features) > 0:
                # å¯¹æ–°æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆä½¿ç”¨å·²æœ‰çš„scalerï¼‰
                if self.scaler is not None:
                    recent_features_scaled = self.scaler.transform(recent_features)
                else:
                    self.logger.warning("ç¼ºå°‘scalerï¼Œè¿›è¡Œå®Œå…¨é‡è®­ç»ƒ")
                    return self.full_train(new_data, strategy_module)

                # ä½¿ç”¨warm_startè¿›è¡Œå¢é‡å­¦ä¹ ï¼ˆä»…å½“ä¸ºPipelineä¸”åº•å±‚åˆ†ç±»å™¨æ”¯æŒï¼‰
                classifier = None
                if hasattr(self.model, 'named_steps') and 'classifier' in getattr(self.model, 'named_steps', {}):
                    classifier = self.model.named_steps['classifier']
                else:
                    self.logger.warning("å½“å‰æ¨¡å‹ä¸æ˜¯Pipelineï¼ˆå¯èƒ½ä¸ºæ ¡å‡†åçš„æ¨¡å‹ï¼‰ï¼Œå›é€€åˆ°å®Œå…¨é‡è®­ç»ƒ")
                    return self.full_train(new_data, strategy_module)

                if hasattr(classifier, 'n_estimators'):
                    classifier.n_estimators += 10  # å¢åŠ æ ‘çš„æ•°é‡
                    classifier.warm_start = True

                    # é‡æ–°è®­ç»ƒï¼ˆè¿™é‡Œå®é™…ä¸Šæ˜¯å¢é‡çš„ï¼‰
                    classifier.fit(recent_features_scaled, recent_labels)

                    self.incremental_count += 1
                    self.logger.info(f"å¢é‡è®­ç»ƒå®Œæˆï¼Œæ›´æ–°æ¬¡æ•°: {self.incremental_count}")

                    return {
                        'success': True,
                        'method': 'incremental',
                        'update_count': self.incremental_count,
                        'new_samples': len(recent_features)
                    }
                else:
                    self.logger.warning("æ¨¡å‹ä¸æ”¯æŒå¢é‡å­¦ä¹ ï¼Œè¿›è¡Œå®Œå…¨é‡è®­ç»ƒ")
                    return self.full_train(new_data, strategy_module)

            return {'success': False, 'error': 'æ²¡æœ‰è¶³å¤Ÿçš„æ–°æ•°æ®è¿›è¡Œå¢é‡è®­ç»ƒ'}

        except Exception as e:
            self.logger.error(f"å¢é‡è®­ç»ƒå¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿›è¡Œå®Œå…¨é‡è®­ç»ƒ
            return self.full_train(new_data, strategy_module)

    def full_train(self, data: pd.DataFrame, strategy_module) -> Dict[str, Any]:
        """
        å®Œæ•´è®­ç»ƒAIæ¨¡å‹
        
        å‚æ•°:
        data: å†å²æ•°æ®
        strategy_module: ç­–ç•¥æ¨¡å—
        
        è¿”å›:
        dict: è®­ç»ƒç»“æœ
        """
        train_start_time = time.time()
        self.logger.info("ğŸ¤– å¼€å§‹AIæ¨¡å‹å®Œæ•´è®­ç»ƒ")
        self.logger.info("=" * 80)

        try:
            # æ­¥éª¤1: ç‰¹å¾å·¥ç¨‹
            self.logger.info("âš™ï¸ æ­¥éª¤1: ç‰¹å¾å·¥ç¨‹...")
            feature_start_time = time.time()

            features, feature_names = self.prepare_features_improved(data)

            if len(features) == 0:
                return {
                    'success': False,
                    'error': 'æ— æ³•æå–ç‰¹å¾'
                }

            self.feature_names = feature_names
            feature_time = time.time() - feature_start_time

            self.logger.info(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ (è€—æ—¶: {feature_time:.2f}s)")
            self.logger.info(f"   ç‰¹å¾æ•°é‡: {len(feature_names)}")
            self.logger.info(f"   æ ·æœ¬æ•°é‡: {len(features)}")
            self.logger.info(f"   ç‰¹å¾åˆ—è¡¨: {', '.join(feature_names[:10])}{'...' if len(feature_names) > 10 else ''}")
            self.logger.info("-" * 60)

            # æ­¥éª¤2: æ ‡ç­¾å‡†å¤‡
            self.logger.info("ğŸ·ï¸ æ­¥éª¤2: æ ‡ç­¾å‡†å¤‡...")
            label_start_time = time.time()

            labels = self._prepare_labels(data, strategy_module)

            if len(labels) != len(features):
                return {
                    'success': False,
                    'error': f'ç‰¹å¾æ•°é‡({len(features)})ä¸æ ‡ç­¾æ•°é‡({len(labels)})ä¸åŒ¹é…'
                }

            label_time = time.time() - label_start_time
            positive_ratio = np.mean(labels)

            self.logger.info(f"âœ… æ ‡ç­¾å‡†å¤‡å®Œæˆ (è€—æ—¶: {label_time:.2f}s)")
            self.logger.info(f"   æ­£æ ·æœ¬æ¯”ä¾‹: {positive_ratio:.2%}")
            self.logger.info(f"   æ­£æ ·æœ¬æ•°é‡: {np.sum(labels)} / {len(labels)}")
            self.logger.info("-" * 60)

            # æ­¥éª¤3: æ ·æœ¬æƒé‡è®¡ç®—
            self.logger.info("âš–ï¸ æ­¥éª¤3: æ ·æœ¬æƒé‡è®¡ç®—...")
            weight_start_time = time.time()

            # ä¸¥æ ¼è¦æ±‚æ•°æ®åŒ…å«æ­£ç¡®çš„æ—¥æœŸä¿¡æ¯
            if 'date' in data.columns:
                date_series = data['date']
                if not pd.api.types.is_datetime64_any_dtype(date_series):
                    raise ValueError(f"data['date']åˆ—ä¸æ˜¯datetimeç±»å‹ï¼Œå®é™…ç±»å‹: {date_series.dtype}")
            elif pd.api.types.is_datetime64_any_dtype(data.index):
                date_series = data.index.to_series()
            else:
                raise ValueError(
                    "æ•°æ®ç¼ºå°‘æœ‰æ•ˆçš„æ—¥æœŸä¿¡æ¯ã€‚è¦æ±‚ï¼š\n"
                    "1. åŒ…å«datetimeç±»å‹çš„'date'åˆ—ï¼Œæˆ–\n"
                    "2. ä½¿ç”¨datetimeç±»å‹çš„ç´¢å¼•\n"
                    f"å®é™…æƒ…å†µï¼š\n"
                    f"  - 'date'åˆ—: {'å­˜åœ¨' if 'date' in data.columns else 'ä¸å­˜åœ¨'}\n"
                    f"  - ç´¢å¼•ç±»å‹: {type(data.index).__name__}\n"
                    f"  - ç´¢å¼•dtype: {data.index.dtype}"
                )

            sample_weights = self._calculate_sample_weights(date_series)
            weight_time = time.time() - weight_start_time

            self.logger.info(f"âœ… æ ·æœ¬æƒé‡è®¡ç®—å®Œæˆ (è€—æ—¶: {weight_time:.2f}s)")
            self.logger.info(f"   æƒé‡èŒƒå›´: {np.min(sample_weights):.4f} - {np.max(sample_weights):.4f}")
            self.logger.info(f"   å¹³å‡æƒé‡: {np.mean(sample_weights):.4f}")
            self.logger.info("-" * 60)

            # æ­¥éª¤4: æ¨¡å‹è®­ç»ƒ
            self.logger.info("ğŸ‹ï¸ æ­¥éª¤4: æ¨¡å‹è®­ç»ƒ...")
            model_start_time = time.time()

            # åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹pipelineï¼ˆé™ä½å¤æ‚åº¦é˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
            model = Pipeline([
                ('scaler', StandardScaler()),
                ('classifier', RandomForestClassifier(
                    n_estimators=100,  # ä»150é™åˆ°100
                    max_depth=8,  # ä»12é™åˆ°8
                    min_samples_split=15,  # ä»8æé«˜åˆ°15
                    min_samples_leaf=8,  # ä»3æé«˜åˆ°8
                    class_weight='balanced',
                    n_jobs=-1,
                    random_state=42,
                    verbose=1  # å¯ç”¨è®­ç»ƒè¿›åº¦è¾“å‡º
                ))
            ])

            self.logger.info("ğŸŒ² RandomForestæ¨¡å‹é…ç½®ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰:")
            self.logger.info("   n_estimators: 100 (å†³ç­–æ ‘æ•°é‡) - é™ä½å¤æ‚åº¦")
            self.logger.info("   max_depth: 8 (æœ€å¤§æ·±åº¦) - å‡å°‘è¿‡æ‹Ÿåˆ")
            self.logger.info("   min_samples_split: 15 (æœ€å°åˆ†å‰²æ ·æœ¬æ•°) - å¢åŠ ç¨³å®šæ€§")
            self.logger.info("   min_samples_leaf: 8 (æœ€å°å¶å­èŠ‚ç‚¹æ ·æœ¬æ•°) - å¢åŠ ç¨³å®šæ€§")
            self.logger.info("   class_weight: balanced (è‡ªåŠ¨å¹³è¡¡ç±»åˆ«æƒé‡)")
            self.logger.info("   n_jobs: -1 (å¹¶è¡Œè®­ç»ƒ)")

            self.logger.info("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
            # è®­ç»ƒæ¨¡å‹
            model.fit(features, labels, classifier__sample_weight=sample_weights)

            model_time = time.time() - model_start_time
            self.model = model
            # ä¿å­˜scalerä¾›å¢é‡è®­ç»ƒå¤ç”¨
            try:
                self.scaler = model.named_steps['scaler'] if hasattr(model, 'named_steps') and 'scaler' in model.named_steps else None
            except Exception:
                self.scaler = None

            self.logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ (è€—æ—¶: {model_time:.2f}s)")
            self.logger.info("-" * 60)

            # === æ¦‚ç‡æ ¡å‡†ï¼ˆCalibrationï¼‰å¼€å§‹ ===
            try:
                calib_cfg = {}
                if isinstance(self.config, dict):
                    calib_cfg = (self.config.get('calibration')
                                 or self.config.get('ai', {}).get('calibration')
                                 or {})
                calib_enabled = bool(calib_cfg.get('enabled', True))
                calib_method = calib_cfg.get('method', 'isotonic')
                calib_cv = calib_cfg.get('cv', 'prefit')
                holdout_size = int(calib_cfg.get('holdout_size', max(30, int(len(features) * 0.2))))
                holdout_size = min(holdout_size, 120)

                if calib_enabled:
                    self.logger.info("ğŸ“ å¯ç”¨æ¦‚ç‡æ ¡å‡†: method=%s, cv=%s", calib_method, str(calib_cv))
                    if calib_cv == 'prefit':
                        if len(features) - holdout_size < 50:
                            self.logger.warning("æ ·æœ¬è¾ƒå°‘ï¼Œprefit ç•™å‡ºé›†ä¸è¶³ï¼Œè‡ªåŠ¨é€€åŒ–ä¸º cv=3")
                            calib_cv = 3
                        else:
                            split_idx = len(features) - holdout_size
                            X_train_base = features[:split_idx]
                            y_train_base = labels[:split_idx]
                            X_calib = features[split_idx:]
                            y_calib = labels[split_idx:]
                            w_train_base = sample_weights[:split_idx] if sample_weights is not None else None

                            base_model = Pipeline([
                                ('scaler', StandardScaler()),
                                ('classifier', RandomForestClassifier(
                                    n_estimators=100,
                                    max_depth=8,
                                    min_samples_split=15,
                                    min_samples_leaf=8,
                                    class_weight='balanced',
                                    n_jobs=-1,
                                    random_state=42,
                                    verbose=1
                                ))
                            ])
                            base_model.fit(X_train_base, y_train_base,
                                           classifier__sample_weight=w_train_base)

                            calibrated = CalibratedClassifierCV(
                                estimator=base_model, method=calib_method, cv='prefit'
                            )
                            calibrated.fit(X_calib, y_calib)

                            self.model = calibrated
                            self.logger.info("âœ… æ¦‚ç‡æ ¡å‡†å®Œæˆï¼ˆprefit + ç•™å‡ºé›†=%dï¼‰", len(X_calib))
                    if isinstance(calib_cv, int) and calib_cv >= 2:
                        calibrated = CalibratedClassifierCV(
                            estimator=self.model, method=calib_method, cv=calib_cv
                        )
                        try:
                            calibrated.fit(features, labels, sample_weight=sample_weights)
                        except TypeError:
                            self.logger.warning("CalibratedClassifierCV.fit ä¸æ”¯æŒ sample_weightï¼Œå·²å›é€€ä¸ºæ— æƒé‡æ‹Ÿåˆ")
                            calibrated.fit(features, labels)
                        self.model = calibrated
                        self.logger.info("âœ… æ¦‚ç‡æ ¡å‡†å®Œæˆï¼ˆ%d æŠ˜CVï¼‰", calib_cv)
            except Exception as e_calib:
                self.logger.warning(f"æ¦‚ç‡æ ¡å‡†é˜¶æ®µå‡ºç°é—®é¢˜ï¼Œå·²è·³è¿‡æ ¡å‡†: {e_calib}")
            # === æ¦‚ç‡æ ¡å‡†ç»“æŸ ===

            # æ­¥éª¤5: æ¨¡å‹ä¿å­˜
            self.logger.info("ğŸ’¾ æ­¥éª¤5: æ¨¡å‹ä¿å­˜...")
            save_start_time = time.time()

            save_success = self._save_model()
            save_time = time.time() - save_start_time

            if save_success:
                self.logger.info(f"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ (è€—æ—¶: {save_time:.2f}s)")
            else:
                self.logger.warning(f"âš ï¸ æ¨¡å‹ä¿å­˜å¤±è´¥ (è€—æ—¶: {save_time:.2f}s)")

            # è®­ç»ƒæ€»ç»“
            total_train_time = time.time() - train_start_time
            self.logger.info("=" * 80)
            self.logger.info("ğŸ‰ AIæ¨¡å‹è®­ç»ƒå®Œæˆ!")
            self.logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_train_time:.2f}s ({total_train_time / 60:.1f}åˆ†é’Ÿ)")
            self.logger.info(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡:")
            self.logger.info(f"   ç‰¹å¾å·¥ç¨‹: {feature_time:.2f}s")
            self.logger.info(f"   æ ‡ç­¾å‡†å¤‡: {label_time:.2f}s")
            self.logger.info(f"   æƒé‡è®¡ç®—: {weight_time:.2f}s")
            self.logger.info(f"   æ¨¡å‹è®­ç»ƒ: {model_time:.2f}s")
            self.logger.info(f"   æ¨¡å‹ä¿å­˜: {save_time:.2f}s")
            self.logger.info(f"ğŸ¯ è®­ç»ƒç»“æœ:")
            self.logger.info(f"   æ ·æœ¬æ•°é‡: {len(features)}")
            self.logger.info(f"   ç‰¹å¾æ•°é‡: {len(feature_names)}")
            self.logger.info(f"   æ­£æ ·æœ¬æ¯”ä¾‹: {positive_ratio:.2%}")
            self.logger.info(f"   æ¨¡å‹ä¿å­˜: {'æˆåŠŸ' if save_success else 'å¤±è´¥'}")
            self.logger.info("=" * 80)

            return {
                'success': True,
                'method': 'improved_full_training',
                'train_samples': len(features),
                'feature_count': len(feature_names),
                'positive_ratio': positive_ratio,
                'training_time': total_train_time,
                'feature_time': feature_time,
                'model_time': model_time,
                'save_time': save_time,
                'save_success': save_success,
                'feature_names': feature_names,
                'training_time_breakdown': {
                    'feature_engineering': feature_time,
                    'label_preparation': label_time,
                    'weight_calculation': weight_time,
                    'model_training': model_time,
                    'model_saving': save_time
                }
            }

        except Exception as e:
            self.logger.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def predict_low_point(self, data: pd.DataFrame, prediction_date: str = None) -> Dict[str, Any]:
        """
        é¢„æµ‹ç›¸å¯¹ä½ç‚¹
        
        å‚æ•°:
        data: å¸‚åœºæ•°æ®
        prediction_date: é¢„æµ‹æ—¥æœŸ
        
        è¿”å›:
        dict: é¢„æµ‹ç»“æœ
        """
        self.logger.info("é¢„æµ‹ç›¸å¯¹ä½ç‚¹")

        try:
            # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœæœªåŠ è½½ï¼‰
            if self.model is None:
                if not self._load_model():
                    return {
                        'is_low_point': False,
                        'confidence': 0.0,
                        'final_confidence': 0.0,
                        'error': 'æ¨¡å‹æœªè®­ç»ƒä¸”æ— æ³•åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹'
                    }

            if len(data) == 0:
                return {
                    'is_low_point': False,
                    'confidence': 0.0,
                    'final_confidence': 0.0,
                    'error': 'æ•°æ®ä¸ºç©º'
                }

            # å‡†å¤‡ç‰¹å¾
            features, feature_names = self.prepare_features_improved(data)

            if len(features) == 0:
                return {
                    'is_low_point': False,
                    'confidence': 0.0,
                    'final_confidence': 0.0,
                    'error': 'æ— æ³•æå–ç‰¹å¾'
                }

            # ä½¿ç”¨æœ€æ–°æ•°æ®è¿›è¡Œé¢„æµ‹
            latest_features = features[-1:].reshape(1, -1)

            # è·å–é¢„æµ‹æ¦‚ç‡ï¼ˆä¸ä½¿ç”¨predictæ–¹æ³•ï¼Œé¿å…å†…ç½®é˜ˆå€¼å½±å“ï¼‰
            prediction_proba = self.model.predict_proba(latest_features)[0]

            # è·å–åŸå§‹ç½®ä¿¡åº¦ï¼ˆä¸å†è¿›è¡Œå¤„ç†ï¼‰
            raw_confidence = prediction_proba[1] if len(prediction_proba) > 1 else 0.0

            # ç›´æ¥ä½¿ç”¨åŸå§‹ç½®ä¿¡åº¦ï¼Œä¸è¿›è¡Œå¤„ç†
            final_confidence = raw_confidence

            # ä½¿ç”¨å›ºå®šé˜ˆå€¼ï¼ˆä»é…ç½®è¯»å–ï¼Œä¸è¿›è¡ŒåŠ¨æ€è°ƒæ•´ï¼‰
            final_threshold = resolve_confidence_param(self.config, 'final_threshold', 0.5)

            # åŸºäºåŸå§‹ç½®ä¿¡åº¦å’Œé…ç½®é˜ˆå€¼è¿›è¡Œé¢„æµ‹
            is_low_point = final_confidence >= final_threshold

            # å®‰å…¨è·å–æ¨¡å‹ç±»å‹ï¼ˆå…¼å®¹ Pipeline æˆ– CalibratedClassifierCVï¼‰
            model_type = type(self.model).__name__
            try:
                if hasattr(self.model, 'named_steps') and 'classifier' in self.model.named_steps:
                    model_type = type(self.model.named_steps['classifier']).__name__
                else:
                    base_est = getattr(self.model, 'base_estimator', getattr(self.model, 'estimator', None))
                    if base_est is not None:
                        if hasattr(base_est, 'named_steps') and 'classifier' in getattr(base_est, 'named_steps', {}):
                            model_type = type(base_est.named_steps['classifier']).__name__
                        else:
                            model_type = type(base_est).__name__
            except Exception:
                pass

            result = {
                'is_low_point': bool(is_low_point),
                'confidence': float(raw_confidence),
                'final_confidence': float(final_confidence),  # ç°åœ¨ç­‰äºåŸå§‹ç½®ä¿¡åº¦
                'prediction_proba': prediction_proba.tolist(),
                'feature_count': len(feature_names),
                'model_type': model_type,
                'threshold_used': final_threshold
            }

            # è¾“å‡ºé¢„æµ‹ç»“æœ
            self.logger.info("----------------------------------------------------")
            self.logger.info("AIé¢„æµ‹ç»“æœ: \033[1m%s\033[0m",
                             "ç›¸å¯¹ä½ç‚¹" if is_low_point else "éç›¸å¯¹ä½ç‚¹")
            self.logger.info("åŸå§‹ç½®ä¿¡åº¦: \033[1m%.4f\033[0m, é˜ˆå€¼: \033[1m%.2f\033[0m",
                             raw_confidence, final_threshold)
            self.logger.info("----------------------------------------------------")

            return result

        except Exception as e:
            self.logger.error(f"é¢„æµ‹ç›¸å¯¹ä½ç‚¹å¤±è´¥: {e}")
            return {
                'is_low_point': False,
                'confidence': 0.0,
                'final_confidence': 0.0,
                'error': str(e)
            }

    def _prepare_labels(self, data: pd.DataFrame, strategy_module) -> np.ndarray:
        """å‡†å¤‡æ ‡ç­¾"""
        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ•°æ®åŒ…å«æŠ€æœ¯æŒ‡æ ‡
        if 'rsi' not in data.columns or 'macd' not in data.columns:
            self.logger.warning("æ•°æ®ç¼ºå°‘æŠ€æœ¯æŒ‡æ ‡ï¼Œè·³è¿‡é¢„å¤„ç†...")
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å‡è®¾å¤–éƒ¨å·²ç»å¤„ç†äº†æ•°æ®é¢„å¤„ç†
            # å¦‚æœç¡®å®éœ€è¦åœ¨è¿™é‡Œå¤„ç†ï¼Œå¯ä»¥æ·»åŠ æ•°æ®æ¨¡å—è°ƒç”¨

        backtest_results = strategy_module.backtest(data)
        return backtest_results['is_low_point'].astype(int).values

    def _calculate_sample_weights(self, dates: pd.Series) -> np.ndarray:
        """
        è®¡ç®—åŸºäºæ—¶é—´è¡°å‡çš„æ ·æœ¬æƒé‡
        
        å‚æ•°:
        dates: æ—¥æœŸåºåˆ—ï¼Œå¿…é¡»æ˜¯datetimeç±»å‹
        
        è¿”å›:
        np.ndarray: æ ·æœ¬æƒé‡æ•°ç»„ï¼Œè¶Šæ–°çš„æ•°æ®æƒé‡è¶Šé«˜
        
        å¼‚å¸¸:
        ValueError: å½“datesä¸æ˜¯datetimeç±»å‹æˆ–ä¸ºç©ºæ—¶
        """
        if len(dates) == 0:
            raise ValueError("æ—¥æœŸåºåˆ—ä¸ºç©ºï¼Œæ— æ³•è®¡ç®—æ ·æœ¬æƒé‡")

        # ä¸¥æ ¼æ£€æŸ¥æ•°æ®ç±»å‹ï¼šåªæ¥å—datetimeç±»å‹
        if not pd.api.types.is_datetime64_any_dtype(dates):
            raise ValueError(f"æ ·æœ¬æƒé‡è®¡ç®—è¦æ±‚datetimeç±»å‹çš„æ—¥æœŸæ•°æ®ï¼Œå®é™…ç±»å‹: {dates.dtype}")

        # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºå€¼
        if dates.isnull().any():
            raise ValueError("æ—¥æœŸåºåˆ—åŒ…å«ç©ºå€¼ï¼Œæ— æ³•è®¡ç®—å‡†ç¡®çš„æ ·æœ¬æƒé‡")

        # è®¡ç®—æ—¶é—´è¡°å‡æƒé‡
        latest_date = dates.max()
        decay_rate = self.config.get("ai", {}).get("data_decay_rate", 0.4)

        weights = np.zeros(len(dates))
        for i, date in enumerate(dates):
            time_diff_days = (latest_date - date).days
            if time_diff_days < 0:
                raise ValueError(f"å‘ç°æœªæ¥æ—¥æœŸ: {date} > {latest_date}")

            time_diff_years = time_diff_days / 365.25
            weight = np.exp(-decay_rate * time_diff_years)
            weights[i] = weight

        # éªŒè¯æƒé‡è®¡ç®—ç»“æœ
        if np.any(weights <= 0) or np.any(np.isnan(weights)) or np.any(np.isinf(weights)):
            raise ValueError("æ ·æœ¬æƒé‡è®¡ç®—ç»“æœå¼‚å¸¸ï¼ŒåŒ…å«éæ­£å€¼æˆ–NaN/Inf")

        return weights

    def _save_model(self) -> bool:
        """ä¿å­˜æ¨¡å‹"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

            # ä¿å­˜æ¨¡å‹
            model_path = os.path.join(self.models_dir, f'improved_model_{timestamp}.pkl')
            with open(model_path, 'wb') as f:
                pickle.dump({
                    'model': self.model,
                    'feature_names': self.feature_names,
                    'incremental_count': self.incremental_count,
                    'scaler': self.scaler
                }, f)

            # ä¿å­˜æœ€æ–°æ¨¡å‹è·¯å¾„
            latest_path = os.path.join(self.models_dir, 'latest_improved_model.txt')
            with open(latest_path, 'w') as f:
                f.write(model_path)

            self.logger.info(f"æ”¹è¿›æ¨¡å‹ä¿å­˜æˆåŠŸ: {model_path}")
            return True

        except Exception as e:
            self.logger.error(f"ä¿å­˜æ”¹è¿›æ¨¡å‹å¤±è´¥: {e}")
            return False

    def _load_model(self) -> bool:
        """åŠ è½½æ¨¡å‹ï¼ˆå®‰å…¨ç‰ˆæœ¬ï¼‰"""
        try:
            latest_path = os.path.join(self.models_dir, 'latest_improved_model.txt')

            if not os.path.exists(latest_path):
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°å·²ä¿å­˜çš„æ”¹è¿›æ¨¡å‹")
                return False

            with open(latest_path, 'r') as f:
                model_path = f.read().strip()

            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
            if not os.path.isabs(model_path):
                # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆä»å½“å‰æ–‡ä»¶ä½ç½®å‘ä¸Šä¸‰çº§ï¼šsrc/ai/ai_optimizer_improved.py -> é¡¹ç›®æ ¹ç›®å½•ï¼‰
                project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                model_path = os.path.join(project_root, model_path)
            
            # å®‰å…¨æ£€æŸ¥ï¼šéªŒè¯æ¨¡å‹æ–‡ä»¶è·¯å¾„
            if not os.path.abspath(model_path).startswith(os.path.abspath(self.models_dir)):
                self.logger.error(f"æ¨¡å‹æ–‡ä»¶è·¯å¾„ä¸å®‰å…¨: {model_path}")
                return False

            # å®‰å…¨æ£€æŸ¥ï¼šéªŒè¯æ–‡ä»¶å¤§å°ï¼ˆé˜²æ­¢è¿‡å¤§çš„æ¶æ„æ–‡ä»¶ï¼‰
            max_file_size = 500 * 1024 * 1024  # 500MBé™åˆ¶
            if os.path.getsize(model_path) > max_file_size:
                self.logger.error(f"æ¨¡å‹æ–‡ä»¶è¿‡å¤§ï¼Œå¯èƒ½å­˜åœ¨å®‰å…¨é£é™©: {model_path}")
                return False

            with open(model_path, 'rb') as f:
                # ä½¿ç”¨å—é™çš„pickleåŠ è½½å™¨ï¼ˆé™åˆ¶å¯å¯¼å…¥çš„æ¨¡å—ï¼‰
                import pickle
                import builtins

                # åˆ›å»ºå®‰å…¨çš„unpickler
                logger = self.logger  # ä¿å­˜loggerå¼•ç”¨

                class SafeUnpickler(pickle.Unpickler):
                    def find_class(self, module, name):
                        # åªå…è®¸åŠ è½½ç‰¹å®šçš„å®‰å…¨æ¨¡å—
                        safe_modules = {
                            'sklearn.ensemble._forest',
                            'sklearn.ensemble',
                            'sklearn.pipeline',
                            'sklearn.preprocessing._data',
                            'sklearn.preprocessing',
                            'numpy',
                            'pandas.core.frame',
                            'pandas',
                            'builtins'
                        }

                        if module in safe_modules or module.startswith('numpy') or module.startswith('sklearn'):
                            return getattr(__import__(module, fromlist=[name]), name)
                        else:
                            logger.warning(f"æ‹’ç»åŠ è½½ä¸å®‰å…¨çš„æ¨¡å—: {module}.{name}")
                            raise pickle.PicklingError(f"Unsafe module: {module}")

                # ä½¿ç”¨å®‰å…¨çš„unpickleråŠ è½½æ•°æ®
                safe_unpickler = SafeUnpickler(f)
                data = safe_unpickler.load()

                # éªŒè¯åŠ è½½çš„æ•°æ®ç»“æ„
                required_keys = ['model', 'feature_names']
                if not isinstance(data, dict) or not all(key in data for key in required_keys):
                    self.logger.error("æ¨¡å‹æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
                    return False

                self.model = data['model']
                self.feature_names = data['feature_names']
                self.incremental_count = data.get('incremental_count', 0)
                self.scaler = data.get('scaler')

            self.logger.info(f"æ”¹è¿›æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
            return True

        except Exception as e:
            self.logger.error(f"åŠ è½½æ”¹è¿›æ¨¡å‹å¤±è´¥: {e}")
            return False

    def get_feature_importance(self) -> Dict[str, float]:
        """
        è·å–ç‰¹å¾é‡è¦æ€§
        
        è¿”å›:
        dict: ç‰¹å¾é‡è¦æ€§å­—å…¸ï¼ŒæŒ‰é‡è¦æ€§é™åºæ’åˆ—
        """
        try:
            if self.model is None:
                self.logger.warning("æ¨¡å‹æœªè®­ç»ƒï¼Œå°è¯•åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹")
                if not self._load_model():
                    self.logger.error("æ— æ³•è·å–ç‰¹å¾é‡è¦æ€§ï¼šæ¨¡å‹æœªè®­ç»ƒä¸”æ— æ³•åŠ è½½")
                    return {}

            if self.feature_names is None:
                self.logger.error("ç‰¹å¾åç§°æœªè®¾ç½®ï¼Œæ— æ³•è·å–ç‰¹å¾é‡è¦æ€§")
                return {}

            # å…¼å®¹ CalibratedClassifierCVï¼Œä¼˜å…ˆæå–åº•å±‚åŸºå­¦ä¹ å™¨
            model_obj = self.model
            base_est = getattr(model_obj, 'base_estimator', getattr(model_obj, 'estimator', None))
            if base_est is not None:
                model_obj = base_est

            # ä»Pipelineä¸­è·å–åˆ†ç±»å™¨
            if hasattr(model_obj, 'named_steps') and 'classifier' in getattr(model_obj, 'named_steps', {}):
                classifier = model_obj.named_steps['classifier']
            else:
                # å¦‚æœæ¨¡å‹ä¸æ˜¯Pipelineï¼Œç›´æ¥ä½¿ç”¨
                classifier = model_obj

            # æ£€æŸ¥åˆ†ç±»å™¨æ˜¯å¦æœ‰feature_importances_å±æ€§
            if hasattr(classifier, 'feature_importances_'):
                importances = classifier.feature_importances_

                # åˆ›å»ºç‰¹å¾é‡è¦æ€§å­—å…¸
                feature_importance = dict(zip(self.feature_names, importances))

                # æŒ‰é‡è¦æ€§é™åºæ’åˆ—
                sorted_importance = dict(sorted(
                    feature_importance.items(),
                    key=lambda x: x[1],
                    reverse=True
                ))

                self.logger.info(f"æˆåŠŸè·å– {len(sorted_importance)} ä¸ªç‰¹å¾çš„é‡è¦æ€§")
                return sorted_importance
            else:
                self.logger.warning(f"åˆ†ç±»å™¨ {type(classifier).__name__} ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§")
                return {}

        except Exception as e:
            self.logger.error(f"è·å–ç‰¹å¾é‡è¦æ€§å¤±è´¥: {e}")
            return {}

    def run_complete_optimization(self, data: pd.DataFrame, strategy_module) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„AIä¼˜åŒ–æµç¨‹ï¼ˆåŒ…å«å‚æ•°ä¼˜åŒ– + æ¨¡å‹è®­ç»ƒï¼‰
        
        å‚æ•°:
        data: å†å²æ•°æ®
        strategy_module: ç­–ç•¥æ¨¡å—
        
        è¿”å›:
        dict: ä¼˜åŒ–ç»“æœ
        """
        from datetime import datetime
        complete_start_time = time.time()

        # åŒæ—¶ä½¿ç”¨printå’Œloggerç¡®ä¿è¾“å‡ºå¯è§
        current_time = datetime.now().strftime("%H:%M:%S")
        print(f"ğŸš€ å¼€å§‹å®Œæ•´çš„AIä¼˜åŒ–æµç¨‹ [{current_time}]")
        print("=" * 80)
        self.logger.info("ğŸš€ å¼€å§‹å®Œæ•´çš„AIä¼˜åŒ–æµç¨‹")
        self.logger.info("=" * 80)

        try:
            optimization_result = {
                'success': False,
                'strategy_optimization': {},
                'model_training': {},
                'final_evaluation': {},
                'errors': []
            }

            # æ­¥éª¤é¢„è§ˆ
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"ğŸ“‹ ä¼˜åŒ–æµç¨‹æ¦‚è§ˆ: [{current_time}]")
            print("   ğŸ”§ æ­¥éª¤A: ç­–ç•¥å‚æ•°ä¼˜åŒ– (è´å¶æ–¯ä¼˜åŒ–)")
            print("   ğŸ¤– æ­¥éª¤B: æ¨¡å‹è®­ç»ƒ")
            print("   ğŸ“Š æ­¥éª¤C: æœ€ç»ˆæ€§èƒ½è¯„ä¼°")
            print("   ğŸ’¾ æ­¥éª¤D: ç»“æœä¿å­˜")
            print("-" * 80)

            # æ­¥éª¤A: ç­–ç•¥å‚æ•°ä¼˜åŒ–
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"ğŸ”§ æ­¥éª¤A: ç­–ç•¥å‚æ•°ä¼˜åŒ– [{current_time}]")
            print("   ğŸ¯ ç›®æ ‡: å¯»æ‰¾æœ€ä¼˜ç­–ç•¥å‚æ•°ç»„åˆ")
            print("   ğŸ“Š æ–¹æ³•: è´å¶æ–¯ä¼˜åŒ–é«˜ç²¾åº¦æœç´¢")
            
            # ğŸ”§ ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–å›ºå®šå‚æ•°å€¼
            strategy_config = self.config.get('strategy', {})

            # ğŸ”§ è·å–å½“å‰ç­–ç•¥åŸºå‡†å¾—åˆ†
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"   ğŸ“Š è¯„ä¼°å½“å‰ç­–ç•¥åŸºå‡†å¾—åˆ†... [{current_time}]")

            current_backtest = strategy_module.backtest(data)
            current_evaluation = strategy_module.evaluate_strategy(current_backtest)
            baseline_score = current_evaluation.get('score', 0)

            print(f"   ğŸ“ˆ å½“å‰ç­–ç•¥åŸºå‡†å¾—åˆ†: {baseline_score:.6f}")
            self.logger.info(f"å½“å‰ç­–ç•¥åŸºå‡†å¾—åˆ†: {baseline_score:.6f}")

            step_a_start = time.time()

            strategy_result = self.optimize_strategy_parameters_improved(strategy_module, data)
            step_a_time = time.time() - step_a_start
            optimization_result['strategy_optimization'] = strategy_result

            if strategy_result['success']:
                optimized_score = strategy_result.get('best_score', 0)

                # ğŸ¯ å…³é”®ä¿®å¤ï¼šåªæœ‰æ–°å¾—åˆ†æ›´é«˜æ‰æ›´æ–°ç­–ç•¥å‚æ•°
                if optimized_score > baseline_score:
                    # æ›´æ–°ç­–ç•¥æ¨¡å—å‚æ•°
                    strategy_module.update_params(strategy_result['best_params'])
                    score_improvement = optimized_score - baseline_score

                    current_time = datetime.now().strftime("%H:%M:%S")
                    print(f"âœ… æ­¥éª¤Aå®Œæˆ - ç­–ç•¥å¾—åˆ†æå‡! (è€—æ—¶: {step_a_time:.2f}s) [{current_time}]")
                    print(f"   ğŸ¯ ä¼˜åŒ–æ–¹æ³•: {strategy_result.get('optimization_method', 'unknown')}")
                    print(f"   ğŸ“ˆ ä¼˜åŒ–å‰å¾—åˆ†: {baseline_score:.6f}")
                    print(f"   ğŸ“ˆ ä¼˜åŒ–åå¾—åˆ†: {optimized_score:.6f}")
                    print(
                        f"   ğŸš€ å¾—åˆ†æå‡: +{score_improvement:.6f} ({(score_improvement / baseline_score * 100):.2f}%)")
                    print(f"   ğŸ“Š æµ‹è¯•é›†æˆåŠŸç‡: {strategy_result.get('test_success_rate', 0):.2%}")

                    self.logger.info(f"âœ… æ­¥éª¤Aå®Œæˆ - ç­–ç•¥å¾—åˆ†æå‡! (è€—æ—¶: {step_a_time:.2f}s)")
                    self.logger.info(f"   ğŸ“ˆ ä¼˜åŒ–å‰å¾—åˆ†: {baseline_score:.6f}")
                    self.logger.info(f"   ğŸ“ˆ ä¼˜åŒ–åå¾—åˆ†: {optimized_score:.6f}")
                    self.logger.info(f"   ğŸš€ å¾—åˆ†æå‡: +{score_improvement:.6f}")
                else:
                    # ä¼˜åŒ–å¾—åˆ†æœªè¶…è¿‡åŸºå‡†ï¼Œä¿æŒåŸå‚æ•°ä¸å˜
                    current_time = datetime.now().strftime("%H:%M:%S")
                    print(f"âš ï¸ æ­¥éª¤Aå®Œæˆä½†æ— æ”¹è¿› (è€—æ—¶: {step_a_time:.2f}s) - ä¿æŒåŸå‚æ•° [{current_time}]")
                    print(f"   ğŸ¯ ä¼˜åŒ–æ–¹æ³•: {strategy_result.get('optimization_method', 'unknown')}")
                    print(f"   ğŸ“ˆ å½“å‰å¾—åˆ†: {baseline_score:.6f}")
                    print(f"   ğŸ“ˆ ä¼˜åŒ–å¾—åˆ†: {optimized_score:.6f}")
                    print(f"   ğŸ“‰ æœªè¾¾åˆ°æ”¹è¿›é˜ˆå€¼ï¼Œä¿æŒåŸç­–ç•¥å‚æ•°")

                    self.logger.info(f"âš ï¸ æ­¥éª¤Aå®Œæˆä½†æ— æ”¹è¿› (è€—æ—¶: {step_a_time:.2f}s)")
                    self.logger.info(f"   ğŸ“ˆ å½“å‰å¾—åˆ†: {baseline_score:.6f} > ä¼˜åŒ–å¾—åˆ†: {optimized_score:.6f}")
                    optimization_result['errors'].append("ä¼˜åŒ–åå¾—åˆ†æœªè¶…è¿‡åŸºå‡†ï¼Œä¿æŒåŸå‚æ•°")

            else:
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"âš ï¸ æ­¥éª¤Aå¤±è´¥ (è€—æ—¶: {step_a_time:.2f}s) - ä½¿ç”¨é»˜è®¤å‚æ•°ç»§ç»­ [{current_time}]")
                self.logger.warning(f"âš ï¸ æ­¥éª¤Aå¤±è´¥ (è€—æ—¶: {step_a_time:.2f}s) - ä½¿ç”¨é»˜è®¤å‚æ•°ç»§ç»­")
                optimization_result['errors'].append("ç­–ç•¥å‚æ•°ä¼˜åŒ–å¤±è´¥")

            print("-" * 80)
            self.logger.info("-" * 80)

            # æ­¥éª¤B: æ¨¡å‹è®­ç»ƒ
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"ğŸ¤– æ­¥éª¤B: æ¨¡å‹è®­ç»ƒ [{current_time}]")
            print("   ğŸ¯ ç›®æ ‡: è®­ç»ƒRandomForeståˆ†ç±»æ¨¡å‹")
            print("   âš™ï¸ é…ç½®: 150æ£µæ ‘, æ·±åº¦12, å¹³è¡¡æƒé‡")
            print("   ğŸ“Š æ•°æ®: ç‰¹å¾å·¥ç¨‹ + æ ·æœ¬æƒé‡ + æ ‡å‡†åŒ–")

            self.logger.info("ğŸ¤– æ­¥éª¤B: æ¨¡å‹è®­ç»ƒ")
            self.logger.info("   ğŸ¯ ç›®æ ‡: è®­ç»ƒRandomForeståˆ†ç±»æ¨¡å‹")
            self.logger.info("   âš™ï¸ é…ç½®: 150æ£µæ ‘, æ·±åº¦12, å¹³è¡¡æƒé‡")
            self.logger.info("   ğŸ“Š æ•°æ®: ç‰¹å¾å·¥ç¨‹ + æ ·æœ¬æƒé‡ + æ ‡å‡†åŒ–")
            step_b_start = time.time()

            model_result = self.full_train(data, strategy_module)
            step_b_time = time.time() - step_b_start
            optimization_result['model_training'] = model_result

            if model_result['success']:
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"âœ… æ­¥éª¤Bå®Œæˆ (è€—æ—¶: {step_b_time:.2f}s) [{current_time}]")
                print(f"   ğŸ“Š è®­ç»ƒæ ·æœ¬: {model_result.get('train_samples', 0):,}æ¡")
                print(f"   ğŸ“ˆ ç‰¹å¾æ•°é‡: {model_result.get('feature_count', 0)}ä¸ª")
                print(f"   ğŸ“Š æ­£æ ·æœ¬æ¯”ä¾‹: {model_result.get('positive_ratio', 0):.2%}")
                print(f"   ğŸ’¾ æ¨¡å‹ä¿å­˜: {'æˆåŠŸ' if model_result.get('save_success', False) else 'å¤±è´¥'}")

                self.logger.info(f"âœ… æ­¥éª¤Bå®Œæˆ (è€—æ—¶: {step_b_time:.2f}s)")
                self.logger.info(f"   ğŸ“Š è®­ç»ƒæ ·æœ¬: {model_result.get('train_samples', 0):,}æ¡")
                self.logger.info(f"   ğŸ“ˆ ç‰¹å¾æ•°é‡: {model_result.get('feature_count', 0)}ä¸ª")
                self.logger.info(f"   ğŸ“Š æ­£æ ·æœ¬æ¯”ä¾‹: {model_result.get('positive_ratio', 0):.2%}")
                self.logger.info(f"   ğŸ’¾ æ¨¡å‹ä¿å­˜: {'æˆåŠŸ' if model_result.get('save_success', False) else 'å¤±è´¥'}")
            else:
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"âŒ æ­¥éª¤Bå¤±è´¥ (è€—æ—¶: {step_b_time:.2f}s) [{current_time}]")
                self.logger.error(f"âŒ æ­¥éª¤Bå¤±è´¥ (è€—æ—¶: {step_b_time:.2f}s)")
                optimization_result['errors'].append("æ¨¡å‹è®­ç»ƒå¤±è´¥")

                # è®¡ç®—å·²è€—æ—¶é—´
                elapsed_time = time.time() - complete_start_time
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"ğŸ’” ä¼˜åŒ–æµç¨‹ä¸­æ–­ (å·²è¿è¡Œ: {elapsed_time:.2f}s) [{current_time}]")
                self.logger.error(f"ğŸ’” ä¼˜åŒ–æµç¨‹ä¸­æ–­ (å·²è¿è¡Œ: {elapsed_time:.2f}s)")
                return optimization_result

            print("-" * 80)
            self.logger.info("-" * 80)

            # æ­¥éª¤C: æœ€ç»ˆæ€§èƒ½è¯„ä¼°
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"ğŸ“Š æ­¥éª¤C: æœ€ç»ˆæ€§èƒ½è¯„ä¼° [{current_time}]")
            print("   ğŸ¯ ç›®æ ‡: éªŒè¯æ•´ä½“ç³»ç»Ÿæ€§èƒ½")
            print("   ğŸ“Š æŒ‡æ ‡: ç­–ç•¥å¾—åˆ† + AIç½®ä¿¡åº¦ + è¯†åˆ«æ•ˆæœ")

            self.logger.info("ğŸ“Š æ­¥éª¤C: æœ€ç»ˆæ€§èƒ½è¯„ä¼°")
            self.logger.info("   ğŸ¯ ç›®æ ‡: éªŒè¯æ•´ä½“ç³»ç»Ÿæ€§èƒ½")
            self.logger.info("   ğŸ“Š æŒ‡æ ‡: ç­–ç•¥å¾—åˆ† + AIç½®ä¿¡åº¦ + è¯†åˆ«æ•ˆæœ")
            step_c_start = time.time()

            evaluation_result = self.evaluate_optimized_system(data, strategy_module)
            step_c_time = time.time() - step_c_start
            optimization_result['final_evaluation'] = evaluation_result

            if evaluation_result.get('success'):
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"âœ… æ­¥éª¤Cå®Œæˆ (è€—æ—¶: {step_c_time:.2f}s) [{current_time}]")
                print(f"   ğŸ¯ ç­–ç•¥å¾—åˆ†: {evaluation_result.get('strategy_score', 0):.4f}")
                print(f"   ğŸ“Š æˆåŠŸç‡: {evaluation_result.get('strategy_success_rate', 0):.2%}")
                print(f"   ğŸ” äº¤æ˜“æ•°: {evaluation_result.get('identified_points', 0)}")
                print(f"   ğŸ¤– AIç½®ä¿¡åº¦: {evaluation_result.get('ai_confidence', 0):.4f}")

                self.logger.info(f"âœ… æ­¥éª¤Cå®Œæˆ (è€—æ—¶: {step_c_time:.2f}s)")
                self.logger.info(f"   ğŸ¯ ç­–ç•¥å¾—åˆ†: {evaluation_result.get('strategy_score', 0):.4f}")
                self.logger.info(f"   ğŸ“Š æˆåŠŸç‡: {evaluation_result.get('strategy_success_rate', 0):.2%}")
                self.logger.info(f"   ğŸ” äº¤æ˜“æ•°: {evaluation_result.get('identified_points', 0)}")
                self.logger.info(f"   ğŸ¤– AIç½®ä¿¡åº¦: {evaluation_result.get('ai_confidence', 0):.4f}")
            else:
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"âš ï¸ æ­¥éª¤Céƒ¨åˆ†å¤±è´¥ (è€—æ—¶: {step_c_time:.2f}s) [{current_time}]")
                self.logger.warning(f"âš ï¸ æ­¥éª¤Céƒ¨åˆ†å¤±è´¥ (è€—æ—¶: {step_c_time:.2f}s)")
                optimization_result['errors'].append("æœ€ç»ˆè¯„ä¼°éƒ¨åˆ†å¤±è´¥")

            print("-" * 80)
            self.logger.info("-" * 80)

            # æ­¥éª¤D: ä¿å­˜ä¼˜åŒ–ç»“æœ
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"ğŸ’¾ æ­¥éª¤D: ä¿å­˜ä¼˜åŒ–ç»“æœ [{current_time}]")
            self.logger.info("ğŸ’¾ æ­¥éª¤D: ä¿å­˜ä¼˜åŒ–ç»“æœ")
            step_d_start = time.time()

            if strategy_result['success']:
                print("   ğŸ“ ä¿å­˜æœ€ä¼˜å‚æ•°åˆ°é…ç½®æ–‡ä»¶...")
                self.logger.info("   ğŸ“ ä¿å­˜æœ€ä¼˜å‚æ•°åˆ°é…ç½®æ–‡ä»¶...")
                self._save_optimized_parameters(strategy_result['best_params'])
                print("   âœ… å‚æ•°ä¿å­˜å®Œæˆ")
                self.logger.info("   âœ… å‚æ•°ä¿å­˜å®Œæˆ")
            else:
                print("   âš ï¸ è·³è¿‡å‚æ•°ä¿å­˜ (ç­–ç•¥ä¼˜åŒ–å¤±è´¥)")
                self.logger.info("   âš ï¸ è·³è¿‡å‚æ•°ä¿å­˜ (ç­–ç•¥ä¼˜åŒ–å¤±è´¥)")

            step_d_time = time.time() - step_d_start
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"âœ… æ­¥éª¤Då®Œæˆ (è€—æ—¶: {step_d_time:.2f}s) [{current_time}]")
            self.logger.info(f"âœ… æ­¥éª¤Då®Œæˆ (è€—æ—¶: {step_d_time:.2f}s)")

            # è®¾ç½®æœ€ç»ˆæˆåŠŸçŠ¶æ€å’Œæœ€ä½³å¾—åˆ†
            optimization_result['success'] = model_result['success']

            # è®¾ç½®æœ€ä½³å¾—åˆ† - ä¼˜å…ˆä½¿ç”¨ç­–ç•¥ä¼˜åŒ–çš„å¾—åˆ†
            if strategy_result.get('success') and 'best_score' in strategy_result:
                optimization_result['best_score'] = strategy_result['best_score']
            elif model_result.get('success') and 'score' in model_result:
                optimization_result['best_score'] = model_result['score']
            elif evaluation_result.get('success') and 'score' in evaluation_result:
                optimization_result['best_score'] = evaluation_result['score']
            else:
                optimization_result['best_score'] = 0.0

            # æ€»ç»“æŠ¥å‘Š
            total_time = time.time() - complete_start_time
            current_time = datetime.now().strftime("%H:%M:%S")
            print("=" * 80)
            print(f"ğŸ‰ å®Œæ•´AIä¼˜åŒ–æµç¨‹å®Œæˆ! [{current_time}]")
            print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}s ({total_time / 60:.1f}åˆ†é’Ÿ)")
            print("ğŸ“Š å„æ­¥éª¤è€—æ—¶åˆ†æ:")
            print(f"   ğŸ”§ ç­–ç•¥ä¼˜åŒ–: {step_a_time:.2f}s ({(step_a_time / total_time) * 100:.1f}%)")
            print(f"   ğŸ¤– æ¨¡å‹è®­ç»ƒ: {step_b_time:.2f}s ({(step_b_time / total_time) * 100:.1f}%)")
            print(f"   ğŸ“Š æ€§èƒ½è¯„ä¼°: {step_c_time:.2f}s ({(step_c_time / total_time) * 100:.1f}%)")
            print(f"   ğŸ’¾ ç»“æœä¿å­˜: {step_d_time:.2f}s ({(step_d_time / total_time) * 100:.1f}%)")

            self.logger.info("=" * 80)
            self.logger.info("ğŸ‰ å®Œæ•´AIä¼˜åŒ–æµç¨‹å®Œæˆ!")
            self.logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}s ({total_time / 60:.1f}åˆ†é’Ÿ)")
            self.logger.info("ğŸ“Š å„æ­¥éª¤è€—æ—¶åˆ†æ:")
            self.logger.info(f"   ğŸ”§ ç­–ç•¥ä¼˜åŒ–: {step_a_time:.2f}s ({(step_a_time / total_time) * 100:.1f}%)")
            self.logger.info(f"   ğŸ¤– æ¨¡å‹è®­ç»ƒ: {step_b_time:.2f}s ({(step_b_time / total_time) * 100:.1f}%)")
            self.logger.info(f"   ğŸ“Š æ€§èƒ½è¯„ä¼°: {step_c_time:.2f}s ({(step_c_time / total_time) * 100:.1f}%)")
            self.logger.info(f"   ğŸ’¾ ç»“æœä¿å­˜: {step_d_time:.2f}s ({(step_d_time / total_time) * 100:.1f}%)")

            success_steps = sum([
                1 if strategy_result['success'] else 0,
                1 if model_result['success'] else 0,
                1 if evaluation_result.get('success', False) else 0
            ])
            print(f"âœ… æˆåŠŸæ­¥éª¤: {success_steps}/3")
            self.logger.info(f"âœ… æˆåŠŸæ­¥éª¤: {success_steps}/3")

            if optimization_result['errors']:
                print("âš ï¸ é‡åˆ°çš„é—®é¢˜:")
                self.logger.warning("âš ï¸ é‡åˆ°çš„é—®é¢˜:")
                for error in optimization_result['errors']:
                    print(f"   - {error}")
                    self.logger.warning(f"   - {error}")

            print("=" * 80)
            self.logger.info("=" * 80)
            return optimization_result

        except Exception as e:
            elapsed_time = time.time() - complete_start_time
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"ğŸ’¥ å®Œæ•´AIä¼˜åŒ–æµç¨‹å¼‚å¸¸å¤±è´¥ (å·²è¿è¡Œ: {elapsed_time:.2f}s): {e} [{current_time}]")
            self.logger.error(f"ğŸ’¥ å®Œæ•´AIä¼˜åŒ–æµç¨‹å¼‚å¸¸å¤±è´¥ (å·²è¿è¡Œ: {elapsed_time:.2f}s): {e}")
            import traceback
            traceback_str = traceback.format_exc()
            print(f"å¼‚å¸¸è¯¦æƒ…: {traceback_str}")
            self.logger.error(f"å¼‚å¸¸è¯¦æƒ…: {traceback_str}")
            return {
                'success': False,
                'error': str(e),
                'strategy_optimization': {},
                'model_training': {},
                'final_evaluation': {},
                'elapsed_time': elapsed_time,
                'best_score': 0.0
            }

    def optimize_strategy_parameters_improved(self, strategy_module, data: pd.DataFrame) -> Dict[str, Any]:
        """
        ç­–ç•¥å‚æ•°ä¼˜åŒ–ï¼ˆè´å¶æ–¯ä¼˜åŒ–é«˜ç²¾åº¦æ¨¡å¼ï¼‰
        
        å‚æ•°:
        strategy_module: ç­–ç•¥æ¨¡å—
        data: å†å²æ•°æ®
        
        è¿”å›:
        dict: ä¼˜åŒ–ç»“æœ
        """
        from datetime import datetime
        optimization_start_time = time.time()

        # åŒæ—¶ä½¿ç”¨printå’Œloggerç¡®ä¿è¾“å‡ºå¯è§
        current_time = datetime.now().strftime("%H:%M:%S")
        print(f"    ğŸš€ å¯åŠ¨ç­–ç•¥å‚æ•°ä¼˜åŒ–å­æµç¨‹ [{current_time}]")
        print(f"    ğŸ“Š æ•°æ®è§„æ¨¡: {len(data)} æ¡è®°å½•")

        self.logger.info("ğŸš€ å¼€å§‹ç­–ç•¥å‚æ•°ä¼˜åŒ–ï¼ˆè´å¶æ–¯ä¼˜åŒ–ï¼‰")
        self.logger.info("=" * 80)

        try:
            # æ­¥éª¤1: æ•°æ®åˆ†å‰²ä¸éªŒè¯
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"    ğŸ“Š å­æ­¥éª¤1: æ•°æ®åˆ†å‰²ä¸éªŒè¯ [{current_time}]")
            self.logger.info("ğŸ“Š æ­¥éª¤1: æ•°æ®åˆ†å‰²ä¸éªŒè¯...")
            split_start_time = time.time()

            # ä½¿ç”¨é…ç½®ä¸­çš„æ•°æ®åˆ†å‰²æ¯”ä¾‹
            train_ratio = self.config.get('validation', {}).get('train_ratio', 0.7)
            validation_ratio = self.config.get('validation', {}).get('validation_ratio', 0.2)
            test_ratio = self.config.get('validation', {}).get('test_ratio', 0.1)

            # æ•°æ®åˆ†å‰²
            train_size = int(len(data) * train_ratio)
            val_size = int(len(data) * validation_ratio)

            train_data = data.iloc[:train_size].copy()
            validation_data = data.iloc[train_size:train_size + val_size].copy()
            test_data = data.iloc[train_size + val_size:].copy()

            split_time = time.time() - split_start_time

            print(f"    âœ… æ•°æ®åˆ†å‰²å®Œæˆ (è€—æ—¶: {split_time:.2f}s)")
            print(f"       ğŸ“Š è®­ç»ƒé›†: {len(train_data)}æ¡ ({train_ratio * 100:.1f}%)")
            print(f"       ğŸ“ˆ éªŒè¯é›†: {len(validation_data)}æ¡ ({validation_ratio * 100:.1f}%)")
            print(f"       ğŸ”’ æµ‹è¯•é›†: {len(test_data)}æ¡ ({test_ratio * 100:.1f}%)")

            self.logger.info(f"âœ… æ•°æ®åˆ†å‰²å®Œæˆ (è€—æ—¶: {split_time:.2f}s):")
            self.logger.info(f"   ğŸ“Š è®­ç»ƒé›†: {len(train_data)}æ¡ ({train_ratio * 100:.1f}%) - ä»…ç”¨äºå‚æ•°ä¼˜åŒ–")
            self.logger.info(
                f"   ğŸ“ˆ éªŒè¯é›†: {len(validation_data)}æ¡ ({validation_ratio * 100:.1f}%) - ç”¨äºæ¨¡å‹éªŒè¯å’Œè¿‡æ‹Ÿåˆæ£€æµ‹")
            self.logger.info(f"   ğŸ”’ æµ‹è¯•é›†: {len(test_data)}æ¡ ({test_ratio * 100:.1f}%) - å®Œå…¨é”å®šï¼Œä»…æœ€ç»ˆè¯„ä¼°")
            self.logger.info("-" * 50)

            # æ­¥éª¤2: é€‰æ‹©ä¼˜åŒ–æ–¹æ³•ï¼ˆä¼˜å…ˆè´å¶æ–¯ä¼˜åŒ–ï¼‰
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"    ğŸ”§ å­æ­¥éª¤2: å‚æ•°ä¼˜åŒ–æ–¹æ³•é€‰æ‹© [{current_time}]")
            self.logger.info("ğŸ”§ æ­¥éª¤2: å‚æ•°ä¼˜åŒ–æ–¹æ³•é€‰æ‹©...")

            # æ£€æŸ¥é…ç½®
            bayesian_config = self.config.get('bayesian_optimization', {})
            bayesian_enabled = bayesian_config.get('enabled', True)
            # è´å¶æ–¯ä¼˜åŒ–ä¸ºä¸»è¦ä¼˜åŒ–æ–¹æ³•
            optimization_method_config = self.config.get('optimization_method', 'bayesian')
            advanced_config = self.config.get('advanced_optimization', {})
            advanced_enabled = advanced_config.get('enabled', True)

            if not advanced_config:
                print("    âš ï¸ advanced_optimizationé…ç½®ç¼ºå¤±ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                self.logger.warning("advanced_optimizationé…ç½®ç¼ºå¤±ï¼Œä½¿ç”¨é»˜è®¤å€¼")

            best_params = {}
            best_score = -float('inf')
            optimization_method = 'initial_params'

            # ğŸ”§ ä¿®å¤ï¼šä¿å­˜åˆå§‹ç­–ç•¥å‚æ•°ä½œä¸ºåŸºå‡†
            initial_params = strategy_module.get_current_params() if hasattr(strategy_module,
                                                                             'get_current_params') else {}
            if initial_params:
                # è¯„ä¼°åˆå§‹å‚æ•°ä½œä¸ºåŸºå‡†
                initial_backtest = strategy_module.backtest(train_data)
                initial_evaluation = strategy_module.evaluate_strategy(initial_backtest)
                initial_score = initial_evaluation.get('score', 0)

                best_params = initial_params.copy()
                best_score = initial_score

                print(f"    ğŸ“Š åˆå§‹å‚æ•°åŸºå‡†å¾—åˆ†: {initial_score:.6f}")
                self.logger.info(f"ğŸ“Š åˆå§‹å‚æ•°åŸºå‡†å¾—åˆ†: {initial_score:.6f}")

            # ğŸ”§ ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–å›ºå®šå‚æ•°å€¼
            strategy_config = self.config.get('strategy', {})
            fixed_rise_threshold = strategy_config.get('rise_threshold', 0.04)
            fixed_max_days = strategy_config.get('max_days', 20)
            
            print(f"   ğŸ”’ å›ºå®šå‚æ•°: rise_threshold={fixed_rise_threshold}, max_days={fixed_max_days}")
            self.logger.info(f"ğŸ”’ å›ºå®šå‚æ•°: rise_threshold={fixed_rise_threshold}, max_days={fixed_max_days}")

       
            # ç›´æ¥è¿›å…¥è´å¶æ–¯ä¼˜åŒ–å‚æ•°ä¼˜åŒ–æµç¨‹
            print("    ğŸ”¬ ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œå‚æ•°ä¼˜åŒ–")
            print("    ğŸ¯ é…ç½®å‚æ•°: 100æ¬¡è¯„ä¼° (æ™ºèƒ½æœç´¢)")
            print("    â³ é¢„è®¡è€—æ—¶: 5-10åˆ†é’Ÿï¼ˆé«˜æ•ˆæœç´¢ï¼‰")

            self.logger.info("ğŸ”¬ ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–è¿›è¡Œå‚æ•°ä¼˜åŒ–")
            bayesian_start_time = time.time()

            # ğŸ”§ å…³é”®ä¿®å¤ï¼šå®šä¹‰ä¸å½±å“ç­–ç•¥æ¨¡å—çŠ¶æ€çš„è¯„ä¼°å‡½æ•°
            current_best_params_in_bayesian = initial_params.copy()
            
            # ğŸ”§ ä¿®å¤ï¼šè®¡ç®—åˆå§‹å‚æ•°çš„ç»Ÿä¸€è¯„åˆ†ä½œä¸ºåŸºå‡†
            if initial_params:
                # è¯„ä¼°åˆå§‹å‚æ•°
                initial_backtest = strategy_module.backtest(train_data)
                initial_evaluation = strategy_module.evaluate_strategy(initial_backtest)
                
                # ä½¿ç”¨ç»Ÿä¸€çš„è¯„åˆ†æ–¹æ³•è®¡ç®—åˆå§‹å¾—åˆ†
                initial_unified_score = self._calculate_unified_score(initial_evaluation)
                
                current_best_score_in_bayesian = initial_unified_score
                print(f"    ğŸ“Š è´å¶æ–¯ä¼˜åŒ–åˆå§‹ç»Ÿä¸€å¾—åˆ†: {initial_unified_score:.6f}")
                self.logger.info(f"ğŸ“Š è´å¶æ–¯ä¼˜åŒ–åˆå§‹ç»Ÿä¸€å¾—åˆ†: {initial_unified_score:.6f}")
            else:
                current_best_score_in_bayesian = 0.0

            def evaluate_strategy_params(params):
                nonlocal current_best_params_in_bayesian, current_best_score_in_bayesian

                try:
                    # ğŸ”§ ä¿®å¤ï¼šrise_thresholdå’Œmax_daysæ˜¯å›ºå®šå‚æ•°ï¼Œä¸åº”è¯¥å‚ä¸ä¼˜åŒ–
                    # ç›´æ¥ä½¿ç”¨ä¼˜åŒ–ç®—æ³•ç”Ÿæˆçš„å‚æ•°ï¼Œä¸è¿›è¡Œå¼ºåˆ¶è¦†ç›–
                    complete_params = params.copy()

                    # ğŸ”§ å…³é”®ä¿®å¤ï¼šä¿å­˜å½“å‰ç­–ç•¥æ¨¡å—çŠ¶æ€
                    original_params = strategy_module.get_current_params() if hasattr(strategy_module,
                                                                                      'get_current_params') else None

                    # ä¸´æ—¶åº”ç”¨å‚æ•°è¿›è¡Œè¯„ä¼°
                    # print("    ä¸´æ—¶åº”ç”¨å‚æ•°è¿›è¡Œè¯„ä¼°")
                    strategy_module.update_params(complete_params)

                    # åœ¨è®­ç»ƒé›†ä¸Šè¯„ä¼°ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„å›ºå®š final_thresholdï¼Œä»…åœ¨ä¼˜åŒ–æœŸé—´ç”Ÿæ•ˆï¼‰
                    orig_ft = None
                    cw = strategy_module.config.setdefault('confidence_weights', {})
                    try:
                        orig_ft = cw.get('final_threshold', None)
                        # ä»é…ç½®è§£æå›ºå®š final_thresholdï¼ˆä¸å‚ä¸ä¼˜åŒ–ï¼‰
                        cw['final_threshold'] = resolve_confidence_param(strategy_module.config, 'final_threshold', 0.5)
                        backtest_results = strategy_module.backtest(train_data)
                        evaluation = strategy_module.evaluate_strategy(backtest_results)
                    finally:
                        # æ¢å¤final_thresholdï¼Œé¿å…æ±¡æŸ“å…¨å±€é…ç½®
                        if orig_ft is None:
                            if 'final_threshold' in cw:
                                del cw['final_threshold']
                        else:
                            cw['final_threshold'] = orig_ft

                    # ä½¿ç”¨ç»Ÿä¸€çš„è¯„åˆ†æ–¹æ³•
                    final_score = self._calculate_unified_score(evaluation)

                    # ğŸ¯ ä¿®å¤åçš„å‚æ•°ç®¡ç†é€»è¾‘ï¼šåªæœ‰æ›´å¥½çš„å‚æ•°æ‰ä¿ç•™åœ¨ç­–ç•¥æ¨¡å—ä¸­
                    if final_score > current_best_score_in_bayesian:
                        # æ–°å‚æ•°æ›´å¥½ï¼Œä¿ç•™åœ¨ç­–ç•¥æ¨¡å—ä¸­
                        prev_score = current_best_score_in_bayesian
                        current_best_params_in_bayesian = complete_params.copy()
                        current_best_score_in_bayesian = final_score
                        # ç­–ç•¥æ¨¡å—å·²ç»æ›´æ–°ä¸ºæ–°å‚æ•°ï¼Œä¸éœ€è¦é¢å¤–æ“ä½œ
                        self.logger.info(f"è´å¶æ–¯ä¼˜åŒ–å‘ç°æ›´ä¼˜å‚æ•°: å¾—åˆ† {final_score:.6f} > {prev_score:.6f}")
                        self.logger.info(f"å‚æ•°è¯¦æƒ…: ")
                        for param_name, param_value in complete_params.items():
                            print(f"          {param_name}: {param_value}")
                    else:
                        # æ–°å‚æ•°è¾ƒå·®ï¼Œå¿…é¡»æ¢å¤åˆ°ä¹‹å‰çš„æœ€ä½³å‚æ•°
                        if current_best_params_in_bayesian:
                            strategy_module.update_params(current_best_params_in_bayesian)
                        else:
                            # å¦‚æœæ²¡æœ‰æœ€ä½³å‚æ•°ï¼Œæ¢å¤åˆ°åŸå§‹å‚æ•°
                            if original_params:
                                strategy_module.update_params(original_params)

                    # è´å¶æ–¯ä¼˜åŒ–éœ€è¦è¿”å›è´Ÿå€¼ï¼ˆå› ä¸ºå®ƒæ˜¯æœ€å°åŒ–ç®—æ³•ï¼‰
                    return -final_score

                except Exception as e:
                    self.logger.warning(f"å‚æ•°è¯„ä¼°å¤±è´¥: {e}")
                    # å‡ºé”™æ—¶æ¢å¤åˆ°æœ€ä½³å‚æ•°æˆ–åŸå§‹å‚æ•°
                    if current_best_params_in_bayesian:
                        strategy_module.update_params(current_best_params_in_bayesian)
                    elif original_params:
                        strategy_module.update_params(original_params)
                    return 1.0  # è¿”å›æ­£å€¼è¡¨ç¤ºå¤±è´¥ï¼ˆè´å¶æ–¯ä¼˜åŒ–ä¼šé¿å…è¿™ä¸ªåŒºåŸŸï¼‰

            # è¿è¡Œè´å¶æ–¯ä¼˜åŒ–
            print(f"    ğŸ”¬ å¼€å§‹è´å¶æ–¯ä¼˜åŒ–å‚æ•°æœç´¢... [{datetime.now().strftime('%H:%M:%S')}]")
            self.logger.info("ğŸ”¬ å¼€å§‹è´å¶æ–¯ä¼˜åŒ–å‚æ•°æœç´¢...")
            bayesian_params = self.run_bayesian_optimization(evaluate_strategy_params)
            bayesian_time = time.time() - bayesian_start_time

            if bayesian_params:
                # ğŸ”§ ä¿®å¤ï¼šè´å¶æ–¯ä¼˜åŒ–å·²ç»é€šè¿‡è¯„ä¼°å‡½æ•°ç®¡ç†äº†æœ€ä½³å‚æ•°
                # è·å–è´å¶æ–¯ä¼˜åŒ–è¿‡ç¨‹ä¸­æ‰¾åˆ°çš„æœ€ä½³å‚æ•°ï¼ˆå·²ç»åœ¨ç­–ç•¥æ¨¡å—ä¸­ï¼‰
                final_bayesian_params = strategy_module.get_current_params() if hasattr(strategy_module,
                                                                                        'get_current_params') else bayesian_params

                # æœ€ç»ˆè¯„ä¼°è´å¶æ–¯ä¼˜åŒ–ç»“æœï¼ˆæ­¤æ—¶ç­–ç•¥æ¨¡å—å·²ç»æ˜¯æœ€ä½³çŠ¶æ€ï¼‰
                bayesian_backtest = strategy_module.backtest(train_data)
                bayesian_evaluation = strategy_module.evaluate_strategy(bayesian_backtest)
                bayesian_unified_score = self._calculate_unified_score(bayesian_evaluation)

                # å¦‚æœè´å¶æ–¯ä¼˜åŒ–ç»“æœæ›´å¥½ï¼Œæ›´æ–°å…¨å±€æœ€ä½³å‚æ•°
                if bayesian_unified_score > best_score:
                    best_params = final_bayesian_params.copy()  # ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ç®¡ç†çš„æœ€ä½³å‚æ•°
                    best_score = bayesian_unified_score
                    optimization_method = 'bayesian_optimization'

                    print(f"    âœ… è´å¶æ–¯ä¼˜åŒ–æ‰¾åˆ°æ›´ä¼˜å‚æ•°! å¾—åˆ†æå‡: {best_score:.6f}")
                    self.logger.info(f"âœ… è´å¶æ–¯ä¼˜åŒ–æ‰¾åˆ°æ›´ä¼˜å‚æ•°! å¾—åˆ†æå‡: {best_score:.6f}")
                else:
                    print(f"    âš ï¸ è´å¶æ–¯ä¼˜åŒ–ç»“æœæœªè¶…è¿‡å½“å‰æœ€ä¼˜ï¼Œæ¢å¤ä¹‹å‰æœ€ä½³å‚æ•°")
                    self.logger.info(f"âš ï¸ è´å¶æ–¯ä¼˜åŒ–ç»“æœæœªè¶…è¿‡å½“å‰æœ€ä¼˜ï¼Œæ¢å¤ä¹‹å‰æœ€ä½³å‚æ•°")
                    # æ¢å¤åˆ°ä¹‹å‰çš„æœ€ä½³å‚æ•°
                    strategy_module.update_params(best_params)
                    # å¦‚æœä½¿ç”¨çš„æ˜¯åˆå§‹å‚æ•°ï¼Œä¿æŒoptimization_methodä¸ºinitial_params
                    if optimization_method == 'initial_params':
                        optimization_method = 'initial_params_retained'

                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"    ğŸ”¬ è´å¶æ–¯ä¼˜åŒ–å®Œæˆ (è€—æ—¶: {bayesian_time:.2f}s) [{current_time}]")
                print(f"       ğŸ“ˆ æœ€ä¼˜å¾—åˆ†: {bayesian_unified_score:.6f}")
                print(f"       ğŸ“Š æˆåŠŸç‡: {bayesian_evaluation.get('success_rate', 0):.2%}")
                print(f"       ğŸ” äº¤æ˜“æ•°: {bayesian_evaluation.get('total_trades', 0)}")
                print(f"       ğŸ“ˆ å¹³å‡æ”¶ç›Š: {bayesian_evaluation.get('avg_return', 0):.2%}")

                self.logger.info(f"ğŸ”¬ è´å¶æ–¯ä¼˜åŒ–å®Œæˆ (è€—æ—¶: {bayesian_time:.2f}s)")
                self.logger.info(f"   æœ€ä¼˜å¾—åˆ†: {bayesian_unified_score:.6f}")
                self.logger.info(f"   æˆåŠŸç‡: {bayesian_evaluation.get('success_rate', 0):.2%}")
                self.logger.info(f"   äº¤æ˜“æ•°: {bayesian_evaluation.get('total_trades', 0)}")
                self.logger.info(f"   å¹³å‡æ”¶ç›Š: {bayesian_evaluation.get('avg_return', 0):.2%}")
            else:
                print("    âš ï¸ è´å¶æ–¯ä¼˜åŒ–æœªæ‰¾åˆ°æœ‰æ•ˆè§£")
                self.logger.warning("âš ï¸ è´å¶æ–¯ä¼˜åŒ–æœªæ‰¾åˆ°æœ‰æ•ˆè§£")
                # å¦‚æœè´å¶æ–¯ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åˆå§‹å‚æ•°
                if optimization_method == 'initial_params':
                    optimization_method = 'initial_params_fallback'

            # éªŒè¯æœ€ä½³å‚æ•°
            if not best_params:
                print("    âŒ æ‰€æœ‰ä¼˜åŒ–æ–¹æ³•éƒ½æœªæ‰¾åˆ°æœ‰æ•ˆå‚æ•°")
                return {
                    'success': False,
                    'error': 'æ‰€æœ‰ä¼˜åŒ–æ–¹æ³•éƒ½æœªæ‰¾åˆ°æœ‰æ•ˆå‚æ•°'
                }

            # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿ç­–ç•¥æ¨¡å—åº”ç”¨æœ€ç»ˆçš„æœ€ä½³å‚æ•°
            print(f"    ğŸ¯ åº”ç”¨æœ€ä½³å‚æ•°åˆ°ç­–ç•¥æ¨¡å— (å¾—åˆ†: {best_score:.6f})")
            self.logger.info(f"ğŸ¯ åº”ç”¨æœ€ä½³å‚æ•°åˆ°ç­–ç•¥æ¨¡å— (å¾—åˆ†: {best_score:.6f})")
            strategy_module.update_params(best_params)

            self.logger.info("-" * 60)

            # æ­¥éª¤3: éªŒè¯é›†éªŒè¯
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"    ğŸ“ˆ å­æ­¥éª¤3: éªŒè¯é›†éªŒè¯ [{current_time}]")
            self.logger.info("ğŸ“ˆ æ­¥éª¤3: åœ¨éªŒè¯é›†ä¸ŠéªŒè¯æœ€ä½³å‚æ•°...")
            validation_start_time = time.time()

            val_backtest = strategy_module.backtest(validation_data)
            val_evaluation = strategy_module.evaluate_strategy(val_backtest)
            val_score = val_evaluation['score']
            val_success_rate = val_evaluation.get('success_rate', 0)
            val_total_points = val_evaluation.get('total_trades', val_evaluation.get('total_points', 0))
            val_avg_rise = val_evaluation.get('avg_return', val_evaluation.get('avg_rise', 0))

            validation_time = time.time() - validation_start_time

            # æ£€æŸ¥è¿‡æ‹Ÿåˆ
            overfitting_threshold = 0.8  # éªŒè¯é›†å¾—åˆ†åº”è¯¥è‡³å°‘æ˜¯è®­ç»ƒé›†å¾—åˆ†çš„80%
            overfitting_passed = val_score >= best_score * overfitting_threshold

            print(f"    âœ… éªŒè¯é›†è¯„ä¼°å®Œæˆ (è€—æ—¶: {validation_time:.2f}s)")
            print(f"       å¾—åˆ†: {val_score:.6f}")
            print(f"       æˆåŠŸç‡: {val_success_rate:.2%}")
            print(f"       äº¤æ˜“æ•°: {val_total_points}")
            print(f"       å¹³å‡æ”¶ç›Š: {val_avg_rise:.2%}")
            print(f"       è¿‡æ‹Ÿåˆæ£€æµ‹: {'âœ… é€šè¿‡' if overfitting_passed else 'âš ï¸ è­¦å‘Š'}")

            self.logger.info(f"âœ… éªŒè¯é›†è¯„ä¼°å®Œæˆ (è€—æ—¶: {validation_time:.2f}s)")
            self.logger.info(f"   å¾—åˆ†: {val_score:.6f}")
            self.logger.info(f"   æˆåŠŸç‡: {val_success_rate:.2%}")
            self.logger.info(f"   äº¤æ˜“æ•°: {val_total_points}")
            self.logger.info(f"   å¹³å‡æ”¶ç›Š: {val_avg_rise:.2%}")
            self.logger.info(f"   è¿‡æ‹Ÿåˆæ£€æµ‹: {'âœ… é€šè¿‡' if overfitting_passed else 'âš ï¸ è­¦å‘Š'}")
            self.logger.info("-" * 60)

            # æ­¥éª¤4: æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"    ğŸ”’ å­æ­¥éª¤4: æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼° [{current_time}]")
            self.logger.info("ğŸ”’ æ­¥éª¤4: åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
            test_start_time = time.time()

            test_backtest = strategy_module.backtest(test_data)
            test_evaluation = strategy_module.evaluate_strategy(test_backtest)
            test_score = test_evaluation['score']
            test_success_rate = test_evaluation.get('success_rate', 0)
            test_total_points = test_evaluation.get('total_trades', test_evaluation.get('total_points', 0))
            test_avg_rise = test_evaluation.get('avg_return', test_evaluation.get('avg_rise', 0))

            test_time = time.time() - test_start_time

            # è¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼ˆæ·»åŠ å®‰å…¨æ£€æŸ¥ï¼‰
            if val_score > 0.001:  # é¿å…é™¤é›¶é”™è¯¯
                generalization_ratio = test_score / val_score
            else:
                generalization_ratio = 0.0
                print("    âš ï¸ éªŒè¯é›†å¾—åˆ†è¿‡ä½ï¼Œæ— æ³•è®¡ç®—æ³›åŒ–æ¯”ç‡")
                self.logger.warning("éªŒè¯é›†å¾—åˆ†è¿‡ä½ï¼Œæ— æ³•è®¡ç®—æ³›åŒ–æ¯”ç‡")

            generalization_passed = generalization_ratio >= 0.85  # æµ‹è¯•é›†å¾—åˆ†åº”è¯¥æ¥è¿‘éªŒè¯é›†

            print(f"    âœ… æµ‹è¯•é›†è¯„ä¼°å®Œæˆ (è€—æ—¶: {test_time:.2f}s)")
            print(f"       å¾—åˆ†: {test_score:.6f}")
            print(f"       æˆåŠŸç‡: {test_success_rate:.2%}")
            print(f"       äº¤æ˜“æ•°: {test_total_points}")
            print(f"       å¹³å‡æ”¶ç›Š: {test_avg_rise:.2%}")
            print(
                f"       æ³›åŒ–èƒ½åŠ›: {'âœ… è‰¯å¥½' if generalization_passed else 'âš ï¸ ä¸€èˆ¬'} (æ¯”ç‡: {generalization_ratio:.3f})")

            self.logger.info(f"âœ… æµ‹è¯•é›†è¯„ä¼°å®Œæˆ (è€—æ—¶: {test_time:.2f}s)")
            self.logger.info(f"   å¾—åˆ†: {test_score:.6f}")
            self.logger.info(f"   æˆåŠŸç‡: {test_success_rate:.2%}")
            self.logger.info(f"   äº¤æ˜“æ•°: {test_total_points}")
            self.logger.info(f"   å¹³å‡æ”¶ç›Š: {test_avg_rise:.2%}")
            self.logger.info(
                f"   æ³›åŒ–èƒ½åŠ›: {'âœ… è‰¯å¥½' if generalization_passed else 'âš ï¸ ä¸€èˆ¬'} (æ¯”ç‡: {generalization_ratio:.3f})")

            # æ€»ç»“
            optimization_total_time = time.time() - optimization_start_time
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"    ğŸ‰ ç­–ç•¥å‚æ•°ä¼˜åŒ–å­æµç¨‹å®Œæˆ! [{current_time}]")
            print(f"    â±ï¸ æ€»è€—æ—¶: {optimization_total_time:.2f}s ({optimization_total_time / 60:.1f}åˆ†é’Ÿ)")
            print(f"    ğŸ”§ ä¼˜åŒ–æ–¹æ³•: {optimization_method}")
            print(f"    ğŸ“Š ä¸‰å±‚éªŒè¯ç»“æœ:")
            print(f"       è®­ç»ƒé›†å¾—åˆ†: {best_score:.6f}")
            print(f"       éªŒè¯é›†å¾—åˆ†: {val_score:.6f} | æˆåŠŸç‡: {val_success_rate:.2%}")
            print(f"       æµ‹è¯•é›†å¾—åˆ†: {test_score:.6f} | æˆåŠŸç‡: {test_success_rate:.2%}")
            print(f"       ğŸ›¡ï¸ è¿‡æ‹Ÿåˆæ£€æµ‹: {'é€šè¿‡' if overfitting_passed else 'è­¦å‘Š'}")
            print(f"       ğŸ¯ æ³›åŒ–èƒ½åŠ›: {'è‰¯å¥½' if generalization_passed else 'ä¸€èˆ¬'}")

            self.logger.info("=" * 80)
            self.logger.info(f"ğŸ‰ ç­–ç•¥å‚æ•°ä¼˜åŒ–å®Œæˆ!")
            self.logger.info(f"â±ï¸ æ€»è€—æ—¶: {optimization_total_time:.2f}s ({optimization_total_time / 60:.1f}åˆ†é’Ÿ)")
            self.logger.info(f"ğŸ”§ ä¼˜åŒ–æ–¹æ³•: {optimization_method}")
            self.logger.info(f"ğŸ“Š ä¸‰å±‚éªŒè¯ç»“æœ:")
            self.logger.info(f"   è®­ç»ƒé›†å¾—åˆ†: {best_score:.6f}")
            self.logger.info(f"   éªŒè¯é›†å¾—åˆ†: {val_score:.6f} | æˆåŠŸç‡: {val_success_rate:.2%}")
            self.logger.info(f"   æµ‹è¯•é›†å¾—åˆ†: {test_score:.6f} | æˆåŠŸç‡: {test_success_rate:.2%}")
            self.logger.info(f"   ğŸ›¡ï¸ è¿‡æ‹Ÿåˆæ£€æµ‹: {'é€šè¿‡' if overfitting_passed else 'è­¦å‘Š'}")
            self.logger.info(f"   ğŸ¯ æ³›åŒ–èƒ½åŠ›: {'è‰¯å¥½' if generalization_passed else 'ä¸€èˆ¬'}")

            # å¦‚æœä½¿ç”¨äº†è´å¶æ–¯ä¼˜åŒ–ï¼Œè¾“å‡ºè¯¦ç»†çš„å‚æ•°ä¿¡æ¯
            if optimization_method == 'bayesian_optimization':
                print(f"    ğŸ”¬ è´å¶æ–¯ä¼˜åŒ–æœ€ä¼˜å‚æ•°è¯¦æƒ…:")
                self.logger.info(f"\nğŸ”¬ è´å¶æ–¯ä¼˜åŒ–æœ€ä¼˜å‚æ•°è¯¦æƒ…:")
                for param_name, param_value in best_params.items():
                    print(f"       {param_name}: {param_value}")
                    self.logger.info(f"   {param_name}: {param_value}")

            self.logger.info("=" * 80)

            return {
                'success': True,
                'best_params': best_params,
                'best_score': best_score,
                'validation_score': val_score,
                'validation_success_rate': val_success_rate,
                # å…¼å®¹å‘½åï¼šä»ä¿ç•™æ—§å­—æ®µï¼Œä½†å«ä¹‰æ”¹ä¸ºäº¤æ˜“æ•°/å¹³å‡æ”¶ç›Š
                'validation_total_points': val_total_points,
                'validation_avg_rise': val_avg_rise,
                # æ–°å­—æ®µ
                'validation_total_trades': val_total_points,
                'validation_avg_return': val_avg_rise,
                'test_score': test_score,
                'test_success_rate': test_success_rate,
                'test_total_points': test_total_points,
                'test_avg_rise': test_avg_rise,
                'test_total_trades': test_total_points,
                'test_avg_return': test_avg_rise,
                'overfitting_passed': overfitting_passed,
                'generalization_passed': generalization_passed,
                'generalization_ratio': generalization_ratio,
                'optimization_method': optimization_method,
                'optimization_time': optimization_total_time,
                'bayesian_optimization_used': optimization_method == 'bayesian_optimization'
            }

        except Exception as e:
            current_time = datetime.now().strftime("%H:%M:%S")
            print(f"    âŒ ç­–ç•¥å‚æ•°ä¼˜åŒ–å¤±è´¥: {e} [{current_time}]")
            self.logger.error(f"ç­–ç•¥å‚æ•°ä¼˜åŒ–å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def evaluate_optimized_system(self, data: pd.DataFrame, strategy_module) -> Dict[str, Any]:
        """
        è¯„ä¼°ä¼˜åŒ–åçš„ç³»ç»Ÿ
        
        å‚æ•°:
        data: æµ‹è¯•æ•°æ®
        strategy_module: ç­–ç•¥æ¨¡å—
        
        è¿”å›:
        dict: è¯„ä¼°ç»“æœ
        """
        self.logger.info("è¯„ä¼°ä¼˜åŒ–åçš„ç³»ç»Ÿ")

        try:
            # ç­–ç•¥è¯„ä¼°
            backtest_results = strategy_module.backtest(data)
            strategy_evaluation = strategy_module.evaluate_strategy(backtest_results)

            # AIæ¨¡å‹é¢„æµ‹è¯„ä¼°
            prediction_result = self.predict_low_point(data)

            # æ–°å¢ï¼šè®°å½•ç”¨äºé¢„æµ‹çš„æ—¥æœŸå’Œæ¦‚ç‡å‘é‡ï¼Œä¾¿äºè¯Šæ–­
            try:
                prediction_date = None
                if 'date' in getattr(data, 'columns', []):
                    try:
                        prediction_date = pd.to_datetime(data['date'].iloc[-1]).strftime('%Y-%m-%d')
                    except Exception:
                        prediction_date = str(data['date'].iloc[-1])
                elif hasattr(data, 'index') and len(data.index) > 0:
                    try:
                        prediction_date = pd.to_datetime(data.index[-1]).strftime('%Y-%m-%d')
                    except Exception:
                        prediction_date = str(data.index[-1])

                proba = prediction_result.get('prediction_proba')

                if isinstance(proba, (list, tuple, np.ndarray)):
                    proba_iter = proba.tolist() if hasattr(proba, 'tolist') else list(proba)
                    proba_str = '[' + ', '.join(f'{float(p):.4f}' for p in proba_iter) + ']'
                else:
                    proba_str = str(proba)

                print(f"    ğŸ” è¯„ä¼°-é¢„æµ‹æ—¥æœŸ: {prediction_date} | æ¦‚ç‡å‘é‡: {proba_str}")
                self.logger.info(f"è¯„ä¼°-é¢„æµ‹æ—¥æœŸ: {prediction_date} | æ¦‚ç‡å‘é‡: {proba_str}")

                if prediction_result.get('error'):
                    print(f"    âš ï¸ è¯„ä¼°-é¢„æµ‹é”™è¯¯: {prediction_result.get('error')}")
                    self.logger.warning(f"è¯„ä¼°-é¢„æµ‹é”™è¯¯: {prediction_result.get('error')}")
            except Exception as log_ex:
                self.logger.warning(f"è®°å½•é¢„æµ‹ç»†èŠ‚æ—¶å‘ç”Ÿå¼‚å¸¸: {log_ex}")

            return {
                'success': True,
                'strategy_score': strategy_evaluation['score'],
                'strategy_success_rate': strategy_evaluation.get('success_rate', 0),
                'identified_points': strategy_evaluation.get('total_trades', strategy_evaluation.get('total_points', 0)),
                'avg_return': strategy_evaluation.get('avg_return', strategy_evaluation.get('avg_rise', 0)),
                'total_profit': strategy_evaluation.get('total_profit', 0),
                'ai_confidence': prediction_result.get('final_confidence', 0),
                'ai_prediction': prediction_result.get('is_low_point', False)
            }

        except Exception as e:
            self.logger.error(f"ç³»ç»Ÿè¯„ä¼°å¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def _save_optimized_parameters(self, best_params: Dict[str, Any]) -> bool:
        """
        ä¿å­˜ä¼˜åŒ–åçš„å‚æ•°åˆ°optimized_params.yamlæ–‡ä»¶
        
        å‚æ•°:
        best_params: ä¼˜åŒ–åçš„å‚æ•°
        """
        try:
            from src.utils.optimized_params_saver import save_optimized_params
            
            # æ·»åŠ ä¼˜åŒ–ä¿¡æ¯
            optimization_info = {
                'method': 'genetic_algorithm',
                'timestamp': datetime.now().isoformat(),
                'param_count': len(best_params)
            }
            
            # ä¿å­˜å‚æ•°åˆ°optimized_params.yaml
            success = save_optimized_params(
                params=best_params,
                optimization_info=optimization_info
            )
            
            if success:
                self.logger.info(f"ä¼˜åŒ–å‚æ•°å·²ä¿å­˜åˆ°optimized_params.yamlï¼Œå…±{len(best_params)}ä¸ªå‚æ•°")
                return True
            else:
                self.logger.error("ä¿å­˜ä¼˜åŒ–å‚æ•°å¤±è´¥")
                return False
                
        except Exception as e:
             self.logger.error(f"ä¿å­˜ä¼˜åŒ–å‚æ•°æ—¶å‘ç”Ÿé”™è¯¯: {e}")
             return False

    # å·²ç§»é™¤_save_params_fallbackæ–¹æ³•ï¼Œç°åœ¨ç»Ÿä¸€ä½¿ç”¨optimized_params_saver

    def run_bayesian_optimization(self, evaluate_func, param_ranges=None) -> Dict[str, Any]:
        """
        è´å¶æ–¯ä¼˜åŒ–å‚æ•°ä¼˜åŒ–ï¼ˆé«˜ç²¾åº¦ç‰ˆæœ¬ï¼‰
        
        ä¸“ä¸ºé«˜å‡†ç¡®åº¦è®¾è®¡ï¼Œä½¿ç”¨scikit-optimizeè¿›è¡Œè´å¶æ–¯ä¼˜åŒ–
        
        å‚æ•°:
        evaluate_func: è¯„ä¼°å‡½æ•°ï¼Œæ¥æ”¶å‚æ•°å­—å…¸ï¼Œè¿”å›è¯„åˆ†
        param_ranges: å‚æ•°èŒƒå›´å­—å…¸ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶
        
        è¿”å›:
        dict: æœ€ä¼˜å‚æ•°å­—å…¸
        """
        from datetime import datetime

        print(f"        ğŸ”¬ åˆå§‹åŒ–è´å¶æ–¯ä¼˜åŒ– [{datetime.now().strftime('%H:%M:%S')}]")
        self.logger.info("ğŸ”¬ å¯åŠ¨è´å¶æ–¯ä¼˜åŒ–ï¼ˆé«˜ç²¾åº¦æ¨¡å¼ï¼‰")
        start_time = time.time()

        try:
            # è·å–è´å¶æ–¯ä¼˜åŒ–é…ç½®
            bayesian_config = self.config.get('bayesian_optimization', {})
            
            # é«˜ç²¾åº¦é…ç½®
            n_calls = bayesian_config.get('n_calls', 120)  # æ€»è°ƒç”¨æ¬¡æ•°
            n_initial_points = bayesian_config.get('n_initial_points', 25)  # åˆå§‹éšæœºç‚¹
            acq_func = bayesian_config.get('acq_func', 'EI')  # é‡‡é›†å‡½æ•°
            xi = bayesian_config.get('xi', 0.01)  # æ¢ç´¢å‚æ•°
            kappa = bayesian_config.get('kappa', 1.96)  # UCBå‚æ•°
            random_state = bayesian_config.get('random_state', 42)

            print(f"        ğŸ“Š è´å¶æ–¯ä¼˜åŒ–é…ç½®:")
            print(f"           æ€»è°ƒç”¨æ¬¡æ•°: {n_calls}")
            print(f"           åˆå§‹éšæœºç‚¹: {n_initial_points}")
            print(f"           é‡‡é›†å‡½æ•°: {acq_func}")
            print(f"           æ¢ç´¢å‚æ•°xi: {xi}")
            print(f"           UCBå‚æ•°kappa: {kappa}")

            self.logger.info(f"é«˜ç²¾åº¦è´å¶æ–¯ä¼˜åŒ–é…ç½®: è°ƒç”¨{n_calls}æ¬¡, åˆå§‹ç‚¹{n_initial_points}ä¸ª")

            # è·å–æˆ–ç”Ÿæˆå‚æ•°èŒƒå›´
            if param_ranges is None:
                param_ranges = self._get_enhanced_parameter_ranges({})

            print(f"        ğŸ¯ ä¼˜åŒ–å‚æ•°æ•°é‡: {len(param_ranges)} ä¸ª")

            # è‹¥æœç´¢ç©ºé—´ä¸ºç©ºï¼Œç›´æ¥è·³è¿‡å¹¶ç»™å‡ºæ¸…æ™°æç¤º
            if not param_ranges:
                msg = "å‚æ•°æœç´¢ç©ºé—´ä¸ºç©ºï¼Œè·³è¿‡è´å¶æ–¯ä¼˜åŒ–ï¼ˆè¯·æ£€æŸ¥é…ç½®optimization_rangesæˆ–å›ºå®šå‚æ•°è®¾ç½®ï¼‰"
                print(f"        âš ï¸ {msg}")
                self.logger.warning(msg)
                return {}

            # æ„å»ºæœç´¢ç©ºé—´
            dimensions = []
            param_names = []
            
            for param_name, param_range in param_ranges.items():
                param_names.append(param_name)
                
                if param_range['type'] == 'int':
                    dimensions.append(Integer(param_range['min'], param_range['max'], name=param_name))
                else:  # float
                    dimensions.append(Real(param_range['min'], param_range['max'], name=param_name))

            print(f"        ğŸŒ± æ„å»ºæœç´¢ç©ºé—´å®Œæˆ")
            self.logger.info(f"æœç´¢ç©ºé—´ç»´åº¦: {len(dimensions)}")

            # è‹¥ç»´åº¦ä¸º0ï¼Œæ— æ³•æ‰§è¡Œä¼˜åŒ–
            if len(dimensions) == 0:
                msg = "æœç´¢ç©ºé—´ç»´åº¦ä¸º0ï¼Œæ— æ³•æ‰§è¡Œè´å¶æ–¯ä¼˜åŒ–ï¼ˆå¯èƒ½æ‰€æœ‰å‚æ•°è¢«å›ºå®šæˆ–èŒƒå›´ç¼ºå¤±ï¼‰"
                print(f"        âš ï¸ {msg}")
                self.logger.warning(msg)
                return {}

            # å®šä¹‰ç›®æ ‡å‡½æ•°ï¼ˆè´å¶æ–¯ä¼˜åŒ–éœ€è¦æœ€å°åŒ–ï¼Œæ‰€ä»¥è¿”å›è´Ÿå€¼ï¼‰
            best_score = -float('inf')
            best_params = None
            evaluation_count = 0
            
            @use_named_args(dimensions)
            def objective(**params):
                nonlocal best_score, best_params, evaluation_count
                evaluation_count += 1
                
                try:
                    # éªŒè¯å¹¶ä¿®å¤å‚æ•°
                    validated_params = self._validate_and_fix_parameters(params, param_ranges)
                    
                    # ğŸ” è¯¦ç»†æ—¥å¿—ï¼šè®°å½•æ¯æ¬¡è¯„ä¼°çš„å‚æ•°å–å€¼
                    param_str = ", ".join([f"{k}={v:.4f}" if isinstance(v, float) else f"{k}={v}" 
                                          for k, v in validated_params.items()])
                    print(f"        ğŸ§ª è¯•éªŒ #{evaluation_count}: {param_str}")
                    self.logger.info(f"ğŸ§ª è¯•éªŒ #{evaluation_count}: {param_str}")
                    
                    # è¯„ä¼°å‚æ•°
                    score = evaluate_func(validated_params)
                    
                    # ğŸ” è¯¦ç»†æ—¥å¿—ï¼šè®°å½•æ¯æ¬¡è¯„ä¼°çš„å¾—åˆ†
                    # ä¿®å¤ï¼ševaluate_func è¿”å›çš„æ˜¯ç”¨äºæœ€å°åŒ–çš„"è´Ÿå¾—åˆ†"ï¼Œè¿™é‡Œè½¬æ¢ä¸ºæ­£å¾—åˆ†æ˜¾ç¤ºä¸æ¯”è¾ƒ
                    if score is None or not isinstance(score, (int, float)) or (isinstance(score, float) and (np.isnan(score) or np.isinf(score))):
                        print(f"           âŒ å¾—åˆ†: æ— æ•ˆ (score={score})")
                        self.logger.info(f"           âŒ å¾—åˆ†: æ— æ•ˆ (score={score})")
                        return 1.0  # è¿”å›æ­£å€¼è¡¨ç¤ºå·®çš„ç»“æœ
                    
                    actual_score = -score  # è½¬å›æ­£çš„ç»Ÿä¸€è¯„åˆ†ç”¨äºå±•ç¤ºä¸æ¯”è¾ƒ
                    print(f"           ğŸ“Š å¾—åˆ†: {actual_score:.6f}")
                    self.logger.info(f"           ğŸ“Š å¾—åˆ†: {actual_score:.6f}")
                    
                    # æ›´æ–°æœ€ä½³ç»“æœï¼ˆä½¿ç”¨æ­£å€¼è¿›è¡Œæ¯”è¾ƒï¼‰
                    if actual_score > best_score:
                        improvement = actual_score - best_score
                        best_score = actual_score
                        best_params = validated_params.copy()
                        print(f"        ğŸ‰ å‘ç°æ–°æœ€ä½³è§£! å¾—åˆ†: {best_score:.6f} (æ”¹è¿›: +{improvement:.6f}) (ç¬¬{evaluation_count}æ¬¡è¯„ä¼°)")
                        self.logger.info(f"ğŸ‰ å‘ç°æ–°æœ€ä½³è§£! å¾—åˆ†: {best_score:.6f} (æ”¹è¿›: +{improvement:.6f}) (ç¬¬{evaluation_count}æ¬¡è¯„ä¼°)")
                    else:
                        deficit = best_score - actual_score
                        print(f"           ğŸ” å½“å‰è§£åŠ£äºæœ€ä½³: -{deficit:.6f}")
                        self.logger.info(f"           ğŸ” å½“å‰è§£åŠ£äºæœ€ä½³: -{deficit:.6f}")
                    
                    # æ¯5æ¬¡è¯„ä¼°æ˜¾ç¤ºè¿›åº¦ï¼ˆæ›´é¢‘ç¹çš„åé¦ˆï¼‰
                    if evaluation_count % 5 == 0:
                        progress = (evaluation_count / n_calls) * 100
                        print(f"        ğŸ“ˆ è¯„ä¼°è¿›åº¦: {evaluation_count}/{n_calls} ({progress:.1f}%) | å½“å‰æœ€ä½³: {best_score:.6f}")
                        self.logger.info(f"ğŸ“ˆ è¯„ä¼°è¿›åº¦: {evaluation_count}/{n_calls} ({progress:.1f}%) | å½“å‰æœ€ä½³: {best_score:.6f}")
                    
                    return score  # ç›´æ¥è¿”å›ç”¨äºæœ€å°åŒ–çš„å€¼ï¼ˆè´Ÿå¾—åˆ†ï¼‰
                    
                except Exception as e:
                    print(f"           âŒ å‚æ•°è¯„ä¼°å¼‚å¸¸: {e}")
                    self.logger.warning(f"å‚æ•°è¯„ä¼°å¤±è´¥: {e}")
                    return 1.0  # è¿”å›æ­£å€¼è¡¨ç¤ºå·®çš„ç»“æœ

            print(f"        ğŸš€ å¼€å§‹è´å¶æ–¯ä¼˜åŒ– (æ€»è®¡ {n_calls} æ¬¡è¯„ä¼°)")
            
            # æ‰§è¡Œè´å¶æ–¯ä¼˜åŒ–
            result = gp_minimize(
                func=objective,
                dimensions=dimensions,
                n_calls=n_calls,
                n_initial_points=n_initial_points,
                acq_func=acq_func,
                xi=xi,
                kappa=kappa,
                random_state=random_state
            )
            
            optimization_time = time.time() - start_time
            
            print(f"        âœ… è´å¶æ–¯ä¼˜åŒ–å®Œæˆ!")
            print(f"        ğŸ† æœ€ä½³å¾—åˆ†: {best_score:.6f}")
            print(f"        â±ï¸ æ€»è€—æ—¶: {optimization_time:.2f}s ({optimization_time/60:.1f}åˆ†é’Ÿ)")
            print(f"        ğŸ“Š æ€»è¯„ä¼°æ¬¡æ•°: {evaluation_count}")
            
            self.logger.info(f"âœ… è´å¶æ–¯ä¼˜åŒ–å®Œæˆ!")
            self.logger.info(f"ğŸ† æœ€ä½³å¾—åˆ†: {best_score:.6f}")
            self.logger.info(f"â±ï¸ æ€»è€—æ—¶: {optimization_time:.2f}s ({optimization_time/60:.1f}åˆ†é’Ÿ)")
            self.logger.info(f"ğŸ“Š æ€»è¯„ä¼°æ¬¡æ•°: {evaluation_count}")
            
            if best_params is None:
                self.logger.error("è´å¶æ–¯ä¼˜åŒ–æœªæ‰¾åˆ°æœ‰æ•ˆå‚æ•°")
                return {}
            
            return best_params
            
        except Exception as e:
            self.logger.error(f"è´å¶æ–¯ä¼˜åŒ–æ‰§è¡Œå¤±è´¥: {e}")
            print(f"        âŒ è´å¶æ–¯ä¼˜åŒ–å¤±è´¥: {e}")
            return {}


    def _get_enhanced_parameter_ranges(self, base_ranges: dict) -> dict:
        """
        è·å–å¢å¼ºçš„å‚æ•°èŒƒå›´ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„èŒƒå›´ï¼‰
        
        å‚æ•°:
        base_ranges: åŸºç¡€å‚æ•°èŒƒå›´
        
        è¿”å›:
        dict: å¢å¼ºçš„å‚æ•°èŒƒå›´
        """
        # ğŸš¨ é‡è¦ï¼šå›ºå®šå‚æ•°ï¼Œä¸å‚ä¸è´å¶æ–¯ä¼˜åŒ–
        # ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–å›ºå®šå‚æ•°å€¼
        strategy_config = self.config.get('strategy', {})
        fixed_rise_threshold = strategy_config.get('rise_threshold', 0.04)
        fixed_max_days = strategy_config.get('max_days', 20)

        # å¯¼å…¥å‚æ•°é…ç½®
        from src.utils.param_config import FIXED_PARAMS, get_all_optimizable_params

        # ä»é…ç½®æ–‡ä»¶ä¸­è·å–å‚æ•°èŒƒå›´
        config = self.config
        strategy_ranges = config.get('strategy_ranges', {})
        optimization_ranges = config.get('optimization_ranges', {})

        enhanced_ranges = {}

        # æ·»åŠ strategy_rangesä¸­çš„å‚æ•°
        for param_name, param_config in strategy_ranges.items():
            # ğŸš¨ è·³è¿‡å›ºå®šå‚æ•°ï¼Œä¸å…è®¸ä¼˜åŒ–
            if param_name in FIXED_PARAMS:
                self.logger.info(f"âš ï¸ è·³è¿‡å›ºå®šå‚æ•° {param_name}ï¼Œæ­¤å‚æ•°ä¸å‚ä¸ä¼˜åŒ–")
                continue

            # è½¬æ¢é…ç½®æ ¼å¼
            enhanced_ranges[param_name] = {
                'min': param_config.get('min', 0),
                'max': param_config.get('max', 1),
                'type': 'int' if param_name.endswith('_threshold') and 'rsi' in param_name else 'float',
                'precision': 4
            }

        # æ·»åŠ optimization_rangesä¸­çš„å‚æ•°
        for param_name, param_config in optimization_ranges.items():
            # ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„ç±»å‹å®šä¹‰ï¼›è‹¥ç¼ºçœåˆ™æŒ‰åç§°è¿›è¡Œæ¨æ–­ï¼ˆä»…å¯¹RSIé˜ˆå€¼ä½¿ç”¨æ•´æ•°ï¼‰
            cfg_type = param_config.get('type') if isinstance(param_config, dict) else None
            inferred_type = 'int' if (param_name.endswith('_threshold') and 'rsi' in param_name) else 'float'
            final_type = cfg_type if cfg_type in ('int', 'float') else inferred_type
            precision = param_config.get('precision', 0 if final_type == 'int' else 4) if isinstance(param_config, dict) else (0 if final_type == 'int' else 4)

            enhanced_ranges[param_name] = {
                'min': param_config.get('min', 0),
                'max': param_config.get('max', 1),
                'type': final_type,
                'precision': precision
            }
            # ä¿ç•™ step ä¿¡æ¯ï¼Œä¾¿äºé—ä¼ ç®—æ³•æˆ–ç½‘æ ¼æœç´¢ä½¿ç”¨
            if isinstance(param_config, dict) and 'step' in param_config:
                enhanced_ranges[param_name]['step'] = param_config['step']

        # åˆå¹¶ç”¨æˆ·é…ç½®çš„èŒƒå›´ï¼ˆä½†æ’é™¤å›ºå®šå‚æ•°ï¼‰
        for param_name, param_config in base_ranges.items():
            # ğŸš¨ è·³è¿‡å›ºå®šå‚æ•°ï¼Œä¸å…è®¸ä¼˜åŒ–
            if param_name in FIXED_PARAMS:
                self.logger.info(f"âš ï¸ è·³è¿‡å›ºå®šå‚æ•° {param_name}ï¼Œæ­¤å‚æ•°ä¸å‚ä¸ä¼˜åŒ–")
                continue

            if param_name in enhanced_ranges:
                # æ›´æ–°ç°æœ‰å‚æ•°èŒƒå›´
                enhanced_ranges[param_name].update(param_config)
            else:
                # æ·»åŠ æ–°å‚æ•°
                enhanced_ranges[param_name] = param_config.copy()
                enhanced_ranges[param_name]['type'] = 'float'  # é»˜è®¤ä¸ºæµ®ç‚¹æ•°
                enhanced_ranges[param_name]['precision'] = 4

        self.logger.info(f"ğŸ¯ å‚æ•°æœç´¢ç©ºé—´: {len(enhanced_ranges)} ä¸ªå‚æ•°")
        self.logger.info(f"ğŸ”’ å›ºå®šå‚æ•°: {', '.join(FIXED_PARAMS)} (ä¸å‚ä¸ä¼˜åŒ–)")
        self.logger.info(f"ğŸ”§ å¯ä¼˜åŒ–å‚æ•°: {len(get_all_optimizable_params())} ä¸ªï¼ˆ14ä¸ªæœ‰æ•ˆå‚æ•°ï¼Œå·²ç§»é™¤final_thresholdï¼‰")

        # è®°å½•å‚æ•°èŒƒå›´
        for param_name, param_config in enhanced_ranges.items():
            self.logger.info(f"   {param_name}: {param_config['min']} - {param_config['max']} ({param_config['type']})")

        return enhanced_ranges

    def _validate_and_fix_parameters(self, params: Dict, param_ranges: dict) -> Dict:
        """
        éªŒè¯å¹¶ä¿®å¤å‚æ•°
        
        å‚æ•°:
        params: å‚æ•°å­—å…¸
        param_ranges: å‚æ•°èŒƒå›´
        
        è¿”å›:
        Dict: ä¿®å¤åçš„å‚æ•°
        """
        fixed_params = {}

        for param_name, param_value in params.items():
            if param_name in param_ranges:
                param_config = param_ranges[param_name]
                min_val = param_config.get('min', 0)
                max_val = param_config.get('max', 1)
                param_type = param_config.get('type', 'float')

                # ç¡®ä¿åœ¨èŒƒå›´å†…
                fixed_value = np.clip(param_value, min_val, max_val)

                if param_type == 'int':
                    fixed_value = int(round(fixed_value))
                else:
                    precision = param_config.get('precision', 4)
                    fixed_value = round(fixed_value, precision)

                fixed_params[param_name] = fixed_value
            else:
                fixed_params[param_name] = param_value

        return fixed_params



    # å·²ç§»é™¤é‡å¤çš„_save_optimized_parametersæ–¹æ³•å®šä¹‰

    def _calculate_unified_score(self, evaluation: Dict[str, Any]) -> float:
        """
        ç»Ÿä¸€è¯„åˆ†ï¼ˆæŒ‰åˆ©æ¶¦ç›®æ ‡ï¼‰ï¼š
        ä½¿ç”¨ åˆ©æ¶¦å› å­ Ã— log1p(äº¤æ˜“æ¬¡æ•°) ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼›è‹¥äº¤æ˜“æ¬¡æ•°è¿‡å°‘åˆ™å¼ºæƒ©ç½šã€‚
        è‹¥evaluationå·²åŒ…å«ç¬¦åˆè§„åˆ™çš„scoreï¼Œåˆ™ç›´æ¥ä½¿ç”¨ä»¥ä¿æŒä¸€è‡´æ€§ã€‚
        """
        # ä¼˜å…ˆä½¿ç”¨evaluationä¸­çš„scoreï¼ˆå…è®¸ä¸ºè´Ÿï¼ŒæŒ‰åˆ©æ¶¦ç›´æ¥ä¼˜åŒ–ï¼‰
        if isinstance(evaluation, dict):
            existing_score = evaluation.get('score', None)
            if isinstance(existing_score, (int, float)) and np.isfinite(existing_score):
                return float(existing_score)
        
        # å¦åˆ™æŒ‰PFä¸äº¤æ˜“æ¬¡æ•°è®¡ç®—å‚è€ƒåˆ†
        profit_factor = float(evaluation.get('profit_factor', 0.0) or 0.0)
        num_trades = int(evaluation.get('total_trades', evaluation.get('total_points', 0)) or 0)

        # æœ€å°‘äº¤æ˜“æ¬¡æ•°é—¨æ§›ï¼ˆå¯ç”±é…ç½®è¦†ç›–ï¼‰
        min_trades_threshold = int(self.config.get('optimization_constraints', {}).get('min_trades_threshold', 10))

        # äº¤æ˜“è¿‡å°‘è¿”å›0ï¼ˆå‚è€ƒï¼‰
        if num_trades < min_trades_threshold:
            return 0.0

        # è®¡ç®—å¤åˆåˆ†æ•°ï¼šPF * log1p(N)
        score = profit_factor * float(np.log1p(num_trades))
        if not np.isfinite(score):
            return 0.0
        return float(score)
