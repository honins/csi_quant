#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ€§èƒ½å¼‚å¸¸åˆ†æè¯Šæ–­è„šæœ¬
ä¸“é—¨è¯Šæ–­æµ‹è¯•é›†æˆåŠŸç‡é«˜äºéªŒè¯é›†å’Œè®­ç»ƒé›†çš„åå¸¸ç°è±¡

æ ¹æ®æœºå™¨å­¦ä¹ ç†è®ºå’Œæ—¶é—´åºåˆ—åˆ†ææœ€ä½³å®è·µï¼Œæ­£å¸¸æƒ…å†µä¸‹åº”è¯¥æ˜¯ï¼š
è®­ç»ƒé›†æ€§èƒ½ > éªŒè¯é›†æ€§èƒ½ > æµ‹è¯•é›†æ€§èƒ½

å½“å‡ºç°ç›¸åæƒ…å†µæ—¶ï¼Œå¯èƒ½å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
1. æ•°æ®æ³„æ¼ (Data Leakage)
2. æ—¶é—´åºåˆ—å‰ç»åå·® (Lookahead Bias) 
3. æ•°æ®åˆ†å¸ƒå˜åŒ– (Distribution Shift)
4. æ ·æœ¬æƒé‡é—®é¢˜
5. è¿‡æ‹Ÿåˆæ£€æµ‹æœºåˆ¶é—®é¢˜
"""

import sys
import os
from pathlib import Path
import pandas as pd
import numpy as np
import logging
import json
import matplotlib.pyplot as plt
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'src'))

from src.utils.config_loader import ConfigLoader
from src.data.data_module import DataModule
from src.strategy.strategy_module import StrategyModule
from src.ai.ai_optimizer_improved import AIOptimizerImproved

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def analyze_data_distribution(train_data, val_data, test_data, logger):
    """åˆ†æä¸‰ä¸ªæ•°æ®é›†çš„åˆ†å¸ƒå·®å¼‚"""
    logger.info("ğŸ” åˆ†ææ•°æ®åˆ†å¸ƒå·®å¼‚...")
    
    results = {
        'distribution_analysis': {},
        'potential_issues': []
    }
    
    # 1. åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
    datasets = {
        'training': train_data,
        'validation': val_data, 
        'test': test_data
    }
    
    print("\nğŸ“Š æ•°æ®åˆ†å¸ƒåˆ†æ:")
    print("=" * 60)
    
    for name, data in datasets.items():
        if 'close' in data.columns:
            price_stats = {
                'mean': data['close'].mean(),
                'std': data['close'].std(),
                'min': data['close'].min(),
                'max': data['close'].max(),
                'skewness': data['close'].skew(),
                'kurtosis': data['close'].kurtosis()
            }
            results['distribution_analysis'][name] = price_stats
            
            print(f"\n{name.capitalize()}é›†ç»Ÿè®¡:")
            print(f"  å‡å€¼: {price_stats['mean']:.2f}")
            print(f"  æ ‡å‡†å·®: {price_stats['std']:.2f}")
            print(f"  ååº¦: {price_stats['skewness']:.3f}")
            print(f"  å³°åº¦: {price_stats['kurtosis']:.3f}")
            print(f"  ä»·æ ¼èŒƒå›´: {price_stats['min']:.2f} - {price_stats['max']:.2f}")
    
    # 2. æ£€æŸ¥åˆ†å¸ƒæ¼‚ç§»
    train_mean = results['distribution_analysis']['training']['mean']
    val_mean = results['distribution_analysis']['validation']['mean'] 
    test_mean = results['distribution_analysis']['test']['mean']
    
    # è®¡ç®—åˆ†å¸ƒæ¼‚ç§»ç¨‹åº¦
    val_drift = abs(val_mean - train_mean) / train_mean
    test_drift = abs(test_mean - train_mean) / train_mean
    
    print(f"\nğŸ“ˆ åˆ†å¸ƒæ¼‚ç§»åˆ†æ:")
    print(f"  éªŒè¯é›†vsè®­ç»ƒé›†: {val_drift:.2%}")
    print(f"  æµ‹è¯•é›†vsè®­ç»ƒé›†: {test_drift:.2%}")
    
    # åˆ†å¸ƒæ¼‚ç§»é˜ˆå€¼æ£€æŸ¥
    if val_drift > 0.1:  # 10%é˜ˆå€¼
        issue = f"éªŒè¯é›†ä¸è®­ç»ƒé›†å­˜åœ¨æ˜¾è‘—åˆ†å¸ƒå·®å¼‚ ({val_drift:.1%})"
        results['potential_issues'].append(issue)
        print(f"  âš ï¸ {issue}")
        
    if test_drift > 0.1:
        issue = f"æµ‹è¯•é›†ä¸è®­ç»ƒé›†å­˜åœ¨æ˜¾è‘—åˆ†å¸ƒå·®å¼‚ ({test_drift:.1%})"
        results['potential_issues'].append(issue)
        print(f"  âš ï¸ {issue}")
    
    # 3. æ£€æŸ¥æ—¶é—´è¶‹åŠ¿
    if 'date' in train_data.columns:
        print(f"\nğŸ“… æ—¶é—´èŒƒå›´åˆ†æ:")
        print(f"  è®­ç»ƒé›†: {train_data['date'].min()} ~ {train_data['date'].max()}")
        print(f"  éªŒè¯é›†: {val_data['date'].min()} ~ {val_data['date'].max()}")
        print(f"  æµ‹è¯•é›†: {test_data['date'].min()} ~ {test_data['date'].max()}")
        
        # æ£€æŸ¥æ—¶é—´é‡å 
        train_dates = set(train_data['date'])
        val_dates = set(val_data['date'])
        test_dates = set(test_data['date'])
        
        if train_dates & val_dates:
            issue = "è®­ç»ƒé›†ä¸éªŒè¯é›†å­˜åœ¨æ—¶é—´é‡å  - å¯èƒ½æ•°æ®æ³„æ¼!"
            results['potential_issues'].append(issue)
            print(f"  ğŸš¨ {issue}")
            
        if train_dates & test_dates:
            issue = "è®­ç»ƒé›†ä¸æµ‹è¯•é›†å­˜åœ¨æ—¶é—´é‡å  - ä¸¥é‡æ•°æ®æ³„æ¼!"
            results['potential_issues'].append(issue)
            print(f"  ğŸš¨ {issue}")
            
        if val_dates & test_dates:
            issue = "éªŒè¯é›†ä¸æµ‹è¯•é›†å­˜åœ¨æ—¶é—´é‡å  - æ•°æ®æ³„æ¼!"
            results['potential_issues'].append(issue)
            print(f"  ğŸš¨ {issue}")
    
    return results

def analyze_market_conditions(train_data, val_data, test_data, logger):
    """åˆ†æä¸åŒæ—¶æœŸçš„å¸‚åœºæ¡ä»¶"""
    logger.info("ğŸ“ˆ åˆ†æå¸‚åœºæ¡ä»¶å·®å¼‚...")
    
    results = {
        'market_analysis': {},
        'potential_explanations': []
    }
    
    datasets = {
        'training': train_data,
        'validation': val_data,
        'test': test_data
    }
    
    print("\nğŸ“ˆ å¸‚åœºæ¡ä»¶åˆ†æ:")
    print("=" * 60)
    
    for name, data in datasets.items():
        if len(data) > 1 and 'close' in data.columns:
            # è®¡ç®—æ”¶ç›Šç‡
            returns = data['close'].pct_change().dropna()
            
            # è®¡ç®—å¸‚åœºæŒ‡æ ‡
            volatility = returns.std() * np.sqrt(252)  # å¹´åŒ–æ³¢åŠ¨ç‡
            trend = (data['close'].iloc[-1] / data['close'].iloc[0] - 1) * 100  # æ€»æ”¶ç›Šç‡
            positive_days = (returns > 0).mean() * 100  # ä¸Šæ¶¨å¤©æ•°æ¯”ä¾‹
            
            market_stats = {
                'volatility': volatility,
                'total_return': trend,
                'positive_days_pct': positive_days,
                'max_drawdown': ((data['close'] / data['close'].cummax() - 1).min()) * 100
            }
            
            results['market_analysis'][name] = market_stats
            
            print(f"\n{name.capitalize()}é›†å¸‚åœºæ¡ä»¶:")
            print(f"  å¹´åŒ–æ³¢åŠ¨ç‡: {volatility:.1%}")
            print(f"  æ€»æ”¶ç›Šç‡: {trend:.1f}%")
            print(f"  ä¸Šæ¶¨å¤©æ•°æ¯”ä¾‹: {positive_days:.1f}%")
            print(f"  æœ€å¤§å›æ’¤: {market_stats['max_drawdown']:.1f}%")
    
    # åˆ†æå¸‚åœºæ¡ä»¶å˜åŒ–
    if len(results['market_analysis']) == 3:
        train_vol = results['market_analysis']['training']['volatility']
        val_vol = results['market_analysis']['validation']['volatility']
        test_vol = results['market_analysis']['test']['volatility']
        
        train_return = results['market_analysis']['training']['total_return']
        val_return = results['market_analysis']['validation']['total_return']
        test_return = results['market_analysis']['test']['total_return']
        
        print(f"\nğŸ“Š å¸‚åœºæ¡ä»¶å˜åŒ–:")
        print(f"  æ³¢åŠ¨ç‡å˜åŒ–: è®­ç»ƒ({train_vol:.1%}) â†’ éªŒè¯({val_vol:.1%}) â†’ æµ‹è¯•({test_vol:.1%})")
        print(f"  æ”¶ç›Šç‡å˜åŒ–: è®­ç»ƒ({train_return:.1f}%) â†’ éªŒè¯({val_return:.1f}%) â†’ æµ‹è¯•({test_return:.1f}%)")
        
        # è§£é‡Šå¯èƒ½çš„åŸå› 
        if test_vol < train_vol * 0.7:
            explanation = "æµ‹è¯•æœŸæ³¢åŠ¨ç‡æ˜¾è‘—é™ä½ï¼Œå¯èƒ½ä½¿é¢„æµ‹æ›´å®¹æ˜“"
            results['potential_explanations'].append(explanation)
            print(f"  ğŸ’¡ {explanation}")
            
        if test_return > val_return > train_return:
            explanation = "æµ‹è¯•æœŸå¸‚åœºè¡¨ç°é€æ­¥æ”¹å–„ï¼Œå¯èƒ½å­˜åœ¨è¶‹åŠ¿æ€§æœºä¼š"
            results['potential_explanations'].append(explanation)
            print(f"  ğŸ’¡ {explanation}")
            
        if abs(test_return) > abs(train_return) * 2:
            explanation = "æµ‹è¯•æœŸå­˜åœ¨æ˜¾è‘—è¶‹åŠ¿ï¼Œå¯èƒ½å¯¼è‡´ç­–ç•¥è¡¨ç°å¼‚å¸¸"
            results['potential_explanations'].append(explanation)
            print(f"  ğŸ’¡ {explanation}")
    
    return results

def check_data_leakage_indicators(data, strategy_module, logger):
    """æ£€æŸ¥æ•°æ®æ³„æ¼çš„æŒ‡æ ‡"""
    logger.info("ğŸ” æ£€æŸ¥æ•°æ®æ³„æ¼æŒ‡æ ‡...")
    
    results = {
        'leakage_indicators': {},
        'warnings': []
    }
    
    print("\nğŸ” æ•°æ®æ³„æ¼æ£€æŸ¥:")
    print("=" * 60)
    
    # 1. æ£€æŸ¥ç‰¹å¾ä¸­æ˜¯å¦åŒ…å«æœªæ¥ä¿¡æ¯
    feature_columns = data.columns.tolist()
    suspicious_features = []
    
    future_keywords = ['future', 'target', 'label', 'next', 'forward', 'ahead']
    for col in feature_columns:
        for keyword in future_keywords:
            if keyword in col.lower():
                suspicious_features.append(col)
    
    if suspicious_features:
        warning = f"å‘ç°å¯ç–‘çš„æœªæ¥ä¿¡æ¯ç‰¹å¾: {suspicious_features}"
        results['warnings'].append(warning)
        print(f"  ğŸš¨ {warning}")
    else:
        print("  âœ… æœªå‘ç°æ˜æ˜¾çš„æœªæ¥ä¿¡æ¯ç‰¹å¾")
    
    # 2. æ£€æŸ¥å›æµ‹é€»è¾‘ä¸­çš„æ•°æ®æ³„æ¼
    # æ¨¡æ‹Ÿä¸€å°æ®µæ•°æ®çš„å›æµ‹ï¼Œæ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æœªæ¥ä¿¡æ¯
    sample_data = data.head(50).copy()  # å–å‰50å¤©æ•°æ®
    
    try:
        # æ£€æŸ¥ç­–ç•¥è¯†åˆ«é€»è¾‘
        backtest_results = strategy_module.backtest(sample_data)
        
        # æ£€æŸ¥æ˜¯å¦åœ¨è®¡ç®—å½“å‰ç‚¹æ—¶ä½¿ç”¨äº†æœªæ¥æ•°æ®
        for i in range(min(10, len(backtest_results)-1)):
            current_date = backtest_results.iloc[i]['date']
            
            # æ£€æŸ¥å½“å‰è¡Œæ˜¯å¦åŒ…å«äº†æœªæ¥æ—¥æœŸçš„ä¿¡æ¯
            if 'max_rise_date' in backtest_results.columns:
                max_rise_date = backtest_results.iloc[i]['max_rise_date']
                if pd.notna(max_rise_date) and max_rise_date > current_date:
                    # è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºmax_rise_dateæ˜¯ç”¨äºéªŒè¯çš„æœªæ¥ä¿¡æ¯
                    pass
    
        print("  âœ… å›æµ‹é€»è¾‘æ£€æŸ¥é€šè¿‡")
        
    except Exception as e:
        warning = f"å›æµ‹é€»è¾‘æ£€æŸ¥å¼‚å¸¸: {e}"
        results['warnings'].append(warning)
        print(f"  âš ï¸ {warning}")
    
    # 3. æ£€æŸ¥æŠ€æœ¯æŒ‡æ ‡è®¡ç®—æ˜¯å¦æ­£ç¡®
    if 'rsi' in data.columns and len(data) > 20:
        # æ£€æŸ¥RSIè®¡ç®—æ˜¯å¦æœ‰å‰ç»åå·®
        rsi_values = data['rsi'].dropna()
        if len(rsi_values) > 0:
            # RSIåº”è¯¥åœ¨0-100ä¹‹é—´
            if rsi_values.min() < 0 or rsi_values.max() > 100:
                warning = f"RSIå€¼å¼‚å¸¸ (èŒƒå›´: {rsi_values.min():.2f} - {rsi_values.max():.2f})"
                results['warnings'].append(warning)
                print(f"  âš ï¸ {warning}")
            else:
                print("  âœ… æŠ€æœ¯æŒ‡æ ‡èŒƒå›´æ­£å¸¸")
    
    return results

def analyze_sample_weights_impact(data, logger):
    """åˆ†ææ ·æœ¬æƒé‡å¯¹ç»“æœçš„å½±å“"""
    logger.info("âš–ï¸ åˆ†ææ ·æœ¬æƒé‡å½±å“...")
    
    results = {
        'weight_analysis': {},
        'insights': []
    }
    
    print("\nâš–ï¸ æ ·æœ¬æƒé‡åˆ†æ:")
    print("=" * 60)
    
    if 'date' in data.columns:
        # æ¨¡æ‹Ÿæ—¶é—´è¡°å‡æƒé‡è®¡ç®—
        dates = pd.to_datetime(data['date'])
        latest_date = dates.max()
        decay_rate = 0.4  # é»˜è®¤è¡°å‡ç‡
        
        # è®¡ç®—æƒé‡
        days_diff = (latest_date - dates).dt.days
        weights = np.exp(-decay_rate * days_diff / 365.0)
        
        # åˆ†ææƒé‡åˆ†å¸ƒ
        print(f"  æƒé‡èŒƒå›´: {weights.min():.4f} - {weights.max():.4f}")
        print(f"  æƒé‡å¹³å‡å€¼: {weights.mean():.4f}")
        print(f"  æƒé‡æ ‡å‡†å·®: {weights.std():.4f}")
        
        # æ£€æŸ¥æƒé‡åå·®
        early_weights = weights[:len(weights)//3].mean()
        late_weights = weights[-len(weights)//3:].mean()
        weight_bias = late_weights / early_weights
        
        print(f"  æ—©æœŸæ•°æ®å¹³å‡æƒé‡: {early_weights:.4f}")
        print(f"  åæœŸæ•°æ®å¹³å‡æƒé‡: {late_weights:.4f}")
        print(f"  æƒé‡åå·®æ¯”ç‡: {weight_bias:.2f}")
        
        if weight_bias > 5:
            insight = f"åæœŸæ•°æ®æƒé‡æ˜¾è‘—é«˜äºæ—©æœŸ ({weight_bias:.1f}å€)ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒåå‘æ–°æ•°æ®"
            results['insights'].append(insight)
            print(f"  ğŸ’¡ {insight}")
        
        results['weight_analysis'] = {
            'min_weight': weights.min(),
            'max_weight': weights.max(),
            'mean_weight': weights.mean(),
            'weight_bias_ratio': weight_bias
        }
    
    return results

def analyze_strategy_complexity(train_data, val_data, test_data, strategy_module, logger):
    """åˆ†æç­–ç•¥å¤æ‚åº¦å’Œæ‹Ÿåˆæƒ…å†µ"""
    logger.info("ğŸ¯ åˆ†æç­–ç•¥å¤æ‚åº¦...")
    
    results = {
        'complexity_analysis': {},
        'recommendations': []
    }
    
    print("\nğŸ¯ ç­–ç•¥å¤æ‚åº¦åˆ†æ:")
    print("=" * 60)
    
    # è·å–ç­–ç•¥å‚æ•°
    params = strategy_module.get_params()
    print(f"  ç­–ç•¥å‚æ•°: {params}")
    
    # åˆ†ææ¯ä¸ªæ•°æ®é›†ä¸Šçš„è¡¨ç°
    datasets = {
        'training': train_data,
        'validation': val_data,
        'test': test_data
    }
    
    performance_scores = {}
    
    for name, data in datasets.items():
        try:
            backtest_results = strategy_module.backtest(data)
            evaluation = strategy_module.evaluate_strategy(backtest_results)
            
            performance_scores[name] = {
                'score': evaluation.get('score', 0),
                'success_rate': evaluation.get('success_rate', 0),
                'total_points': evaluation.get('total_points', 0),
                'avg_rise': evaluation.get('avg_rise', 0)
            }
            
            print(f"\n  {name.capitalize()}é›†è¡¨ç°:")
            print(f"    ç»¼åˆå¾—åˆ†: {evaluation.get('score', 0):.4f}")
            print(f"    æˆåŠŸç‡: {evaluation.get('success_rate', 0):.2%}")
            print(f"    è¯†åˆ«ç‚¹æ•°: {evaluation.get('total_points', 0)}")
            print(f"    å¹³å‡æ¶¨å¹…: {evaluation.get('avg_rise', 0):.2%}")
            
        except Exception as e:
            logger.warning(f"åˆ†æ{name}é›†æ—¶å‡ºé”™: {e}")
    
    # åˆ†ææ€§èƒ½è¶‹åŠ¿
    if len(performance_scores) == 3:
        train_score = performance_scores['training']['score']
        val_score = performance_scores['validation']['score'] 
        test_score = performance_scores['test']['score']
        
        print(f"\nğŸ“Š æ€§èƒ½è¶‹åŠ¿åˆ†æ:")
        print(f"  è®­ç»ƒé›† â†’ éªŒè¯é›† â†’ æµ‹è¯•é›†: {train_score:.4f} â†’ {val_score:.4f} â†’ {test_score:.4f}")
        
        # å¼‚å¸¸æ¨¡å¼æ£€æµ‹
        if test_score > val_score > train_score:
            recommendation = "å¼‚å¸¸æ¨¡å¼ï¼šæµ‹è¯•é›†>éªŒè¯é›†>è®­ç»ƒé›†ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®æ³„æ¼æˆ–åˆ†å¸ƒåç§»"
            results['recommendations'].append(recommendation)
            print(f"  ğŸš¨ {recommendation}")
            
        elif test_score > val_score * 1.2:
            recommendation = "æµ‹è¯•é›†æ€§èƒ½å¼‚å¸¸ä¼˜ç§€ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®åˆ†å‰²å’Œæ—¶é—´åºåˆ—è¿ç»­æ€§"
            results['recommendations'].append(recommendation)
            print(f"  âš ï¸ {recommendation}")
            
        elif val_score < train_score * 0.8:
            recommendation = "å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆï¼Œå»ºè®®é™ä½æ¨¡å‹å¤æ‚åº¦"
            results['recommendations'].append(recommendation)
            print(f"  ğŸ’¡ {recommendation}")
    
    results['complexity_analysis'] = performance_scores
    return results

def generate_diagnostic_report(all_results, logger):
    """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
    logger.info("ğŸ“ ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š...")
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = f"results/performance_anomaly_diagnosis_{timestamp}.json"
    
    # æ±‡æ€»æ‰€æœ‰å‘ç°çš„é—®é¢˜
    all_issues = []
    all_explanations = []
    all_recommendations = []
    
    for result in all_results:
        all_issues.extend(result.get('potential_issues', []))
        all_issues.extend(result.get('warnings', []))
        all_explanations.extend(result.get('potential_explanations', []))
        all_explanations.extend(result.get('insights', []))
        all_recommendations.extend(result.get('recommendations', []))
    
    # ç”Ÿæˆæœ€ç»ˆè¯Šæ–­
    diagnosis = {
        'timestamp': timestamp,
        'summary': {
            'total_issues_found': len(all_issues),
            'total_explanations': len(all_explanations),
            'total_recommendations': len(all_recommendations)
        },
        'identified_issues': all_issues,
        'potential_explanations': all_explanations,
        'recommendations': all_recommendations,
        'detailed_analysis': all_results
    }
    
    # ä¿å­˜æŠ¥å‘Š
    os.makedirs('results', exist_ok=True)
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(diagnosis, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ“ è¯Šæ–­æŠ¥å‘Š:")
    print("=" * 60)
    print(f"ğŸ” å‘ç°é—®é¢˜æ•°é‡: {len(all_issues)}")
    print(f"ğŸ’¡ å¯èƒ½è§£é‡Šæ•°é‡: {len(all_explanations)}")
    print(f"ğŸ¯ æ”¹è¿›å»ºè®®æ•°é‡: {len(all_recommendations)}")
    print(f"ğŸ“„ æŠ¥å‘Šä¿å­˜è·¯å¾„: {report_path}")
    
    # è¾“å‡ºä¸»è¦å‘ç°
    if all_issues:
        print(f"\nğŸš¨ ä¸»è¦é—®é¢˜:")
        for i, issue in enumerate(all_issues[:5], 1):
            print(f"  {i}. {issue}")
    
    if all_explanations:
        print(f"\nğŸ’¡ å¯èƒ½è§£é‡Š:")
        for i, explanation in enumerate(all_explanations[:3], 1):
            print(f"  {i}. {explanation}")
    
    if all_recommendations:
        print(f"\nğŸ¯ æ”¹è¿›å»ºè®®:")
        for i, rec in enumerate(all_recommendations[:3], 1):
            print(f"  {i}. {rec}")
    
    return diagnosis

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æ€§èƒ½å¼‚å¸¸åˆ†æè¯Šæ–­")
    print("=" * 80)
    print("ç›®æ ‡ï¼šè¯Šæ–­æµ‹è¯•é›†æˆåŠŸç‡é«˜äºéªŒè¯é›†å’Œè®­ç»ƒé›†çš„åå¸¸ç°è±¡")
    print("=" * 80)
    
    logger = setup_logging()
    
    try:
        # åŠ è½½é…ç½®å’Œæ•°æ®
        config_loader = ConfigLoader()
        config = config_loader.get_config()
        
        data_module = DataModule(config)
        # ä»é…ç½®æ–‡ä»¶è·å–æ—¶é—´èŒƒå›´
        data_config = config.get('data', {})
        time_range = data_config.get('time_range', {})
        start_date = time_range.get('start_date', '2019-01-01')
        end_date = time_range.get('end_date', '2025-07-15')
        
        data = data_module.get_history_data(start_date, end_date)
        data = data_module.preprocess_data(data)
        
        strategy_module = StrategyModule(config)
        
        # æ¨¡æ‹ŸAIä¼˜åŒ–å™¨çš„æ•°æ®åˆ†å‰²æ–¹å¼
        validation_config = config.get('ai', {}).get('validation', {})
        train_ratio = validation_config.get('train_ratio', 0.70)
        val_ratio = validation_config.get('validation_ratio', 0.20)
        test_ratio = validation_config.get('test_ratio', 0.10)
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        train_end = int(len(data) * train_ratio)
        val_end = int(len(data) * (train_ratio + val_ratio))
        
        # åˆ†å‰²æ•°æ®
        train_data = data.iloc[:train_end].copy()
        validation_data = data.iloc[train_end:val_end].copy()
        test_data = data.iloc[val_end:].copy()
        
        print(f"ğŸ“Š æ•°æ®åˆ†å‰²ç»“æœ:")
        print(f"  è®­ç»ƒé›†: {len(train_data)}æ¡ ({train_ratio:.1%})")
        print(f"  éªŒè¯é›†: {len(validation_data)}æ¡ ({val_ratio:.1%})")
        print(f"  æµ‹è¯•é›†: {len(test_data)}æ¡ ({test_ratio:.1%})")
        
        # å¼€å§‹è¯Šæ–­åˆ†æ
        all_results = []
        
        # 1. æ•°æ®åˆ†å¸ƒåˆ†æ
        dist_result = analyze_data_distribution(train_data, validation_data, test_data, logger)
        all_results.append(dist_result)
        
        # 2. å¸‚åœºæ¡ä»¶åˆ†æ
        market_result = analyze_market_conditions(train_data, validation_data, test_data, logger)
        all_results.append(market_result)
        
        # 3. æ•°æ®æ³„æ¼æ£€æŸ¥
        leakage_result = check_data_leakage_indicators(data, strategy_module, logger)
        all_results.append(leakage_result)
        
        # 4. æ ·æœ¬æƒé‡å½±å“åˆ†æ
        weight_result = analyze_sample_weights_impact(data, logger)
        all_results.append(weight_result)
        
        # 5. ç­–ç•¥å¤æ‚åº¦åˆ†æ
        complexity_result = analyze_strategy_complexity(train_data, validation_data, test_data, strategy_module, logger)
        all_results.append(complexity_result)
        
        # 6. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
        final_diagnosis = generate_diagnostic_report(all_results, logger)
        
        print(f"\nâœ… è¯Šæ–­åˆ†æå®Œæˆï¼")
        return True
        
    except Exception as e:
        logger.error(f"è¯Šæ–­åˆ†æå¼‚å¸¸: {e}")
        print(f"\nâŒ è¯Šæ–­åˆ†æå¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    main() 